1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation with knowledge-grounded responses . The authors propose a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . The paper is well-written and easy to follow . However , I have the following concerns . 1 .The authors claim that their model is the first neural model which incorporates the posterior distribution as guidance , enabling accurate knowledge lookups and high quality response generation . But , it is not clear to me how this is achieved . For example , the prior distribution p ( k|x ) is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution is inferred only from utterances only , so that appropriate knowledge can be selected even without responses during the inference process . Compared with the previous work , our model can better incorporate appropriate knowledge in response generation , which is better than previous work . 2 .In Table 1 , the authors compare the performance of their model with Seq2Seq model with a GRU decoder where knowledge is concatenated . It would be better if the authors can show the results of different ways of incorporating knowledge incorporating knowledge . 3 .It is better to show the effect of different loss functions in Table 1 .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT model . The main idea is to prune the parameters of each layer of the model by 5 different dimensions . The pruning is done by optimizing the design dimensions in an optimization procedure . The paper is well written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me how the proposed pruning method can be applied to other objectives such as FLOPs or latency . 2 .The experiments are only conducted on GLUE and SQuAD datasets . It would be better if the authors can conduct experiments on other NLP tasks such as CIFAR-10/100/MNIST/etc . 3 .The proposed method is not compared with other pruning methods such as DistilBERT ( Sanh et al. , 2019 ) and RoBERTa ( Liu et al .2019 ) .The authors should compare with these methods to show the effectiveness of this method .
1_2102.07983 = This paper introduces FEWS ( Few-shot Examples of Word Senses ) , a new low-shot WSD dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , this paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of few-shot and zero-shot senses extracted from Wikdatasets , which is not a new dataset . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low-cost manner , but it is not clear to me why this is the case . It would be better if the authors can provide more explanation on this point . 3 .In Table 1 , it is unclear to me what is the performance of the biencoder model on the zero shot senses . Is it the same as the baseline model ? 4 .The evaluation set is not very comprehensive . It seems that the authors did not compare with the SemCor dataset , which contains 761 unique zero-shots and 761 few-shots senses . It will be better to provide more details on the evaluation set . 5 .The paper is not well-organized . For example , the abstract and introduction should be moved to the main paper . 6 .The experiments are not convincing . The results are not compared with the state-of-the-art .
1_1812.02425 = This paper proposes to use a teacher-student ensembling framework to learn an ensemble of multiple neural networks without incurring any additional testing costs . The idea is to use the outputs of different neural networks as supervisions to guide the target network training . To further improve the robustness of the student networks , the authors introduce an adversarial learning strategy to force the student to generate similar outputs as teachers . The experiments show that MEAL consistently improves the accuracy across a variety of popular network architectures on different datasets . The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a number of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . It is a simple extension of Swapout ( Singh , Hoiem , and Forsyth 2016 ) . 2 .The proposed method can achieve the goal of ensembleling multiple neural network with no additional testing cost . 3 .The experimental results are not convincing . For example , in Table 1 , the performance of Shake-Shake ( Gastaldi 2017 ) and MEAL ( Meals et al. , 2017 ) are not better than Shake-shake and Meals , respectively , in terms of accuracy .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper is technically sound . Cons : 1.1 . The novelty of the proposed method is limited . 2.2 .The experimental results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
