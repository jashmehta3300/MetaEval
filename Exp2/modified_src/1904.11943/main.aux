\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{jouppi2017datacenter,projectbrainwave}
\citation{jouppi2017datacenter}
\citation{dorefa-net,WAGE,scalable8bittraining,8bitfloat}
\citation{binaryconnect,WAGE,8bitfloat}
\citation{8bitfloat}
\citation{SWA}
\citation{SWA}
\citation{large-batch}
\citation{SWA}
\citation{preact-resnet}
\citation{CIFAR10}
\citation{VGG}
\citation{deepCompression}
\citation{binary-net,ternary-quant,incremental-quant}
\citation{gupta2015deep,flexpoint,log-quant,dyanmic-fixed-point,8bitfloat}
\citation{WAGE,xor-net,dorefa-net,binaryconnect,scalable8bittraining}
\citation{SWA}
\citation{SWA}
\citation{SWA}
\citation{training-quantized-network-deeper-understanding}
\citation{HALP}
\citation{training-quantized-network-deeper-understanding}
\citation{QSGD}
\citation{ZipML}
\citation{SWA}
\citation{log-quant,flexpoint,dyanmic-fixed-point,low-precision-multiply}
\citation{error-analysis}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:swalpexp}{{1}{2}{SWALP intuition. The trajectory of low-precision SGD, with a modified learning rate, over the training objective (with given contours), and the SWALP solution obtained by averaging. \relax }{figure.caption.1}{}}
\newlabel{sec:related}{{2}{2}{}{section.2}{}}
\newlabel{sec:method}{{3}{2}{}{section.3}{}}
\citation{training-quantized-network-deeper-understanding}
\citation{gupta2015deep}
\citation{error-analysis,Mixed-precision-training}
\citation{error-analysis}
\citation{SWA}
\citation{binaryconnect,dorefa-net,WAGE,8bitfloat}
\citation{training-quantized-network-deeper-understanding,HALP}
\newlabel{sec:method-quant}{{3.1}{3}{}{subsection.3.1}{}}
\newlabel{eq:fixed-point-SR}{{1}{3}{}{equation.3.1}{}}
\newlabel{alg:SWALP}{{1}{3}{SWALP\relax }{algorithm.1}{}}
\newlabel{sec:method-algo}{{3.2}{3}{}{subsection.3.2}{}}
\citation{WAGE,8bitfloat}
\citation{HALP}
\newlabel{alg:SWALP-all}{{2}{4}{SWALP with all numbers quantized.\relax }{algorithm.2}{}}
\newlabel{sec:method-quant-all}{{3.3}{4}{}{subsection.3.3}{}}
\newlabel{sec:convergence}{{4}{4}{}{section.4}{}}
\newlabel{sec:convergence-quadratic}{{4.1}{4}{}{subsection.4.1}{}}
\citation{francisbach}
\citation{francisbach}
\citation{training-quantized-network-deeper-understanding}
\citation{training-quantized-network-deeper-understanding}
\newlabel{thm:quadratic}{{1}{5}{}{theorem.1}{}}
\newlabel{sec:convergence-strongly-convex}{{4.2}{5}{}{subsection.4.2}{}}
\newlabel{lemmaNoiseBall}{{1}{5}{}{lemma.1}{}}
\newlabel{thm:SWALP}{{2}{5}{}{theorem.2}{}}
\citation{MNIST}
\citation{HALP,SVRG}
\citation{HALP}
\newlabel{fig:convergence}{{2}{6}{Empirical verification of two theorems with linear regression and logistic regression. (Left) SWALP converges below the quantization noise and to the optimal solution in linear regression; (Middle) SWALP can converge to a smaller noise ball than SGD-LP and SGD; (Right) SWALP requires less than half of the float bits to achieve the same performance compared to SGD-LP. \relax }{figure.caption.2}{}}
\newlabel{thm:lower-bound}{{3}{6}{}{theorem.3}{}}
\newlabel{sec:convergence-verify}{{4.3}{6}{}{subsection.4.3}{}}
\citation{CIFAR10}
\citation{imagenet}
\citation{SWA,WAGE}
\citation{PyTorch}
\citation{VGG}
\citation{preact-resnet}
\citation{SWA,SWA-repo}
\citation{resnet}
\citation{error-analysis}
\citation{error-analysis}
\citation{dorefa-net}
\citation{SWA}
\citation{SWA}
\newlabel{table:base-results}{{1}{7}{Test error (\%) on CIFAR-10 and CIFAR-100 for VGG16 and PreResNet-164 trained in different quantization setting. \relax }{table.caption.3}{}}
\newlabel{sec:expr}{{5}{7}{}{section.5}{}}
\newlabel{sec:expr-perf}{{5.1}{7}{}{subsection.5.1}{}}
\citation{resnet}
\newlabel{fig:diffc_qswa}{{3}{8}{CIFAR-100 classification test error (\%). \textbf {Left:} Different averaging frequency. \textbf {Right:} Different averaging precision. \relax }{figure.caption.4}{}}
\newlabel{table:imagenet}{{2}{8}{ImageNet experiment results with ResNet-18. 90+$X$ epochs of SWA (or SWALP) means running weight averaging for $X$ epochs starting at 90 epocth. \relax }{table.caption.5}{}}
\newlabel{sec:concl}{{6}{8}{}{section.6}{}}
\bibdata{ref}
\bibcite{QSGD}{{1}{2017}{{Alistarh et~al.}}{{Alistarh, Grubic, Li, Tomioka, and Vojnovic}}}
\bibcite{incremental-quant}{{2}{2017}{{Aojun~Zhou}}{{}}}
\bibcite{scalable8bittraining}{{3}{2018}{{Banner et~al.}}{{Banner, Hubara, Hoffer, and Soudry}}}
\bibcite{projectbrainwave}{{4}{2017}{{Burger}}{{}}}
\bibcite{low-precision-multiply}{{5}{2014}{{Courbariaux et~al.}}{{Courbariaux, Bengio, and David}}}
\bibcite{binaryconnect}{{6}{2015}{{Courbariaux et~al.}}{{Courbariaux, Bengio, and David}}}
\bibcite{Mixed-precision-training}{{7}{2018}{{Das et~al.}}{{Das, Mellempudi, Mudigere, Kalamkar, Avancha, Banerjee, Sridharan, Vaidyanathan, Kaul, Georganas, Heinecke, Dubey, Corbal, Shustrov, Dubtsov, Fomenko, and Pirogov}}}
\bibcite{HALP}{{8}{2018}{{De~Sa et~al.}}{{De~Sa, Leszczynski, Zhang, Marzoev, Aberger, Olukotun, and R{\'{e}}}}}
\bibcite{gupta2015deep}{{9}{2015}{{Gupta et~al.}}{{Gupta, Agrawal, Gopalakrishnan, and Narayanan}}}
\bibcite{deepCompression}{{10}{2015}{{Han et~al.}}{{Han, Mao, and Dally}}}
\bibcite{he-init}{{11}{2015{a}}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{resnet}{{12}{2015{b}}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{preact-resnet}{{13}{2016}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{binary-net}{{14}{2016}{{Hubara et~al.}}{{Hubara, Courbariaux, Soudry, El-Yaniv, and Bengio}}}
\bibcite{SWA}{{15}{2018{a}}{{Izmailov et~al.}}{{Izmailov, Podoprikhin, Garipov, Vetrov, and Wilson}}}
\bibcite{SWA-repo}{{16}{2018{b}}{{Izmailov et~al.}}{{Izmailov, Podoprikhin, Garipov, Vetrov, and Wilson}}}
\bibcite{SVRG}{{17}{2013}{{Johnson \& Zhang}}{{Johnson and Zhang}}}
\bibcite{jouppi2017datacenter}{{18}{2017}{{Jouppi et~al.}}{{Jouppi, Young, Patil, Patterson, Agrawal, Bajwa, Bates, Bhatia, Boden, Borchers, et~al.}}}
\bibcite{large-batch}{{19}{2016}{{Keskar et~al.}}{{Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang}}}
\bibcite{flexpoint}{{20}{2017}{{K{\"o}ster et~al.}}{{K{\"o}ster, Webb, Wang, Nassar, Bansal, Constable, Elibol, Gray, Hall, Hornof, et~al.}}}
\bibcite{CIFAR10}{{21}{2009}{{Krizhevsky \& Hinton}}{{Krizhevsky and Hinton}}}
\newlabel{sec:ack}{{6}{9}{Acknowledgements}{section*.7}{}}
\bibcite{MNIST}{{22}{1998}{{LeCun et~al.}}{{LeCun, Bottou, Bengio, and Haffner}}}
\bibcite{training-quantized-network-deeper-understanding}{{23}{2017}{{Li et~al.}}{{Li, De, Xu, Studer, Samet, and Goldstein}}}
\bibcite{dyanmic-fixed-point}{{24}{2017}{{Mellempudi et~al.}}{{Mellempudi, Kundu, Das, Mudigere, and Kaul}}}
\bibcite{log-quant}{{25}{2016}{{Miyashita et~al.}}{{Miyashita, Lee, and Murmann}}}
\bibcite{francisbach}{{26}{2011}{{Moulines \& Bach}}{{Moulines and Bach}}}
\bibcite{PyTorch}{{27}{2017}{{Paszke et~al.}}{{Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin, Desmaison, Antiga, and Lerer}}}
\bibcite{xor-net}{{28}{2016}{{Rastegari et~al.}}{{Rastegari, Ordonez, Redmon, and Farhadi}}}
\bibcite{imagenet}{{29}{2014}{{Russakovsky et~al.}}{{Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, Berg, and Li}}}
\bibcite{VGG}{{30}{2014}{{Simonyan \& Zisserman}}{{Simonyan and Zisserman}}}
\bibcite{error-analysis}{{31}{2017}{{Song et~al.}}{{Song, Liu, Wang, and Wang}}}
\bibcite{8bitfloat}{{32}{2018}{{Wang et~al.}}{{Wang, Choi, Brand, Chen, and Gopalakrishnan}}}
\bibcite{WAGE}{{33}{2018}{{Wu et~al.}}{{Wu, Li, Chen, and Shi}}}
\bibcite{ZipML}{{34}{2017}{{Zhang et~al.}}{{Zhang, Li, Kara, Alistarh, Liu, and Zhang}}}
\bibcite{dorefa-net}{{35}{2016}{{Zhou et~al.}}{{Zhou, Wu, Ni, Zhou, Wen, and Zou}}}
\bibcite{ternary-quant}{{36}{2016}{{Zhu et~al.}}{{Zhu, Han, Mao, and Dally}}}
\bibstyle{icml2019}
\citation{WAGE}
\newlabel{sec:appendix-thm-quadratic}{{B}{11}{Acknowledgements}{appendix.1.B}{}}
\newlabel{sec:appendix-lemma}{{C}{13}{Acknowledgements}{appendix.1.C}{}}
\newlabel{lemmaNewSingleIterBound}{{\ref  {lemmaNoiseBall}}{14}{}{innercustomgeneric.2}{}}
\newlabel{sec:thm4.3}{{D}{17}{Acknowledgements}{appendix.1.D}{}}
\citation{training-quantized-network-deeper-understanding}
\newlabel{sec:thm-lowerbound}{{E}{21}{Acknowledgements}{appendix.1.E}{}}
\newlabel{lemmaCzbeta}{{2}{22}{}{lemma.2}{}}
\newlabel{thm:lower-bound-appendix}{{\ref  {thm:lower-bound}}{23}{}{innercustomgeneric.4}{}}
\citation{WAGE}
\citation{WAGE}
\citation{WAGE}
\citation{MNIST}
\citation{MNIST}
\citation{HALP,SVRG}
\citation{HALP}
\newlabel{fig:log-log}{{4(a)}{25}{Subfigure 4(a)}{subfigure.4.1}{}}
\newlabel{sub@fig:log-log}{{(a)}{25}{Subfigure 4(a)\relax }{subfigure.4.1}{}}
\newlabel{fig:logreg-diff-prec-test}{{4(b)}{25}{Subfigure 4(b)}{subfigure.4.2}{}}
\newlabel{sub@fig:logreg-diff-prec-test}{{(b)}{25}{Subfigure 4(b)\relax }{subfigure.4.2}{}}
\newlabel{sec:wage-swalp}{{F}{25}{Acknowledgements}{appendix.1.F}{}}
\newlabel{tab:wage-swalp}{{3}{25}{Test Error (\%) on CIFAR10. SWALP has positive interaction with state-of-the-art low-precision training algorithm.\relax }{table.caption.9}{}}
\newlabel{sec:linreg}{{G}{25}{Acknowledgements}{appendix.1.G}{}}
\citation{SWA-repo}
\citation{SWA}
\citation{resnet}
\citation{SWA}
\citation{he-init}
\citation{he-init}
\newlabel{sec:logreg}{{H}{26}{Acknowledgements}{appendix.1.H}{}}
\newlabel{table:logreg-diffprec-stats}{{4}{26}{MNIST training and testing error (\%) for logistic regression experiment with different fractional bits for training.\relax }{table.caption.11}{}}
\newlabel{sec:dlexp}{{I}{26}{Acknowledgements}{appendix.1.I}{}}
\newlabel{sec:data-figure3}{{J}{27}{Acknowledgements}{appendix.1.J}{}}
\newlabel{table:figure3-data-diffc}{{\caption@xref {table:figure3-data-diffc}{ on input line 83}}{27}{Acknowledgements}{table.caption.12}{}}
\newlabel{table:figure3-data-qswa}{{\caption@xref {table:figure3-data-qswa}{ on input line 101}}{27}{Acknowledgements}{table.caption.13}{}}
\gdef \@abspage@last{27}
