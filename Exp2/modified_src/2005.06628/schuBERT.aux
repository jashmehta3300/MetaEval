\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{vaswani2017attention}
\citation{devlin2018bert}
\citation{vaswani2017attention}
\citation{dai2015semi,peters2018deep,radford2018improving,howard2018universal}
\citation{vaswani2017attention,ott2018scaling}
\citation{devlin2018bert}
\citation{bowman2015large,williams2017broad}
\citation{strubell2018linguistically}
\citation{devlin2018bert}
\citation{vaswani2017attention}
\citation{yang2019xlnet}
\citation{liu2019roberta}
\citation{michel2019sixteen}
\citation{lan2019albert}
\citation{devlin2018bert}
\citation{vaswani2017attention}
\citation{devlin2018bert}
\citation{devlin2018bert}
\citation{devlin2018bert}
\citation{devlin2018bert}
\citation{vaswani2017attention}
\citation{vaswani2017attention}
\citation{lecun1990optimal,hassibi1993second}
\citation{han2015deep}
\citation{li2016pruning,molchanov2016pruning}
\citation{anwar2017structured}
\citation{murray2015auto}
\citation{see2016compression}
\citation{kim2016sequence}
\citation{pham2018efficient,liu2018darts,singh2019darc}
\citation{rastegari2016xnor,hubara2017quantized}
\citation{zhu2016trained}
\citation{han2015deep}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:bert_ablation}{{1}{2}{Ablation study over BERT model size, Table $6$ in \citet {devlin2018bert}. $\#$M denotes number of model parameters in millions.\relax }{table.caption.1}{}}
\citation{sanh2019distilbert}
\citation{devlin2018bert}
\citation{michel2019sixteen,lan2019albert}
\citation{devlin2018bert}
\citation{vaswani2017attention}
\newlabel{fig:bert1}{{1}{3}{BERT pre-training\relax }{figure.caption.2}{}}
\newlabel{tab:elem1}{{2}{3}{Elements of BERT\relax }{table.caption.3}{}}
\citation{vaswani2017attention}
\newlabel{tab:elem2}{{3}{4}{Elements of schuBERT\relax }{table.caption.4}{}}
\newlabel{fig:bert2}{{2}{4}{An encoder layer of schuBERT\relax }{figure.caption.5}{}}
\citation{yang2019xlnet}
\citation{liu2019roberta}
\citation{lan2019albert}
\citation{devlin2018bert}
\newlabel{tab:prune1}{{4}{5}{Prunable parameters.\relax }{table.caption.6}{}}
\newlabel{tab:prune2}{{5}{5}{Prunable BERT parameter matrices/tensors.\relax }{table.caption.7}{}}
\newlabel{eq:org_loss}{{1}{6}{Optimization Method}{equation.5.1}{}}
\newlabel{eq:prune_loss}{{2}{6}{Optimization Method}{equation.5.2}{}}
\newlabel{algo:algo1}{{1}{6}{Pruning Transformers\relax }{algorithm.1}{}}
\citation{michel2019sixteen}
\newlabel{tab:results_99}{{6}{7}{Accuracy results on SQuAD and GLUE datasets obtained by fine-tuning BERT and schuBERTs with total of $99$ million parameters.\relax }{table.caption.8}{}}
\newlabel{tab:arch_99}{{7}{7}{Design dimensions of schuBERT-all for $99$ million parameters.\relax }{table.caption.9}{}}
\citation{vaswani2017attention}
\citation{vaswani2017attention}
\citation{vaswani2017attention}
\newlabel{tab:results_88}{{8}{8}{Accuracy results on SQuAD and GLUE datasets obtained by fine-tuning BERT, ALBERT, and schuBERTs with total of $88$ million parameters. \relax }{table.caption.10}{}}
\newlabel{tab:arch_88}{{9}{8}{Design dimensions of schuBERT-all for $88$ million parameters.\relax }{table.caption.11}{}}
\newlabel{tab:results_77}{{10}{9}{Accuracy results on SQuAD and GLUE datasets obtained by fine-tuning BERT and schuBERTs with total of $77$ million parameters. \relax }{table.caption.12}{}}
\newlabel{tab:results_66}{{11}{9}{Accuracy results on SQuAD and GLUE datasets obtained by fine-tuning BERT and schuBERTs with total of $66$ million parameters. \relax }{table.caption.13}{}}
\newlabel{fig:hidden}{{3}{9}{Feed-forward size across the encoder layers in schuBERT-all for various model sizes. \relax }{figure.caption.17}{}}
\newlabel{tab:results_55}{{12}{10}{Accuracy results on SQuAD and GLUE datasets obtained by fine-tuning BERT and schuBERTs with total of $55$ million parameters. \relax }{table.caption.14}{}}
\newlabel{tab:results_43}{{13}{10}{Accuracy results on SQuAD and GLUE datasets obtained by fine-tuning BERT and schuBERTs with total of $43$ million parameters. \relax }{table.caption.15}{}}
\newlabel{tab:schuBERTs}{{14}{10}{Best schuBERT architectures for different number of model parameters. BERT base has $108$M parameters.\relax }{table.caption.16}{}}
\bibdata{bert}
\bibcite{anwar2017structured}{{1}{2017}{{Anwar et~al.}}{{Anwar, Hwang, and Sung}}}
\bibcite{bowman2015large}{{2}{2015}{{Bowman et~al.}}{{Bowman, Angeli, Potts, and Manning}}}
\bibcite{dai2015semi}{{3}{2015}{{Dai and Le}}{{}}}
\bibcite{devlin2018bert}{{4}{2018}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{han2015deep}{{5}{2015}{{Han et~al.}}{{Han, Mao, and Dally}}}
\bibcite{hassibi1993second}{{6}{1993}{{Hassibi and Stork}}{{}}}
\bibcite{howard2018universal}{{7}{2018}{{Howard and Ruder}}{{}}}
\bibcite{hubara2017quantized}{{8}{2017}{{Hubara et~al.}}{{Hubara, Courbariaux, Soudry, El-Yaniv, and Bengio}}}
\bibcite{kim2016sequence}{{9}{2016}{{Kim and Rush}}{{}}}
\bibcite{lan2019albert}{{10}{2019}{{Lan et~al.}}{{Lan, Chen, Goodman, Gimpel, Sharma, and Soricut}}}
\bibcite{lecun1990optimal}{{11}{1990}{{LeCun et~al.}}{{LeCun, Denker, and Solla}}}
\bibcite{li2016pruning}{{12}{2016}{{Li et~al.}}{{Li, Kadav, Durdanovic, Samet, and Graf}}}
\bibcite{liu2018darts}{{13}{2018}{{Liu et~al.}}{{Liu, Simonyan, and Yang}}}
\bibcite{liu2019roberta}{{14}{2019}{{Liu et~al.}}{{Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov}}}
\bibcite{michel2019sixteen}{{15}{2019}{{Michel et~al.}}{{Michel, Levy, and Neubig}}}
\bibcite{molchanov2016pruning}{{16}{2016}{{Molchanov et~al.}}{{Molchanov, Tyree, Karras, Aila, and Kautz}}}
\bibcite{murray2015auto}{{17}{2015}{{Murray and Chiang}}{{}}}
\bibcite{ott2018scaling}{{18}{2018}{{Ott et~al.}}{{Ott, Edunov, Grangier, and Auli}}}
\bibcite{peters2018deep}{{19}{2018}{{Peters et~al.}}{{Peters, Neumann, Iyyer, Gardner, Clark, Lee, and Zettlemoyer}}}
\bibcite{pham2018efficient}{{20}{2018}{{Pham et~al.}}{{Pham, Guan, Zoph, Le, and Dean}}}
\bibcite{radford2018improving}{{21}{2018}{{Radford et~al.}}{{Radford, Narasimhan, Salimans, and Sutskever}}}
\bibcite{rastegari2016xnor}{{22}{2016}{{Rastegari et~al.}}{{Rastegari, Ordonez, Redmon, and Farhadi}}}
\bibcite{sanh2019distilbert}{{23}{2019}{{Sanh et~al.}}{{Sanh, Debut, Chaumond, and Wolf}}}
\bibcite{see2016compression}{{24}{2016}{{See et~al.}}{{See, Luong, and Manning}}}
\bibcite{singh2019darc}{{25}{2019}{{Singh et~al.}}{{Singh, Khetan, and Karnin}}}
\bibcite{strubell2018linguistically}{{26}{2018}{{Strubell et~al.}}{{Strubell, Verga, Andor, Weiss, and McCallum}}}
\bibcite{vaswani2017attention}{{27}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{williams2017broad}{{28}{2017}{{Williams et~al.}}{{Williams, Nangia, and Bowman}}}
\bibcite{yang2019xlnet}{{29}{2019}{{Yang et~al.}}{{Yang, Dai, Yang, Carbonell, Salakhutdinov, and Le}}}
\bibcite{zhu2016trained}{{30}{2016}{{Zhu et~al.}}{{Zhu, Han, Mao, and Dally}}}
\bibstyle{acl_natbib}
\newlabel{sec:appendix}{{A}{12}{Appendix}{appendix.A}{}}
\newlabel{fig:heads}{{4}{12}{Number of multi-attention heads across the encoder layers in schuBERT-all for various model sizes. \relax }{figure.caption.19}{}}
\newlabel{fig:key}{{5}{12}{Dimension of key-query vectors across the encoder layers in schuBERT-all for various model sizes. \relax }{figure.caption.20}{}}
\newlabel{fig:value}{{6}{12}{Dimension of value vectors across the encoder layers in schuBERT-all for various model sizes. \relax }{figure.caption.21}{}}
\gdef \@abspage@last{12}
