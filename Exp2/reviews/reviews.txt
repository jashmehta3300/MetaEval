1_2012.04715 = This paper proposes a new SAT solver to solve the problem of determining the existence of a projective plane of order 10 . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates were generated with the assistance of the symbolic computation library Traces ( McKay and Piperno 2014 ) , but it is not clear how one can verify these certificates without needing to trust the output of the library . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that their work is the first SAT-based resolution of Lamâs problem with all of the exhaustive search work completed by SAT solvers . I am not sure if this statement is correct . The previous SAT-SAT solvers have become so strong that they are “ the best solution in most cases ” . Even still , they note that some problems such as some problems like Lam’s Problem have only been solved by specialpurpose means . 2 .In the proof of Theorem 3.1 , the authors assume that the code associated with projective planes of order ten must contain words that are referred to as weight 15 , weight 16 , or ‘ weight 19 ’ words . The former two cases were first ruled out by the searches of ( Lam , Thiel , and Swiercz 1989 ) and ( MacWilliams , Sloane , and Thompson ) and were recently settled via SAT-Based Existence certificates ( Bright et al. , 2020a ,b ) . It would be better if the authors can clarify the difference between their work and these previous works . 3 .The proof of theorem 3.2 is not very clear . It seems that the proof is based on the fact that there are 66 possible A1s and A2s , which is not true for the case of A2 . It is better to clarify this point . 4 .The authors should provide more details about the verification procedure . For example , how to verify the correctness of the certificates ? 5 .The paper is not well-organized . For instance , it is hard to follow the description of the proof in Section 4.1 .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and the Weisfeiler-Lehman ( WL ) graph isomorphism test . The authors show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of nodes as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNN is thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . The paper rigorously study several variants of multisett functions and theoretically characterize their discriminativity power . The results show that under certain conditions , GNN-based graph structures that can not be distinguished by popular GNN variants , such as GCN ( Kipf & Welling , 2017 ) and GraphSAGE ( Hamilton et al. , 2017a ) , and show that the power of GIN ( Santoro et al .2017 ) is equal to that of WL .The authors also develop a simple neural architecture , Graph Isomorphism Network ( GIN ) , which can capture graph structures such as graph structure . The proposed GIN can achieve state-of-the-art performance on node classification , link prediction , and graph classification tasks . However , there is little theoretical understanding of the properties and limitations of GNN , and formal analysis of the representational capacity is limited . Therefore , the design of new GNN based models is mostly based on empirical intuition , heuristics , and experimental trial-anderror .
1_1907.07355 = This paper investigates the performance of BERT on the ARCT dataset . The authors show that the model is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated by adding a copy of each data point with the claim negated and the label inverted . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that BERT ’ s performance can be entirely accounted for in terms of exploiting spurious statistical cues , but they do not provide any evidence to support this claim . 2 .In Table 1 , BERT achieves 77 % test set accuracy with its best run ( Table 1 ) , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question “ what has BERT learned about argument comprehension ? ” 3 .In the experiments , the authors only compare BERT with the SemEval dataset . It would be better if the authors can also compare with the proposed adversarial dataset , which is more challenging .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The proposed method is simple to implement and has little computational overhead . The paper is well written and easy to follow . The authors prove that for quadratic objective functions , SWA can converge to the optimal solution as well as to a smaller asymptotic noise ball than low precision SGD for strongly convex objectives . Empirically , the authors show that training with 8 bit SWA with full precision can match the full precision baseline in deep learning tasks such as ResNet-164 and CIFAR-10/100 datasets . The experimental results show that SWA outperforms the state-of-the-art in terms of generalization performance . Pros : 1 . The idea of using SWA to reduce the quantization noise during training is interesting and novel . 2 .The paper is clearly written . 3 .The experimental results are convincing . 4 .The theoretical analysis is sound . Cons : 1.1 . It is not clear to me why SWA works well with a relatively high learning rate and can tolerate additional noise . It would be better to show the performance of SWA when the learning rate is low . 2.2 .It would be more convincing if the authors can show the convergence rate of the proposed method with a fixed learning rate . 3.3 .It is better to compare the performance with other quantization methods such as quantized SGD . 4.4 .There are some typos and grammatical errors in the paper .
1_1902.04911 = This paper proposes a novel approach to knowledge-grounded dialogue generation . The authors propose to separate the posterior distribution over knowledge from the prior distribution over utterance and response . The proposed model is trained to minimize the KL divergence between the prior and posterior distributions . Then , during the inference process , the model samples knowledge merely based on the prior prior distribution and incorporates the sampled knowledge into response generation . It is proved that this model can effectively learn to generate proper and informative responses by utilizing appropriate knowledge by utilizing knowledge . The paper is well-written and easy to follow . The idea of separating the posterior and prior distributions over knowledge is interesting and novel . However , I have the following concerns . 1 .The authors claim that the proposed method is able to avoid the discrepancy between the posterior distributions over utterances and responses . But , it is not clear to me how the model can avoid this discrepancy . For example , given an utterance , different responses can be generated depending on whether appropriate knowledge is used in the response . If the model is training by selecting wrong knowledge ( e.g. , K2 in R2 ) or knowledge irrelevant to the true response ( i.e .K1 in R1 ) , it can be seen that they are completely useless since they can not provide any helpful information . This kind of discrepancy would stop the model from learning to generating proper responses by using appropriate knowledge . 2 .The proposed approach is not compared with other related works . For instance , [ Dinan et al. , 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 3 .In Table 1 , there is no comparison with [ Ghazvininejad et al .2018 ] , which also uses knowledge selection to guide the knowledge selection . 4 .It is also important to properly incorporate knowledge in response generation , such as K1 and K3 in R4 in the Wizard of Wikipedia dataset . 5 .The paper is not well-motivated . The motivation of this paper is to solve the problem of knowledge-based dialogue generation , which has not been sufficiently studied in the previous work . 6 .There are many typos in the paper . For the example in Section 3.1 , the sentence `` In this dataset , each agent is associated with a persona profile , which is served as knowledge . `` should be removed .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea of the paper is to prune the architecture of the model by optimizing the number of parameters and the pre-training loss . The experiments show that the proposed pruning method can improve the performance of BERT . The paper is well written and easy to follow . However , I have some concerns about the experiments . 1 .The experiments are only conducted on MNLI dataset . It would be more convincing if the authors can conduct experiments on other NLP tasks . 2 .The pruning based architecture search is only applied to BERT architecture . It is not clear how the pruning can be applied to other models . 3 .The authors should compare with other pruning methods , e.g. , [ 1 ] , [ 2 ] and [ 3 ] . 4 .It would be interesting to see the results of pruning BERT on other language modeling tasks , such as NLP , machine translation , etc . 5 .The paper is not well organized . For example , in the introduction , the authors claim that “ The ratio of the architecture design dimensions within a BERT encoder layer can be modified to obtain a layer with better performance . ” .However , the paper does not explain how this can be done . 6 .In the experiments , it would be better to show the results on other tasks . 7 .In Table 1 , it is better to report the accuracy of the pruned BERT models . 8 .The results on Table 2 are not convincing . For instance , the results in Table 1 are not consistent with the results reported in the paper . 9 .In Figure 2 , why the results are not better than the ones reported in [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,47,48,50,49,51,52,53,54,55,56,58,59,60,61,62,63,64,70,72,73,72 ,73,74,75,76,80,83,84,81,88,89,90,91,93,90 ,93,94,94 ,94,96,98,99,99 ,98,101,102,103,103 ,104,104 ,104 ,105,104,105,106 ,107,108 ,108 ,109 ,110 ,111 ,112 ,113 ,114 ,115 ,116 ,117 ,118 ,119 ,122 ,123 , 123 , 128 , 130 , 131 , 128,129 , 130,131 ,131 ,132 ,133 ,134 ,135 ,136 ,137 ,138 ,139 ,140 ,141 ,142 ,143 ,144 ,145 ,150 ,153 ,154 ,155 ,156 ,157 ,163 ,168 ,178 ,179 ,180 ,181 ,183 ,184 ,185 ,186 ,197 ,198 ,199 ,207 ,208 ,207 . 10 .In Section 3.1 , it seems that the authors did not compare with the existing pruning techniques in the literature . It will be better if the author can compare with them . 11 .In section 3.2 , the author claims that ‘ The fully-connected component applied to each token separately plays a much more significant role in the top layers as compared to the bottom layers ’ . This is not true . In fact , the fully connected component is not the same as the attention head . 12 .In table 1 , the result of ‘ tall and narrow ’ architecture is better than ‘ wide and shallow ’ and ‘ shallow and tall ’ architectures . 13 .In figure 2 , the difference between ‘ top and bottom ’ is not very clear . 14 .The author should provide more details about the hyper-parameters of the proposed method . 15 .The proposed method is not compared with the other pruned methods . The author should also compare with these methods .
1_2102.07983 = This paper introduces a new dataset , FEWS ( Few-shot Examples of Word Senses ) , to train and evaluate word sense disambiguation models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary , which contains examples for over 71,000 senses , which is much higher than existing datasets such as SemCor , the largest manually annotated WSD dataset , which only covers about 33,000 . The authors also conduct experiments on knowledge-based approaches and a recent neural biencoder model for WSD . The paper is well-written and easy to follow . The proposed dataset is a good addition to the literature on WSD , and the experiments are well-conducted . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset for training and evaluating WSD models in a few- and zero shot setting . The contribution of the dataset is two-fold : ( 1 ) it exposes models a broad array of senses in a low-shot setting , and ( 2 ) the large evaluation set allows for more robust evaluation of rare senses . 2 .The experiments are conducted on knowledge based approaches and the recent Neural BienCoder model , which are the strongest baselines . However it is not clear whether the proposed dataset can be used as additional training data for other WSD tasks such as transfer learning . It would be better if the authors can show the performance of the proposed datasets on transfer learning tasks .
1_1812.02425 = This paper proposes to use a teacher-student ensembling framework to learn an ensemble of multiple neural networks without incurring any additional testing costs . The idea is to use the outputs of different neural networks as supervisions to guide the target network training . To further improve the robustness of the student networks , the authors introduce an adversarial learning strategy to force the student to generate similar outputs as teachers . The experiments show that MEAL consistently improves the accuracy across a variety of popular network architectures on different datasets . The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a number of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . It is a simple extension of Swapout ( Singh , Hoiem , and Forsyth 2016 ) . 2 .The proposed method can achieve the goal of ensembleling multiple neural network with no additional testing cost . 3 .The experimental results are not convincing . For example , in Table 1 , the performance of Shake-Shake ( Gastaldi 2017 ) and MEAL ( Meals et al. , 2017 ) are not better than Shake-shake and Meals , respectively , in terms of accuracy .
1_1901.06829 = This paper proposes a novel method for grounding natural language descriptions in videos . The proposed method is based on an end-to-end reinforcement learning ( RL ) framework , where the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . At each time step , the agent makes an observation that details the currently selected video clip , and takes an action according to its policy . The environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . In addition , the authors introduce a location feature , which enables the agent to explicitly access the current grounding boundaries . The paper is well-written and easy to follow . The method is novel and well-motivated . The experimental results show that the proposed method outperforms the state-of-the-art methods . However , I have the following concerns : 1 . The novelty of the method is limited . There are several existing methods for video grounding , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) , which can be applied only when all candidates are exhaustively enumerated , as they need to rank all possible clip-sentence pairs to be successful in generating the final grounding result . Moreover , the ranking strategy is specially designed for pre-segmented videos and is not applicable to general cases . 2 .The proposed method can not be applied to general video grounding tasks . 3 .The experiments are not convincing . For example , in Table 1 , the performance of the proposed methods is not better than the baseline methods . 4 .In Table 2 , the results are not consistent with the results reported in the original paper .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a new SAT solver to solve the problem of determining the existence of a projective plane of order 10 . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates were generated with the assistance of the symbolic computation library Traces ( McKay and Piperno 2014 ) , but it is not clear how one can verify these certificates without needing to trust the output of the library . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that their work is the first SAT-based resolution of Lamâs problem with all of the exhaustive search work completed by SAT solvers . I am not sure if this statement is correct . The previous SAT-SAT solvers have become so strong that they are “ the best solution in most cases ” . Even still , they note that some problems such as some problems like Lam’s Problem have only been solved by specialpurpose means . 2 .In the proof of Theorem 3.1 , the authors assume that the code associated with projective planes of order ten must contain words that are referred to as weight 15 , weight 16 , or ‘ weight 19 ’ words . The former two cases were first ruled out by the searches of ( Lam , Thiel , and Swiercz 1989 ) and ( MacWilliams , Sloane , and Thompson ) and were recently settled via SAT-Based Existence certificates ( Bright et al. , 2020a ,b ) . It would be better if the authors can clarify the difference between their work and these previous works . 3 .The proof of theorem 3.2 is not very clear . It seems that the proof is based on the fact that there are 66 possible A1s and A2s , which is not true for the case of A2 . It is better to clarify this point . 4 .The authors should provide more details about the verification procedure . For example , how to verify the correctness of the certificates ? 5 .The paper is not well-organized . For instance , it is hard to follow the description of the proof in Section 4.1 .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and the Weisfeiler-Lehman ( WL ) graph isomorphism test . The authors show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of nodes as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNN is thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . The paper rigorously study several variants of multisett functions and theoretically characterize their discriminativity power . The results show that under certain conditions , GNN-based graph structures that can not be distinguished by popular GNN variants , such as GCN ( Kipf & Welling , 2017 ) and GraphSAGE ( Hamilton et al. , 2017a ) , and show that the power of GIN ( Santoro et al .2017 ) is equal to that of WL .The authors also develop a simple neural architecture , Graph Isomorphism Network ( GIN ) , which can capture graph structures such as graph structure . The proposed GIN can achieve state-of-the-art performance on node classification , link prediction , and graph classification tasks . However , there is little theoretical understanding of the properties and limitations of GNN , and formal analysis of the representational capacity is limited . Therefore , the design of new GNN based models is mostly based on empirical intuition , heuristics , and experimental trial-anderror .
1_1907.07355 = This paper investigates the performance of BERT on the ARCT dataset . The authors show that the model is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated by adding a copy of each data point with the claim negated and the label inverted . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that BERT ’ s performance can be entirely accounted for in terms of exploiting spurious statistical cues , but they do not provide any evidence to support this claim . 2 .In Table 1 , BERT achieves 77 % test set accuracy with its best run ( Table 1 ) , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question “ what has BERT learned about argument comprehension ? ” 3 .In the experiments , the authors only compare BERT with the SemEval dataset . It would be better if the authors can also compare with the proposed adversarial dataset , which is more challenging .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The proposed method is simple to implement and has little computational overhead . The paper is well written and easy to follow . The authors prove that for quadratic objective functions , SWA can converge to the optimal solution as well as to a smaller asymptotic noise ball than low precision SGD for strongly convex objectives . Empirically , the authors show that training with 8 bit SWA with full precision can match the full precision baseline in deep learning tasks such as ResNet-164 and CIFAR-10/100 datasets . The experimental results show that SWA outperforms the state-of-the-art in terms of generalization performance . Pros : 1 . The idea of using SWA to reduce the quantization noise during training is interesting and novel . 2 .The paper is clearly written . 3 .The experimental results are convincing . 4 .The theoretical analysis is sound . Cons : 1.1 . It is not clear to me why SWA works well with a relatively high learning rate and can tolerate additional noise . It would be better to show the performance of SWA when the learning rate is low . 2.2 .It would be more convincing if the authors can show the convergence rate of the proposed method with a fixed learning rate . 3.3 .It is better to compare the performance with other quantization methods such as quantized SGD . 4.4 .There are some typos and grammatical errors in the paper .
1_1902.04911 = This paper proposes a novel approach to knowledge-grounded dialogue generation . The authors propose to separate the posterior distribution over knowledge from the prior distribution over utterance and response . The proposed model is trained to minimize the KL divergence between the prior and posterior distributions . Then , during the inference process , the model samples knowledge merely based on the prior prior distribution and incorporates the sampled knowledge into response generation . It is proved that this model can effectively learn to generate proper and informative responses by utilizing appropriate knowledge by utilizing knowledge . The paper is well-written and easy to follow . The idea of separating the posterior and prior distributions over knowledge is interesting and novel . However , I have the following concerns . 1 .The authors claim that the proposed method is able to avoid the discrepancy between the posterior distributions over utterances and responses . But , it is not clear to me how the model can avoid this discrepancy . For example , given an utterance , different responses can be generated depending on whether appropriate knowledge is used in the response . If the model is training by selecting wrong knowledge ( e.g. , K2 in R2 ) or knowledge irrelevant to the true response ( i.e .K1 in R1 ) , it can be seen that they are completely useless since they can not provide any helpful information . This kind of discrepancy would stop the model from learning to generating proper responses by using appropriate knowledge . 2 .The proposed approach is not compared with other related works . For instance , [ Dinan et al. , 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 3 .In Table 1 , there is no comparison with [ Ghazvininejad et al .2018 ] , which also uses knowledge selection to guide the knowledge selection . 4 .It is also important to properly incorporate knowledge in response generation , such as K1 and K3 in R4 in the Wizard of Wikipedia dataset . 5 .The paper is not well-motivated . The motivation of this paper is to solve the problem of knowledge-based dialogue generation , which has not been sufficiently studied in the previous work . 6 .There are many typos in the paper . For the example in Section 3.1 , the sentence `` In this dataset , each agent is associated with a persona profile , which is served as knowledge . `` should be removed .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea of the paper is to prune the architecture of the model by optimizing the number of parameters and the pre-training loss . The experiments show that the proposed pruning method can improve the performance of BERT . The paper is well written and easy to follow . However , I have some concerns about the experiments . 1 .The experiments are only conducted on MNLI dataset . It would be more convincing if the authors can conduct experiments on other NLP tasks . 2 .The pruning based architecture search is only applied to BERT architecture . It is not clear how the pruning can be applied to other models . 3 .The authors should compare with other pruning methods , e.g. , [ 1 ] , [ 2 ] and [ 3 ] . 4 .It would be interesting to see the results of pruning BERT on other language modeling tasks , such as NLP , machine translation , etc . 5 .The paper is not well organized . For example , in the introduction , the authors claim that “ The ratio of the architecture design dimensions within a BERT encoder layer can be modified to obtain a layer with better performance . ” .However , the paper does not explain how this can be done . 6 .In the experiments , it would be better to show the results on other tasks . 7 .In Table 1 , it is better to report the accuracy of the pruned BERT models . 8 .The results on Table 2 are not convincing . For instance , the results in Table 1 are not consistent with the results reported in the paper . 9 .In Figure 2 , why the results are not better than the ones reported in [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,47,48,50,49,51,52,53,54,55,56,58,59,60,61,62,63,64,70,72,73,72 ,73,74,75,76,80,83,84,81,88,89,90,91,93,90 ,93,94,94 ,94,96,98,99,99 ,98,101,102,103,103 ,104,104 ,104 ,105,104,105,106 ,107,108 ,108 ,109 ,110 ,111 ,112 ,113 ,114 ,115 ,116 ,117 ,118 ,119 ,122 ,123 , 123 , 128 , 130 , 131 , 128,129 , 130,131 ,131 ,132 ,133 ,134 ,135 ,136 ,137 ,138 ,139 ,140 ,141 ,142 ,143 ,144 ,145 ,150 ,153 ,154 ,155 ,157 ,164 ,166 ,166,166 ,167 ,168 ,169 ,170 ,170 .
1_2102.07983 = This paper introduces a new dataset , FEWS ( Few-shot Examples of Word Senses ) , to train and evaluate word sense disambiguation models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary , which contains examples for over 71,000 senses , which is much higher than existing datasets such as SemCor , the largest manually annotated WSD dataset , which only covers about 33,000 . The authors also conduct experiments on knowledge-based approaches and a recent neural biencoder model for WSD . The paper is well-written and easy to follow . The proposed dataset is a good addition to the literature on WSD , and the experiments are well-conducted . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset for training and evaluating WSD models in a few- and zero shot setting . The contribution of the dataset is two-fold : ( 1 ) it exposes models a broad array of senses in a low-shot setting , and ( 2 ) the large evaluation set allows for more robust evaluation of rare senses . 2 .The experiments are conducted on knowledge based approaches and the recent Neural BienCoder model , which are the strongest baselines . However it is not clear whether the proposed dataset can be used as additional training data for other WSD tasks such as transfer learning . It would be better if the authors can show the performance of the proposed datasets on transfer learning tasks .
1_1812.02425 = This paper proposes to use a teacher-student ensembling framework to learn an ensemble of multiple neural networks without incurring any additional testing costs . The idea is to use the outputs of different neural networks as supervisions to guide the target network training . To further improve the robustness of the student networks , the authors introduce an adversarial learning strategy to force the student to generate similar outputs as teachers . The experiments show that MEAL consistently improves the accuracy across a variety of popular network architectures on different datasets . The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a number of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . It is a simple extension of Swapout ( Singh , Hoiem , and Forsyth 2016 ) . 2 .The proposed method can achieve the goal of ensembleling multiple neural network with no additional testing cost . 3 .The experimental results are not convincing . For example , in Table 1 , the performance of Shake-Shake ( Gastaldi 2017 ) and MEAL ( Meals et al. , 2017 ) are not better than Shake-shake and Meals , respectively , in terms of accuracy .
1_1901.06829 = This paper proposes a novel method for grounding natural language descriptions in videos . The proposed method is based on an end-to-end reinforcement learning ( RL ) framework , where the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . At each time step , the agent makes an observation that details the currently selected video clip , and takes an action according to its policy . The environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . In addition , the authors introduce a location feature , which enables the agent to explicitly access the current grounding boundaries . The paper is well-written and easy to follow . The method is novel and well-motivated . The experimental results show that the proposed method outperforms the state-of-the-art methods . However , I have the following concerns : 1 . The novelty of the method is limited . There are several existing methods for video grounding , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) , which can be applied only when all candidates are exhaustively enumerated , as they need to rank all possible clip-sentence pairs to be successful in generating the final grounding result . Moreover , the ranking strategy is specially designed for pre-segmented videos and is not applicable to general cases . 2 .The proposed method can not be applied to general video grounding tasks . 3 .The experiments are not convincing . For example , in Table 1 , the performance of the proposed methods is not better than the baseline methods . 4 .In Table 2 , the results are not consistent with the results reported in the original paper .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a new SAT solver to solve the problem of determining the existence of a projective plane of order 10 . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates were generated with the assistance of the symbolic computation library Traces ( McKay and Piperno 2014 ) , but it is not clear how one can verify these certificates without needing to trust the output of the library . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that their work is the first SAT-based resolution of Lamâs problem with all of the exhaustive search work completed by SAT solvers . I am not sure if this statement is correct . The previous SAT-SAT solvers have become so strong that they are “ the best solution in most cases ” . Even still , they note that some problems such as some problems like Lam’s Problem have only been solved by specialpurpose means . 2 .In the proof of Theorem 3.1 , the authors assume that the code associated with projective planes of order ten must contain words that are referred to as weight 15 , weight 16 , or ‘ weight 19 ’ words . The former two cases were first ruled out by the searches of ( Lam , Thiel , and Swiercz 1989 ) and ( MacWilliams , Sloane , and Thompson ) and were recently settled via SAT-Based Existence certificates ( Bright et al. , 2020a ,b ) . It would be better if the authors can clarify the difference between their work and these previous works . 3 .The proof of theorem 3.2 is not very clear . It seems that the proof is based on the fact that there are 66 possible A1s and A2s , which is not true for the case of A2 . It is better to clarify this point . 4 .The authors should provide more details about the verification procedure . For example , how to verify the correctness of the certificates ? 5 .The paper is not well-organized . For instance , it is hard to follow the description of the proof in Section 4.1 .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and the Weisfeiler-Lehman ( WL ) graph isomorphism test . The authors show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of nodes as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNN is thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . The paper rigorously study several variants of multisett functions and theoretically characterize their discriminativity power . The results show that under certain conditions , GNN-based graph structures that can not be distinguished by popular GNN variants , such as GCN ( Kipf & Welling , 2017 ) and GraphSAGE ( Hamilton et al. , 2017a ) , and show that the power of GIN ( Santoro et al .2017 ) is equal to that of WL .The authors also develop a simple neural architecture , Graph Isomorphism Network ( GIN ) , which can capture graph structures such as graph structure . The proposed GIN can achieve state-of-the-art performance on node classification , link prediction , and graph classification tasks . However , there is little theoretical understanding of the properties and limitations of GNN , and formal analysis of the representational capacity is limited . Therefore , the design of new GNN based models is mostly based on empirical intuition , heuristics , and experimental trial-anderror .
1_1907.07355 = This paper investigates the performance of BERT on the ARCT dataset . The authors show that the model is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated by adding a copy of each data point with the claim negated and the label inverted . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that BERT ’ s performance can be entirely accounted for in terms of exploiting spurious statistical cues , but they do not provide any evidence to support this claim . 2 .In Table 1 , BERT achieves 77 % test set accuracy with its best run ( Table 1 ) , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question “ what has BERT learned about argument comprehension ? ” 3 .In the experiments , the authors only compare BERT with the SemEval dataset . It would be better if the authors can also compare with the proposed adversarial dataset , which is more robust .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The proposed method is simple to implement and has little computational overhead . The paper is well written and easy to follow . The authors prove that for quadratic objective functions , SWA can converge to the optimal solution as well as to a smaller asymptotic noise ball than low precision SGD for strongly convex objectives . Empirically , the authors show that training with 8 bit SWA with full precision can match the full precision baseline in deep learning tasks such as ResNet-164 and CIFAR-10/100 datasets . The experimental results show that SWA outperforms the state-of-the-art in terms of generalization performance . Pros : 1 . The idea of using SWA to reduce the quantization noise during training is interesting and novel . 2 .The paper is clearly written . 3 .The experimental results are convincing . 4 .The theoretical analysis is sound . Cons : 1.1 . It is not clear to me why SWA works well with a relatively high learning rate and can tolerate additional noise . It would be better to show the performance of SWA when the learning rate is low . 2.2 .It would be more convincing if the authors can show the convergence rate of the proposed method with a fixed learning rate . 3.3 .It is better to compare the performance with other quantization methods such as quantized SGD . 4.4 .There are some typos and grammatical errors in the paper .
1_1902.04911 = This paper proposes a novel approach to knowledge-grounded dialogue generation . The authors propose to separate the posterior distribution over knowledge from the prior distribution over utterance and response . The proposed model is trained to minimize the KL divergence between the prior and posterior distributions . Then , during the inference process , the model samples knowledge merely based on the prior prior distribution and incorporates the sampled knowledge into response generation . It is proved that this model can effectively learn to generate proper and informative responses by utilizing appropriate knowledge by utilizing knowledge . The paper is well-written and easy to follow . The idea of separating the posterior and prior distributions over knowledge is interesting and novel . However , I have the following concerns . 1 .The authors claim that the proposed method is able to avoid the discrepancy between the posterior distributions over utterances and responses . But , it is not clear to me how the model can avoid this discrepancy . For example , given an utterance , different responses can be generated depending on whether appropriate knowledge is used in the response . If the model is training by selecting wrong knowledge ( e.g. , K2 in R2 ) or knowledge irrelevant to the true response ( i.e .K1 in R1 ) , it can be seen that they are completely useless since they can not provide any helpful information . This kind of discrepancy would stop the model from learning to generating proper responses by using appropriate knowledge . 2 .The proposed approach is not compared with other related works . For instance , [ Dinan et al. , 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 3 .In Table 1 , there is no comparison with [ Ghazvininejad et al .2018 ] , which also uses knowledge selection to guide the knowledge selection . 4 .It is also important to properly incorporate knowledge in response generation , such as K1 and K3 in R4 in the Wizard of Wikipedia dataset . 5 .The paper is not well-motivated . The motivation of this paper is to solve the problem of knowledge-based dialogue generation , which has not been sufficiently studied in the previous work . 6 .There are many typos in the paper . For the example in Section 3.1 , the sentence `` In this dataset , each agent is associated with a persona profile , which is served as knowledge . `` should be removed .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea of the paper is to prune the architecture of the model by optimizing the number of parameters and the pre-training loss . The experiments show that the proposed pruning method can improve the performance of BERT . The paper is well written and easy to follow . However , I have some concerns about the experiments . 1 .The experiments are only conducted on MNLI dataset . It would be more convincing if the authors can conduct experiments on other NLP tasks . 2 .The pruning based architecture search is only applied to BERT architecture . It is not clear how the pruning can be applied to other models . 3 .The authors should compare with other pruning methods , e.g. , [ 1 ] , [ 2 ] and [ 3 ] . 4 .It would be interesting to see the results of pruning BERT on other language modeling tasks , such as NLP , machine translation , etc . 5 .The paper is not well organized . For example , in the introduction , the authors claim that “ The ratio of the architecture design dimensions within a BERT encoder layer can be modified to obtain a layer with better performance . ” .However , the paper does not explain how this can be done . 6 .In the experiments , it would be better to show the results on other tasks . 7 .In Table 1 , it is better to report the accuracy of the pruned BERT models . 8 .The results on Table 2 are not convincing . For instance , the results in Table 1 are not consistent with the results reported in the paper . 9 .In Figure 2 , why the results are not better than the ones reported in [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,47,48,50,49,51,52,53,54,55,56,58,59,60,61,62,63,64,70,72,73,72 ,73,74,75,76,80,83,84,81,88,89,90,91,93,90 ,93,94,94 ,94,96,98,99,99 ,98,101,102,103,103 ,104,104 ,104 ,105,104,105,106 ,107,108 ,108 ,109 ,110 ,111 ,112 ,113 ,114 ,115 ,116 ,117 ,118 ,119 ,122 ,123 , 123 , 128 , 130 , 131 , 128,129 , 130,131 ,131 ,132 ,133 ,134 ,135 ,136 ,137 ,138 ,139 ,140 ,141 ,142 ,143 ,144 ,145 ,150 ,153 ,154 ,155 ,156 ,157 ,163 ,168 ,178 ,179 ,180 ,181 ,183 ,184 ,185 ,186 ,197 ,198 ,199 ,207 ,208 ,207 . 10 .In Section 3.1 , it seems that the authors did not compare with the existing pruning techniques in the literature . It will be better if the author can compare with them . 11 .In section 3.2 , the author claims that ‘ The fully-connected component applied to each token separately plays a much more significant role in the top layers as compared to the bottom layers ’ . This is not true . In fact , the fully connected component is not the same as the attention head . 12 .In table 1 , the result of ‘ tall and narrow ’ architecture is better than ‘ wide and shallow ’ and ‘ shallow and tall ’ architectures . 13 .In figure 2 , the difference between ‘ top and bottom ’ is not very clear . 14 .The author should provide more details about the hyper-parameters of the proposed method . 15 .The proposed method is not compared with the other pruned methods . The author should also compare with these methods .
1_2102.07983 = This paper introduces a new dataset , FEWS ( Few-shot Examples of Word Senses ) , to train and evaluate word sense disambiguation models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary , which contains examples for over 71,000 senses , which is much higher than existing datasets such as SemCor , the largest manually annotated WSD dataset , which only covers about 33,000 . The authors also conduct experiments on knowledge-based approaches and a recent neural biencoder model for WSD . The paper is well-written and easy to follow . The proposed dataset is a good addition to the literature on WSD , and the experiments are well-conducted . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset for training and evaluating WSD models in a few- and zero shot setting . The contribution of the dataset is two-fold : ( 1 ) it exposes models a broad array of senses in a low-shot setting , and ( 2 ) the large evaluation set allows for more robust evaluation of rare senses . 2 .The experiments are conducted on knowledge based approaches and the recent Neural BienCoder model , which are the strongest baselines . However it is not clear whether the proposed dataset can be used as additional training data for other WSD tasks such as transfer learning . It would be better if the authors can show the performance of the proposed datasets on transfer learning tasks .
1_1812.02425 = This paper proposes to use a teacher-student ensembling framework to learn an ensemble of multiple neural networks without incurring any additional testing costs . The idea is to use the outputs of different neural networks as supervisions to guide the target network training . To further improve the robustness of the student networks , the authors introduce an adversarial learning strategy to force the student to generate similar outputs as teachers . The experiments show that MEAL consistently improves the accuracy across a variety of popular network architectures on different datasets . The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a number of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . It is a simple extension of Swapout ( Singh , Hoiem , and Forsyth 2016 ) . 2 .The proposed method can achieve the goal of ensembleling multiple neural network with no additional testing cost . 3 .The experimental results are not convincing . For example , in Table 1 , the performance of Shake-Shake ( Gastaldi 2017 ) and MEAL ( Meals et al. , 2017 ) are not better than Shake-shake and Meals , respectively , in terms of accuracy .
1_1901.06829 = This paper proposes a novel method for grounding natural language descriptions in videos . The proposed method is based on an end-to-end reinforcement learning ( RL ) framework , where the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . At each time step , the agent makes an observation that details the currently selected video clip , and takes an action according to its policy . The environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . In addition , the authors introduce a location feature , which enables the agent to explicitly access the current grounding boundaries . The paper is well-written and easy to follow . The method is novel and well-motivated . The experimental results show that the proposed method outperforms the state-of-the-art methods . However , I have the following concerns : 1 . The novelty of the method is limited . There are several existing methods for video grounding , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) , which can be applied only when all candidates are exhaustively enumerated , as they need to rank all possible clip-sentence pairs to be successful in generating the final grounding result . Moreover , the ranking strategy is specially designed for pre-segmented videos and is not applicable to general cases . 2 .The proposed method can not be applied to general video grounding tasks . 3 .The experiments are not convincing . For example , in Table 1 , the performance of the proposed methods is not better than the baseline methods . 4 .In Table 2 , the results are not consistent with the results reported in the original paper .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a new SAT solver to solve the problem of determining the existence of a projective plane of order 10 . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates were generated with the assistance of the symbolic computation library Traces ( McKay and Piperno 2014 ) , but it is not clear how one can verify these certificates without needing to trust the output of the library . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that their work is the first SAT-based resolution of Lamâs problem with all of the exhaustive search work completed by SAT solvers . I am not sure if this statement is correct . The previous SAT-SAT solvers have become so strong that they are “ the best solution in most cases ” . Even still , they note that some problems such as some problems like Lam’s Problem have only been solved by specialpurpose means . 2 .In the proof of Theorem 3.1 , the authors assume that the code associated with projective planes of order ten must contain words that are referred to as weight 15 , weight 16 , or ‘ weight 19 ’ words . The former two cases were first ruled out by the searches of ( Lam , Thiel , and Swiercz 1989 ) and ( MacWilliams , Sloane , and Thompson ) and were recently settled via SAT-Based Existence certificates ( Bright et al. , 2020a ,b ) . It would be better if the authors can clarify the difference between their work and these previous works . 3 .The proof of theorem 3.2 is not very clear . It seems that the proof is based on the fact that there are 66 possible A1s and A2s , which is not true for the case of A2 . It is better to clarify this point . 4 .The authors should provide more details about the verification procedure . For example , how to verify the correctness of the certificates ? 5 .The paper is not well-organized . For instance , it is hard to follow the description of the proof in Section 4.1 .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and the Weisfeiler-Lehman ( WL ) graph isomorphism test . The authors show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of nodes as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNN is thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . The paper rigorously study several variants of multisett functions and theoretically characterize their discriminativity power . The results show that under certain conditions , GNN-based graph structures that can not be distinguished by popular GNN variants , such as GCN ( Kipf & Welling , 2017 ) and GraphSAGE ( Hamilton et al. , 2017a ) , and show that the power of GIN ( Santoro et al .2017 ) is equal to that of WL .The authors also develop a simple neural architecture , Graph Isomorphism Network ( GIN ) , which can capture graph structures such as graph structure . The proposed GIN can achieve state-of-the-art performance on node classification , link prediction , and graph classification tasks . However , there is little theoretical understanding of the properties and limitations of GNN , and formal analysis of the representational capacity is limited . Therefore , the design of new GNN based models is mostly based on empirical intuition , heuristics , and experimental trial-anderror .
1_1907.07355 = This paper investigates the performance of BERT on the ARCT dataset . The authors show that the model is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated by adding a copy of each data point with the claim negated and the label inverted . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that BERT ’ s performance can be entirely accounted for in terms of exploiting spurious statistical cues , but they do not provide any evidence to support this claim . 2 .In Table 1 , BERT achieves 77 % test set accuracy with its best run ( Table 1 ) , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question “ what has BERT learned about argument comprehension ? ” 3 .In the experiments , the authors only compare BERT with the SemEval dataset . It would be better if the authors can also compare with the proposed adversarial dataset , which is more challenging .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The proposed method is simple to implement and has little computational overhead . The paper is well written and easy to follow . The authors prove that for quadratic objective functions , SWA can converge to the optimal solution as well as to a smaller asymptotic noise ball than low precision SGD for strongly convex objectives . Empirically , the authors show that training with 8 bit SWA with full precision can match the full precision baseline in deep learning tasks such as ResNet-164 and CIFAR-10/100 datasets . The experimental results show that SWA outperforms the state-of-the-art in terms of generalization performance . Pros : 1 . The idea of using SWA to reduce the quantization noise during training is interesting and novel . 2 .The paper is clearly written . 3 .The experimental results are convincing . 4 .The theoretical analysis is sound . Cons : 1.1 . It is not clear to me why SWA works well with a relatively high learning rate and can tolerate additional noise . It would be better to show the performance of SWA when the learning rate is low . 2.2 .It would be more convincing if the authors can show the convergence rate of the proposed method with a fixed learning rate . 3.3 .It is better to compare the performance with other quantization methods such as quantized SGD . 4.4 .There are some typos and grammatical errors in the paper .
1_1902.04911 = This paper proposes a novel approach to knowledge-grounded dialogue generation . The authors propose to separate the posterior distribution over knowledge from the prior distribution over utterance and response . The proposed model is trained to minimize the KL divergence between the prior and posterior distributions . Then , during the inference process , the model samples knowledge merely based on the prior prior distribution and incorporates the sampled knowledge into response generation . It is proved that this model can effectively learn to generate proper and informative responses by utilizing appropriate knowledge by utilizing knowledge . The paper is well-written and easy to follow . The idea of separating the posterior and prior distributions over knowledge is interesting and novel . However , I have the following concerns . 1 .The authors claim that the proposed method is able to avoid the discrepancy between the posterior distributions over utterances and responses . But , it is not clear to me how the model can avoid this discrepancy . For example , given an utterance , different responses can be generated depending on whether appropriate knowledge is used in the response . If the model is training by selecting wrong knowledge ( e.g. , K2 in R2 ) or knowledge irrelevant to the true response ( i.e .K1 in R1 ) , it can be seen that they are completely useless since they can not provide any helpful information . This kind of discrepancy would stop the model from learning to generating proper responses by using appropriate knowledge . 2 .The proposed approach is not compared with other related works . For instance , [ Dinan et al. , 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 3 .In Table 1 , there is no comparison with [ Ghazvininejad et al .2018 ] , which also uses knowledge selection to guide the knowledge selection . 4 .It is also important to properly incorporate knowledge in response generation , such as K1 and K3 in R4 in the Wizard of Wikipedia dataset . 5 .The paper is not well-motivated . The motivation of this paper is to solve the problem of knowledge-based dialogue generation , which has not been sufficiently studied in the previous work . 6 .There are many typos in the paper . For the example in Section 3.1 , the sentence `` In this dataset , each agent is associated with a persona profile , which is served as knowledge . `` should be removed .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea of the paper is to prune the architecture of the model by optimizing the number of parameters and the pre-training loss . The experiments show that the proposed pruning method can improve the performance of BERT . The paper is well written and easy to follow . However , I have some concerns about the experiments . 1 .The experiments are only conducted on MNLI dataset . It would be more convincing if the authors can conduct experiments on other NLP tasks . 2 .The pruning based architecture search is only applied to BERT architecture . It is not clear how the pruning can be applied to other models . 3 .The authors should compare with other pruning methods , e.g. , [ 1 ] , [ 2 ] and [ 3 ] . 4 .It would be interesting to see the results of pruning BERT on other language modeling tasks , such as NLP , machine translation , etc . 5 .The paper is not well organized . For example , in the introduction , the authors claim that “ The ratio of the architecture design dimensions within a BERT encoder layer can be modified to obtain a layer with better performance . ” .However , the paper does not explain how this can be done . 6 .In the experiments , it would be better to show the results on other tasks . 7 .In Table 1 , it is better to report the accuracy of the pruned BERT models . 8 .The results on Table 2 are not convincing . For instance , the results in Table 1 are not consistent with the results reported in the paper . 9 .In Figure 2 , why the results are not better than the ones reported in [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,47,48,50,49,51,52,53,54,55,56,58,59,60,61,62,63,64,70,72,73,72 ,73,74,75,76,80,83,84,81,88,89,90,91,93,90 ,93,94,94 ,94,96,98,99,99 ,98,101,102,103,103 ,104,104 ,104 ,105,104,105,106 ,107,108 ,108 ,109 ,110 ,111 ,112 ,113 ,114 ,115 ,116 ,117 ,118 ,119 ,122 ,123 , 123 , 128 , 130 , 131 , 128,129 , 130,131 ,131 ,132 ,133 ,134 ,135 ,136 ,137 ,138 ,139 ,140 ,141 ,142 ,143 ,144 ,145 ,150 ,153 ,154 ,155 ,156 ,157 ,163 ,168 ,178 ,179 ,180 ,181 ,183 ,184 ,185 ,186 ,197 ,198 ,199 ,207 ,208 ,207 . 10 .In Section 3.1 , it seems that the authors did not compare with the existing pruning techniques in the literature . It will be better if the author can compare with them . 11 .In section 3.2 , the author claims that ‘ The fully-connected component applied to each token separately plays a much more significant role in the top layers as compared to the bottom layers ’ . This is not true . In fact , the fully connected component is not the same as the attention head . 12 .In table 1 , the result of ‘ tall and narrow ’ architecture is better than ‘ wide and shallow ’ and ‘ shallow and tall ’ architectures . 13 .In figure 2 , the difference between ‘ top and bottom ’ is not very clear . 14 .The author should provide more details about the hyper-parameters of the proposed method . 15 .The proposed method is not compared with the other pruned methods . The author should also compare with these methods .
1_2102.07983 = This paper introduces a new dataset , FEWS ( Few-shot Examples of Word Senses ) , to train and evaluate word sense disambiguation models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary , which contains examples for over 71,000 senses , which is much higher than existing datasets such as SemCor , the largest manually annotated WSD dataset , which only covers about 33,000 . The authors also conduct experiments on knowledge-based approaches and a recent neural biencoder model for WSD . The paper is well-written and easy to follow . The proposed dataset is a good addition to the literature on WSD , and the experiments are well-conducted . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset for training and evaluating WSD models in a few- and zero shot setting . The contribution of the dataset is two-fold : ( 1 ) it exposes models a broad array of senses in a low-shot setting , and ( 2 ) the large evaluation set allows for more robust evaluation of rare senses . 2 .The experiments are conducted on knowledge based approaches and the recent Neural BienCoder model , which are the strongest baselines . However it is not clear whether the proposed dataset can be used as additional training data for other WSD tasks such as transfer learning . It would be better if the authors can show the performance of the proposed datasets on transfer learning tasks .
1_1812.02425 = This paper proposes to use a teacher-student ensembling framework to learn an ensemble of multiple neural networks without incurring any additional testing costs . The idea is to use the outputs of different neural networks as supervisions to guide the target network training . To further improve the robustness of the student networks , the authors introduce an adversarial learning strategy to force the student to generate similar outputs as teachers . The experiments show that MEAL consistently improves the accuracy across a variety of popular network architectures on different datasets . The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a number of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . It is a simple extension of Swapout ( Singh , Hoiem , and Forsyth 2016 ) . 2 .The proposed method can achieve the goal of ensembleling multiple neural network with no additional testing cost . 3 .The experimental results are not convincing . For example , in Table 1 , the performance of Shake-Shake ( Gastaldi 2017 ) and MEAL ( Meals et al. , 2017 ) are not better than Shake-shake and Meals , respectively , in terms of accuracy .
1_1901.06829 = This paper proposes a novel method for grounding natural language descriptions in videos . The proposed method is based on an end-to-end reinforcement learning ( RL ) framework , where the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . At each time step , the agent makes an observation that details the currently selected video clip , and takes an action according to its policy . The environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . In addition , the authors introduce a location feature , which enables the agent to explicitly access the current grounding boundaries . The paper is well-written and easy to follow . The method is novel and well-motivated . The experimental results show that the proposed method outperforms the state-of-the-art methods . However , I have the following concerns : 1 . The novelty of the method is limited . There are several existing methods for video grounding , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) , which can be applied only when all candidates are exhaustively enumerated , as they need to rank all possible clip-sentence pairs to be successful in generating the final grounding result . Moreover , the ranking strategy is specially designed for pre-segmented videos and is not applicable to general cases . 2 .The proposed method can not be applied to general video grounding tasks . 3 .The experiments are not convincing . For example , in Table 1 , the performance of the proposed methods is not better than the baseline methods . 4 .In Table 2 , the results are not consistent with the results reported in the original paper .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a new SAT solver to solve the problem of determining the existence of a projective plane of order 10 . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates were generated with the assistance of the symbolic computation library Traces ( McKay and Piperno 2014 ) , but it is not clear how one can verify these certificates without needing to trust the output of the library . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that their work is the first SAT-based resolution of Lamâs problem with all of the exhaustive search work completed by SAT solvers . I am not sure if this statement is correct . The previous SAT-SAT solvers have become so strong that they are “ the best solution in most cases ” . Even still , they note that some problems such as some problems like Lam’s Problem have only been solved by specialpurpose means . 2 .In the proof of Theorem 3.1 , the authors assume that the code associated with projective planes of order ten must contain words that are referred to as weight 15 , weight 16 , or ‘ weight 19 ’ words . The former two cases were first ruled out by the searches of ( Lam , Thiel , and Swiercz 1989 ) and ( MacWilliams , Sloane , and Thompson ) and were recently settled via SAT-Based Existence certificates ( Bright et al. , 2020a ,b ) . It would be better if the authors can clarify the difference between their work and these previous works . 3 .The proof of theorem 3.2 is not very clear . It seems that the proof is based on the fact that there are 66 possible A1s and A2s , which is not true for the case of A2 . It is better to clarify this point . 4 .The authors should provide more details about the verification procedure . For example , how to verify the correctness of the certificates ? 5 .The paper is not well-organized . For instance , it is hard to follow the description of the proof in Section 4.1 .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and the Weisfeiler-Lehman ( WL ) graph isomorphism test . The authors show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of nodes as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNN is thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . The paper rigorously study several variants of multisett functions and theoretically characterize their discriminativity power . The results show that under certain conditions , GNN-based graph structures that can not be distinguished by popular GNN variants , such as GCN ( Kipf & Welling , 2017 ) and GraphSAGE ( Hamilton et al. , 2017a ) , and show that the power of GIN ( Santoro et al .2017 ) is equal to that of WL .The authors also develop a simple neural architecture , Graph Isomorphism Network ( GIN ) , which can capture graph structures such as graph structure . The proposed GIN can achieve state-of-the-art performance on node classification , link prediction , and graph classification tasks . However , there is little theoretical understanding of the properties and limitations of GNN , and formal analysis of the representational capacity is limited . Therefore , the design of new GNN based models is mostly based on empirical intuition , heuristics , and experimental trial-anderror .
1_1907.07355 = This paper investigates the performance of BERT on the ARCT dataset . The authors show that the model is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated by adding a copy of each data point with the claim negated and the label inverted . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that BERT ’ s performance can be entirely accounted for in terms of exploiting spurious statistical cues , but they do not provide any evidence to support this claim . 2 .In Table 1 , BERT achieves 77 % test set accuracy with its best run ( Table 1 ) , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question “ what has BERT learned about argument comprehension ? ” 3 .In the experiments , the authors only compare BERT with the SemEval dataset . It would be better if the authors can also compare with the proposed adversarial dataset , which is more challenging .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The proposed method is simple to implement and has little computational overhead . The paper is well written and easy to follow . The authors prove that for quadratic objective functions , SWA can converge to the optimal solution as well as to a smaller asymptotic noise ball than low precision SGD for strongly convex objectives . Empirically , the authors show that training with 8 bit SWA with full precision can match the full precision baseline in deep learning tasks such as ResNet-164 and CIFAR-10/100 datasets . The experimental results show that SWA outperforms the state-of-the-art in terms of generalization performance . Pros : 1 . The idea of using SWA to reduce the quantization noise during training is interesting and novel . 2 .The paper is clearly written . 3 .The experimental results are convincing . 4 .The theoretical analysis is sound . Cons : 1.1 . It is not clear to me why SWA works well with a relatively high learning rate and can tolerate additional noise . It would be better to show the performance of SWA when the learning rate is low . 2.2 .It would be more convincing if the authors can show the convergence rate of the proposed method with a fixed learning rate . 3.3 .It is better to compare the performance with other quantization methods such as quantized SGD . 4.4 .There are some typos and grammatical errors in the paper .
1_1902.04911 = This paper proposes a novel approach to knowledge-grounded dialogue generation . The authors propose to separate the posterior distribution over knowledge from the prior distribution over utterance and response . The proposed model is trained to minimize the KL divergence between the prior and posterior distributions . Then , during the inference process , the model samples knowledge merely based on the prior prior distribution and incorporates the sampled knowledge into response generation . It is proved that this model can effectively learn to generate proper and informative responses by utilizing appropriate knowledge by utilizing knowledge . The paper is well-written and easy to follow . The idea of separating the posterior and prior distributions over knowledge is interesting and novel . However , I have the following concerns . 1 .The authors claim that the proposed method is able to avoid the discrepancy between the posterior distributions over utterances and responses . But , it is not clear to me how the model can avoid this discrepancy . For example , given an utterance , different responses can be generated depending on whether appropriate knowledge is used in the response . If the model is training by selecting wrong knowledge ( e.g. , K2 in R2 ) or knowledge irrelevant to the true response ( i.e .K1 in R1 ) , it can be seen that they are completely useless since they can not provide any helpful information . This kind of discrepancy would stop the model from learning to generating proper responses by using appropriate knowledge . 2 .The proposed approach is not compared with other related works . For instance , [ Dinan et al. , 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 3 .In Table 1 , there is no comparison with [ Ghazvininejad et al .2018 ] , which also uses knowledge selection to guide the knowledge selection . 4 .It is also important to properly incorporate knowledge in response generation , such as K1 and K3 in R4 in the Wizard of Wikipedia dataset . 5 .The paper is not well-motivated . The motivation of this paper is to solve the problem of knowledge-based dialogue generation , which has not been sufficiently studied in the previous work . 6 .There are many typos in the paper . For the example in Section 3.1 , the sentence `` In this dataset , each agent is associated with a persona profile , which is served as knowledge . `` should be removed .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea of the paper is to prune the architecture of the model by optimizing the number of parameters and the pre-training loss . The experiments show that the proposed pruning method can improve the performance of BERT . The paper is well written and easy to follow . However , I have some concerns about the experiments . 1 .The experiments are only conducted on MNLI dataset . It would be more convincing if the authors can conduct experiments on other NLP tasks . 2 .The pruning based architecture search is only applied to BERT architecture . It is not clear how the pruning can be applied to other models . 3 .The authors should compare with other pruning methods , e.g. , [ 1 ] , [ 2 ] and [ 3 ] . 4 .It would be interesting to see the results of pruning BERT on other language modeling tasks , such as NLP , machine translation , etc . 5 .The paper is not well organized . For example , in the introduction , the authors claim that “ The ratio of the architecture design dimensions within a BERT encoder layer can be modified to obtain a layer with better performance . ” .However , the paper does not explain how this can be done . 6 .In the experiments , it would be better to show the results on other tasks . 7 .In Table 1 , it is better to report the accuracy of the pruned BERT models . 8 .The results on Table 2 are not convincing . For instance , the results in Table 1 are not consistent with the results reported in the paper . 9 .In Figure 2 , why the results are not better than the ones reported in [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,47,48,50,49,51,52,53,54,55,56,58,59,60,61,62,63,64,70,72,73,72 ,73,74,75,76,80,83,84,81,88,89,90,91,93,90 ,93,94,94 ,94,96,98,99,99 ,98,101,102,103,103 ,104,104 ,104 ,105,104,105,106 ,107,108 ,108 ,109 ,110 ,111 ,112 ,113 ,114 ,115 ,116 ,117 ,118 ,119 ,122 ,123 , 123 , 128 , 130 , 131 , 128,129 , 130,131 ,131 ,132 ,133 ,134 ,135 ,136 ,137 ,138 ,139 ,140 ,141 ,142 ,143 ,144 ,145 ,150 ,153 ,154 ,155 ,156 ,157 ,163 ,168 ,178 ,179 ,180 ,181 ,183 ,184 ,185 ,186 ,197 ,198 ,199 ,207 ,208 ,207 . 10 .In Section 3.1 , it seems that the authors did not compare with the existing pruning techniques in the literature . It will be better if the author can compare with them . 11 .In section 3.2 , the author claims that ‘ The fully-connected component applied to each token separately plays a much more significant role in the top layers as compared to the bottom layers ’ . This is not true . In fact , the fully connected component is not the same as the attention head . 12 .In table 1 , the result of ‘ tall and narrow ’ architecture is better than ‘ wide and shallow ’ and ‘ shallow and tall ’ architectures . 13 .In figure 2 , the difference between ‘ top and bottom ’ is not very clear . 14 .The author should provide more details about the hyper-parameters of the proposed method . 15 .The proposed method is not compared with the other pruned methods . The author should also compare with these methods .
1_2102.07983 = This paper introduces a new dataset , FEWS ( Few-shot Examples of Word Senses ) , to train and evaluate word sense disambiguation models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary , which contains examples for over 71,000 senses , which is much higher than existing datasets such as SemCor , the largest manually annotated WSD dataset , which only covers about 33,000 . The authors also conduct experiments on knowledge-based approaches and a recent neural biencoder model for WSD . The paper is well-written and easy to follow . The proposed dataset is a good addition to the literature on WSD , and the experiments are well-conducted . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset for training and evaluating WSD models in a few- and zero shot setting . The contribution of the dataset is two-fold : ( 1 ) it exposes models a broad array of senses in a low-shot setting , and ( 2 ) the large evaluation set allows for more robust evaluation of rare senses . 2 .The experiments are conducted on knowledge based approaches and the recent Neural BienCoder model , which are the strongest baselines . However it is not clear whether the proposed dataset can be used as additional training data for other WSD tasks such as transfer learning . It would be better if the authors can show the performance of the proposed datasets on transfer learning tasks .
1_1812.02425 = This paper proposes to use a teacher-student ensembling framework to learn an ensemble of multiple neural networks without incurring any additional testing costs . The idea is to use the outputs of different neural networks as supervisions to guide the target network training . To further improve the robustness of the student networks , the authors introduce an adversarial learning strategy to force the student to generate similar outputs as teachers . The experiments show that MEAL consistently improves the accuracy across a variety of popular network architectures on different datasets . The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a number of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . It is a simple extension of Swapout ( Singh , Hoiem , and Forsyth 2016 ) . 2 .The proposed method can achieve the goal of ensembleling multiple neural network with no additional testing cost . 3 .The experimental results are not convincing . For example , in Table 1 , the performance of Shake-Shake ( Gastaldi 2017 ) and MEAL ( Meals et al. , 2017 ) are not better than Shake-shake and Meals , respectively , in terms of accuracy .
1_1901.06829 = This paper proposes a novel method for grounding natural language descriptions in videos . The proposed method is based on an end-to-end reinforcement learning ( RL ) framework , where the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . At each time step , the agent makes an observation that details the currently selected video clip , and takes an action according to its policy . The environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . In addition , the authors introduce a location feature , which enables the agent to explicitly access the current grounding boundaries . The paper is well-written and easy to follow . The method is novel and well-motivated . The experimental results show that the proposed method outperforms the state-of-the-art methods . However , I have the following concerns : 1 . The novelty of the method is limited . There are several existing methods for video grounding , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) , which can be applied only when all candidates are exhaustively enumerated , as they need to rank all possible clip-sentence pairs to be successful in generating the final grounding result . Moreover , the ranking strategy is specially designed for pre-segmented videos and is not applicable to general cases . 2 .The proposed method can not be applied to general video grounding tasks . 3 .The experiments are not convincing . For example , in Table 1 , the performance of the proposed methods is not better than the baseline methods . 4 .In Table 2 , the results are not consistent with the results reported in the original paper .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a new SAT solver to solve the problem of determining the existence of a projective plane of order 10 . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates were generated with the assistance of the symbolic computation library Traces ( McKay and Piperno 2014 ) , but it is not clear how one can verify these certificates without needing to trust the output of the library . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that their work is the first SAT-based resolution of Lamâs problem with all of the exhaustive search work completed by SAT solvers . I am not sure if this statement is correct . The previous SAT-SAT solvers have become so strong that they are “ the best solution in most cases ” . Even still , they note that some problems such as some problems like Lam’s Problem have only been solved by specialpurpose means . 2 .In the proof of Theorem 3.1 , the authors assume that the code associated with projective planes of order ten must contain words that are referred to as weight 15 , weight 16 , or ‘ weight 19 ’ words . The former two cases were first ruled out by the searches of ( Lam , Thiel , and Swiercz 1989 ) and ( MacWilliams , Sloane , and Thompson ) and were recently settled via SAT-Based Existence certificates ( Bright et al. , 2020a ,b ) . It would be better if the authors can clarify the difference between their work and these previous works . 3 .The proof of theorem 3.2 is not very clear . It seems that the proof is based on the fact that there are 66 possible A1s and A2s , which is not true for the case of A2 . It is better to clarify this point . 4 .The authors should provide more details about the verification procedure . For example , how to verify the correctness of the certificates ? 5 .The paper is not well-organized . For instance , it is hard to follow the description of the proof in Section 4.1 .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and the Weisfeiler-Lehman ( WL ) graph isomorphism test . The authors show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of nodes as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNN is thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . The paper rigorously study several variants of multisett functions and theoretically characterize their discriminativity power . The results show that under certain conditions , GNN-based graph structures that can not be distinguished by popular GNN variants , such as GCN ( Kipf & Welling , 2017 ) and GraphSAGE ( Hamilton et al. , 2017a ) , and show that the power of GIN ( Santoro et al .2017 ) is equal to that of WL .The authors also develop a simple neural architecture , Graph Isomorphism Network ( GIN ) , which can capture graph structures such as graph structure . The proposed GIN can achieve state-of-the-art performance on node classification , link prediction , and graph classification tasks . However , there is little theoretical understanding of the properties and limitations of GNN , and formal analysis of the representational capacity is limited . Therefore , the design of new GNN based models is mostly based on empirical intuition , heuristics , and experimental trial-anderror .
1_1907.07355 = This paper investigates the performance of BERT on the ARCT dataset . The authors show that the model is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated by adding a copy of each data point with the claim negated and the label inverted . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that BERT ’ s performance can be entirely accounted for in terms of exploiting spurious statistical cues , but they do not provide any evidence to support this claim . 2 .In Table 1 , BERT achieves 77 % test set accuracy with its best run ( Table 1 ) , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question “ what has BERT learned about argument comprehension ? ” 3 .In the experiments , the authors only compare BERT with the SemEval dataset . It would be better if the authors can also compare with the proposed adversarial dataset , which is more challenging .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The proposed method is simple to implement and has little computational overhead . The paper is well written and easy to follow . The authors prove that for quadratic objective functions , SWA can converge to the optimal solution as well as to a smaller asymptotic noise ball than low precision SGD for strongly convex objectives . Empirically , the authors show that training with 8 bit SWA with full precision can match the full precision baseline in deep learning tasks such as ResNet-164 and CIFAR-10/100 datasets . The experimental results show that SWA outperforms the state-of-the-art in terms of generalization performance . Pros : 1 . The idea of using SWA to reduce the quantization noise during training is interesting and novel . 2 .The paper is clearly written . 3 .The experimental results are convincing . 4 .The theoretical analysis is sound . Cons : 1.1 . It is not clear to me why SWA works well with a relatively high learning rate and can tolerate additional noise . It would be better to show the performance of SWA when the learning rate is low . 2.2 .It would be more convincing if the authors can show the convergence rate of the proposed method with a fixed learning rate . 3.3 .It is better to compare the performance with other quantization methods such as quantized SGD . 4.4 .There are some typos and grammatical errors in the paper .
1_1902.04911 = This paper proposes a novel approach to knowledge-grounded dialogue generation . The authors propose to separate the posterior distribution over knowledge from the prior distribution over utterance and response . The proposed model is trained to minimize the KL divergence between the prior and posterior distributions . Then , during the inference process , the model samples knowledge merely based on the prior prior distribution and incorporates the sampled knowledge into response generation . It is proved that this model can effectively learn to generate proper and informative responses by utilizing appropriate knowledge by utilizing knowledge . The paper is well-written and easy to follow . The idea of separating the posterior and prior distributions over knowledge is interesting and novel . However , I have the following concerns . 1 .The authors claim that the proposed method is able to avoid the discrepancy between the posterior distributions over utterances and responses . But , it is not clear to me how the model can avoid this discrepancy . For example , given an utterance , different responses can be generated depending on whether appropriate knowledge is used in the response . If the model is training by selecting wrong knowledge ( e.g. , K2 in R2 ) or knowledge irrelevant to the true response ( i.e .K1 in R1 ) , it can be seen that they are completely useless since they can not provide any helpful information . This kind of discrepancy would stop the model from learning to generating proper responses by using appropriate knowledge . 2 .The proposed approach is not compared with other related works . For instance , [ Dinan et al. , 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 3 .In Table 1 , there is no comparison with [ Ghazvininejad et al .2018 ] , which also uses knowledge selection to guide the knowledge selection . 4 .It is also important to properly incorporate knowledge in response generation , such as K1 and K3 in R4 in the Wizard of Wikipedia dataset . 5 .The paper is not well-motivated . The motivation of this paper is to solve the problem of knowledge-based dialogue generation , which has not been sufficiently studied in the previous work . 6 .There are many typos in the paper . For the example in Section 3.1 , the sentence `` In this dataset , each agent is associated with a persona profile , which is served as knowledge . `` should be removed .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea of the paper is to prune the architecture of the model by optimizing the number of parameters and the pre-training loss . The experiments show that the proposed pruning method can improve the performance of BERT . The paper is well written and easy to follow . However , I have some concerns about the experiments . 1 .The experiments are only conducted on MNLI dataset . It would be more convincing if the authors can conduct experiments on other NLP tasks . 2 .The pruning based architecture search is only applied to BERT architecture . It is not clear how the pruning can be applied to other models . 3 .The authors should compare with other pruning methods , e.g. , [ 1 ] , [ 2 ] and [ 3 ] . 4 .It would be interesting to see the results of pruning BERT on other language modeling tasks , such as NLP , machine translation , etc . 5 .The paper is not well organized . For example , in the introduction , the authors claim that “ The ratio of the architecture design dimensions within a BERT encoder layer can be modified to obtain a layer with better performance . ” .However , the paper does not explain how this can be done . 6 .In the experiments , it would be better to show the results on other tasks . 7 .In Table 1 , it is better to report the accuracy of the pruned BERT models . 8 .The results on Table 2 are not convincing . For instance , the results in Table 1 are not consistent with the results reported in the paper . 9 .In Figure 2 , why the results are not better than the ones reported in [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,47,48,50,49,51,52,53,54,55,56,58,59,60,61,62,63,64,70,72,73,72 ,73,74,75,76,80,83,84,81,88,89,90,91,93,90 ,93,94,94 ,94,96,98,99,99 ,98,101,102,103,103 ,104,104 ,104 ,105,104,105,106 ,107,108 ,108 ,109 ,110 ,111 ,112 ,113 ,114 ,115 ,116 ,117 ,118 ,119 ,122 ,123 , 123 , 128 , 130 , 131 , 128,129 , 130,131 ,131 ,132 ,133 ,134 ,135 ,136 ,137 ,138 ,139 ,140 ,141 ,142 ,143 ,144 ,145 ,150 ,153 ,154 ,155 ,156 ,157 ,163 ,168 ,178 ,179 ,180 ,181 ,183 ,184 ,185 ,186 ,197 ,198 ,199 ,207 ,208 ,207 . 10 .In Section 3.1 , it seems that the authors did not compare with the existing pruning techniques in the literature . It will be better if the author can compare with them . 11 .In section 3.2 , the author claims that ‘ The fully-connected component applied to each token separately plays a much more significant role in the top layers as compared to the bottom layers ’ . This is not true . In fact , the fully connected component is not the same as the attention head . 12 .In table 1 , the result of ‘ tall and narrow ’ architecture is better than ‘ wide and shallow ’ and ‘ shallow and tall ’ architectures . 13 .In figure 2 , the difference between ‘ top and bottom ’ is not very clear . 14 .The author should provide more details about the hyper-parameters of the proposed method . 15 .The proposed method is not compared with the other pruned methods . The author should also compare with these methods .
1_2102.07983 = This paper introduces a new dataset , FEWS ( Few-shot Examples of Word Senses ) , to train and evaluate word sense disambiguation models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary , which contains examples for over 71,000 senses , which is much higher than existing datasets such as SemCor , the largest manually annotated WSD dataset , which only covers about 33,000 . The authors also conduct experiments on knowledge-based approaches and a recent neural biencoder model for WSD . The paper is well-written and easy to follow . The proposed dataset is a good addition to the literature on WSD , and the experiments are well-conducted . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset for training and evaluating WSD models in a few- and zero shot setting . The contribution of the dataset is two-fold : ( 1 ) it exposes models a broad array of senses in a low-shot setting , and ( 2 ) the large evaluation set allows for more robust evaluation of rare senses . 2 .The experiments are conducted on knowledge based approaches and the recent Neural BienCoder model , which are the strongest baselines . However it is not clear whether the proposed dataset can be used as additional training data for other WSD tasks such as transfer learning . It would be better if the authors can show the performance of the proposed datasets on transfer learning tasks .
1_1812.02425 = This paper proposes to use a teacher-student ensembling framework to learn an ensemble of multiple neural networks without incurring any additional testing costs . The idea is to use the outputs of different neural networks as supervisions to guide the target network training . To further improve the robustness of the student networks , the authors introduce an adversarial learning strategy to force the student to generate similar outputs as teachers . The experiments show that MEAL consistently improves the accuracy across a variety of popular network architectures on different datasets . The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a number of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . It is a simple extension of Swapout ( Singh , Hoiem , and Forsyth 2016 ) . 2 .The proposed method can achieve the goal of ensembleling multiple neural network with no additional testing cost . 3 .The experimental results are not convincing . For example , in Table 1 , the performance of Shake-Shake ( Gastaldi 2017 ) and MEAL ( Meals et al. , 2017 ) are not better than Shake-shake and Meals , respectively , in terms of accuracy .
1_1901.06829 = This paper proposes a novel method for grounding natural language descriptions in videos . The proposed method is based on an end-to-end reinforcement learning ( RL ) framework , where the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . At each time step , the agent makes an observation that details the currently selected video clip , and takes an action according to its policy . The environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . In addition , the authors introduce a location feature , which enables the agent to explicitly access the current grounding boundaries . The paper is well-written and easy to follow . The method is novel and well-motivated . The experimental results show that the proposed method outperforms the state-of-the-art methods . However , I have the following concerns : 1 . The novelty of the method is limited . There are several existing methods for video grounding , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) , which can be applied only when all candidates are exhaustively enumerated , as they need to rank all possible clip-sentence pairs to be successful in generating the final grounding result . Moreover , the ranking strategy is specially designed for pre-segmented videos and is not applicable to general cases . 2 .The proposed method can not be applied to general video grounding tasks . 3 .The experiments are not convincing . For example , in Table 1 , the performance of the proposed methods is not better than the baseline methods . 4 .In Table 2 , the results are not consistent with the results reported in the original paper .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a new SAT solver to solve the problem of determining the existence of a projective plane of order 10 . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates were generated with the assistance of the symbolic computation library Traces ( McKay and Piperno 2014 ) , but it is not clear how one can verify these certificates without needing to trust the output of the library . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that their work is the first SAT-based resolution of Lamâs problem with all of the exhaustive search work completed by SAT solvers . I am not sure if this statement is correct . The previous SAT-SAT solvers have become so strong that they are “ the best solution in most cases ” . Even still , they note that some problems such as some problems like Lam’s Problem have only been solved by specialpurpose means . 2 .In the proof of Theorem 3.1 , the authors assume that the code associated with projective planes of order ten must contain words that are referred to as weight 15 , weight 16 , or ‘ weight 19 ’ words . The former two cases were first ruled out by the searches of ( Lam , Thiel , and Swiercz 1989 ) and ( MacWilliams , Sloane , and Thompson ) and were recently settled via SAT-Based Existence certificates ( Bright et al. , 2020a ,b ) . It would be better if the authors can clarify the difference between their work and these previous works . 3 .The proof of theorem 3.2 is not very clear . It seems that the proof is based on the fact that there are 66 possible A1s and A2s , which is not true for the case of A2 . It is better to clarify this point . 4 .The authors should provide more details about the verification procedure . For example , how to verify the correctness of the certificates ? 5 .The paper is not well-organized . For instance , it is hard to follow the description of the proof in Section 4.1 .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and the Weisfeiler-Lehman ( WL ) graph isomorphism test . The authors show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of nodes as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNN is thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . The paper rigorously study several variants of multisett functions and theoretically characterize their discriminativity power . The results show that under certain conditions , GNN-based graph structures that can not be distinguished by popular GNN variants , such as GCN ( Kipf & Welling , 2017 ) and GraphSAGE ( Hamilton et al. , 2017a ) , and show that the power of GIN ( Santoro et al .2017 ) is equal to that of WL .The authors also develop a simple neural architecture , Graph Isomorphism Network ( GIN ) , which can capture graph structures such as graph structure . The proposed GIN can achieve state-of-the-art performance on node classification , link prediction , and graph classification tasks . However , there is little theoretical understanding of the properties and limitations of GNN , and formal analysis of the representational capacity is limited . Therefore , the design of new GNN based models is mostly based on empirical intuition , heuristics , and experimental trial-anderror .
1_1907.07355 = This paper investigates the performance of BERT on the ARCT dataset . The authors show that the model is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated by adding a copy of each data point with the claim negated and the label inverted . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that BERT ’ s performance can be entirely accounted for in terms of exploiting spurious statistical cues , but they do not provide any evidence to support this claim . 2 .In Table 1 , BERT achieves 77 % test set accuracy with its best run ( Table 1 ) , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question “ what has BERT learned about argument comprehension ? ” 3 .In the experiments , the authors only compare BERT with the SemEval dataset . It would be better if the authors can also compare with the proposed adversarial dataset , which is more challenging .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The proposed method is simple to implement and has little computational overhead . The paper is well written and easy to follow . The authors prove that for quadratic objective functions , SWA can converge to the optimal solution as well as to a smaller asymptotic noise ball than low precision SGD for strongly convex objectives . Empirically , the authors show that training with 8 bit SWA with full precision can match the full precision baseline in deep learning tasks such as ResNet-164 and CIFAR-10/100 datasets . The experimental results show that SWA outperforms the state-of-the-art in terms of generalization performance . Pros : 1 . The idea of using SWA to reduce the quantization noise during training is interesting and novel . 2 .The paper is clearly written . 3 .The experimental results are convincing . 4 .The theoretical analysis is sound . Cons : 1.1 . It is not clear to me why SWA works well with a relatively high learning rate and can tolerate additional noise . It would be better to show the performance of SWA when the learning rate is low . 2.2 .It would be more convincing if the authors can show the convergence rate of the proposed method with a fixed learning rate . 3.3 .It is better to compare the performance with other quantization methods such as quantized SGD . 4.4 .There are some typos and grammatical errors in the paper .
1_1902.04911 = This paper proposes a novel approach to knowledge-grounded dialogue generation . The authors propose to separate the posterior distribution over knowledge from the prior distribution over utterance and response . The proposed model is trained to minimize the KL divergence between the prior and posterior distributions . Then , during the inference process , the model samples knowledge merely based on the prior prior distribution and incorporates the sampled knowledge into response generation . It is proved that this model can effectively learn to generate proper and informative responses by utilizing appropriate knowledge by utilizing knowledge . The paper is well-written and easy to follow . The idea of separating the posterior and prior distributions over knowledge is interesting and novel . However , I have the following concerns . 1 .The authors claim that the proposed method is able to avoid the discrepancy between the posterior distributions over utterances and responses . But , it is not clear to me how the model can avoid this discrepancy . For example , given an utterance , different responses can be generated depending on whether appropriate knowledge is used in the response . If the model is training by selecting wrong knowledge ( e.g. , K2 in R2 ) or knowledge irrelevant to the true response ( i.e .K1 in R1 ) , it can be seen that they are completely useless since they can not provide any helpful information . This kind of discrepancy would stop the model from learning to generating proper responses by using appropriate knowledge . 2 .The proposed approach is not compared with other related works . For instance , [ Dinan et al. , 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 3 .In Table 1 , there is no comparison with [ Ghazvininejad et al .2018 ] , which also uses knowledge selection to guide the knowledge selection . 4 .It is also important to properly incorporate knowledge in response generation , such as K1 and K3 in R4 in the Wizard of Wikipedia dataset . 5 .The paper is not well-motivated . The motivation of this paper is to solve the problem of knowledge-based dialogue generation , which has not been sufficiently studied in the previous work . 6 .There are many typos in the paper . For the example in Section 3.1 , the sentence `` In this dataset , each agent is associated with a persona profile , which is served as knowledge . `` should be removed .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea of the paper is to prune the architecture of the model by optimizing the number of parameters and the pre-training loss . The experiments show that the proposed pruning method can improve the performance of BERT . The paper is well written and easy to follow . However , I have some concerns about the experiments . 1 .The experiments are only conducted on MNLI dataset . It would be more convincing if the authors can conduct experiments on other NLP tasks . 2 .The pruning based architecture search is only applied to BERT architecture . It is not clear how the pruning can be applied to other models . 3 .The authors should compare with other pruning methods , e.g. , [ 1 ] , [ 2 ] and [ 3 ] . 4 .It would be interesting to see the results of pruning BERT on other language modeling tasks , such as NLP , machine translation , etc . 5 .The paper is not well organized . For example , in the introduction , the authors claim that “ The ratio of the architecture design dimensions within a BERT encoder layer can be modified to obtain a layer with better performance . ” .However , the paper does not explain how this can be done . 6 .In the experiments , it would be better to show the results on other tasks . 7 .In Table 1 , it is better to report the accuracy of the pruned BERT models . 8 .The results on Table 2 are not convincing . For instance , the results in Table 1 are not consistent with the results reported in the paper . 9 .In Figure 2 , why the results are not better than the ones reported in [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,47,48,50,49,51,52,53,54,55,56,58,59,60,61,62,63,64,70,72,73,72 ,73,74,75,76,80,83,84,81,88,89,90,91,93,90 ,93,94,94 ,94,96,98,99,99 ,98,101,102,103,103 ,104,104 ,104 ,105,104,105,106 ,107,108 ,108 ,109 ,110 ,111 ,112 ,113 ,114 ,115 ,116 ,117 ,118 ,119 ,122 ,123 , 123 , 128 , 130 , 131 , 128,129 , 130,131 ,131 ,132 ,133 ,134 ,135 ,136 ,137 ,138 ,139 ,140 ,141 ,142 ,143 ,144 ,145 ,150 ,153 ,154 ,155 ,156 ,157 ,163 ,168 ,178 ,179 ,180 ,181 ,183 ,184 ,185 ,186 ,197 ,198 ,199 ,207 ,208 ,207 . 10 .In Section 3.1 , it seems that the authors did not compare with the existing pruning techniques in the literature . It will be better if the author can compare with them . 11 .In section 3.2 , the author claims that ‘ The fully-connected component applied to each token separately plays a much more significant role in the top layers as compared to the bottom layers ’ . This is not true . In fact , the fully connected component is not the same as the attention head . 12 .In table 1 , the result of ‘ tall and narrow ’ architecture is better than ‘ wide and shallow ’ and ‘ shallow and tall ’ architectures . 13 .In figure 2 , the difference between ‘ top and bottom ’ is not very clear . 14 .The author should provide more details about the hyper-parameters of the proposed method . 15 .The proposed method is not compared with the other pruned methods . The author should also compare with these methods .
1_2102.07983 = This paper introduces a new dataset , FEWS ( Few-shot Examples of Word Senses ) , to train and evaluate word sense disambiguation models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary , which contains examples for over 71,000 senses , which is much higher than existing datasets such as SemCor , the largest manually annotated WSD dataset , which only covers about 33,000 . The authors also conduct experiments on knowledge-based approaches and a recent neural biencoder model for WSD . The paper is well-written and easy to follow . The proposed dataset is a good addition to the literature on WSD , and the experiments are well-conducted . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset for training and evaluating WSD models in a few- and zero shot setting . The contribution of the dataset is two-fold : ( 1 ) it exposes models a broad array of senses in a low-shot setting , and ( 2 ) the large evaluation set allows for more robust evaluation of rare senses . 2 .The experiments are conducted on knowledge based approaches and the recent Neural BienCoder model , which are the strongest baselines . However it is not clear whether the proposed dataset can be used as additional training data for other WSD tasks such as transfer learning . It would be better if the authors can show the performance of the proposed datasets on transfer learning tasks .
1_1812.02425 = This paper proposes to use a teacher-student ensembling framework to learn an ensemble of multiple neural networks without incurring any additional testing costs . The idea is to use the outputs of different neural networks as supervisions to guide the target network training . To further improve the robustness of the student networks , the authors introduce an adversarial learning strategy to force the student to generate similar outputs as teachers . The experiments show that MEAL consistently improves the accuracy across a variety of popular network architectures on different datasets . The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a number of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . It is a simple extension of Swapout ( Singh , Hoiem , and Forsyth 2016 ) . 2 .The proposed method can achieve the goal of ensembleling multiple neural network with no additional testing cost . 3 .The experimental results are not convincing . For example , in Table 1 , the performance of Shake-Shake ( Gastaldi 2017 ) and MEAL ( Meals et al. , 2017 ) are not better than Shake-shake and Meals , respectively , in terms of accuracy .
1_1901.06829 = This paper proposes a novel method for grounding natural language descriptions in videos . The proposed method is based on an end-to-end reinforcement learning ( RL ) framework , where the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . At each time step , the agent makes an observation that details the currently selected video clip , and takes an action according to its policy . The environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . In addition , the authors introduce a location feature , which enables the agent to explicitly access the current grounding boundaries . The paper is well-written and easy to follow . The method is novel and well-motivated . The experimental results show that the proposed method outperforms the state-of-the-art methods . However , I have the following concerns : 1 . The novelty of the method is limited . There are several existing methods for video grounding , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) , which can be applied only when all candidates are exhaustively enumerated , as they need to rank all possible clip-sentence pairs to be successful in generating the final grounding result . Moreover , the ranking strategy is specially designed for pre-segmented videos and is not applicable to general cases . 2 .The proposed method can not be applied to general video grounding tasks . 3 .The experiments are not convincing . For example , in Table 1 , the performance of the proposed methods is not better than the baseline methods . 4 .In Table 2 , the results are not consistent with the results reported in the original paper .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a new SAT solver to solve the problem of determining the existence of a projective plane of order 10 . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates were generated with the assistance of the symbolic computation library Traces ( McKay and Piperno 2014 ) , but it is not clear how one can verify these certificates without needing to trust the output of the library . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that their work is the first SAT-based resolution of Lamâs problem with all of the exhaustive search work completed by SAT solvers . I am not sure if this statement is correct . The previous SAT-SAT solvers have become so strong that they are “ the best solution in most cases ” . Even still , they note that some problems such as some problems like Lam’s Problem have only been solved by specialpurpose means . 2 .In the proof of Theorem 3.1 , the authors assume that the code associated with projective planes of order ten must contain words that are referred to as weight 15 , weight 16 , or ‘ weight 19 ’ words . The former two cases were first ruled out by the searches of ( Lam , Thiel , and Swiercz 1989 ) and ( MacWilliams , Sloane , and Thompson ) and were recently settled via SAT-Based Existence certificates ( Bright et al. , 2020a ,b ) . It would be better if the authors can clarify the difference between their work and these previous works . 3 .The proof of theorem 3.2 is not very clear . It seems that the proof is based on the fact that there are 66 possible A1s and A2s , which is not true for the case of A2 . It is better to clarify this point . 4 .The authors should provide more details about the verification procedure . For example , how to verify the correctness of the certificates ? 5 .The paper is not well-organized . For instance , it is hard to follow the description of the proof in Section 4.1 .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and the Weisfeiler-Lehman ( WL ) graph isomorphism test . The authors show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of nodes as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNN is thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . The paper rigorously study several variants of multisett functions and theoretically characterize their discriminativity power . The results show that under certain conditions , GNN-based graph structures that can not be distinguished by popular GNN variants , such as GCN ( Kipf & Welling , 2017 ) and GraphSAGE ( Hamilton et al. , 2017a ) , and show that the power of GIN ( Santoro et al .2017 ) is equal to that of WL .The authors also develop a simple neural architecture , Graph Isomorphism Network ( GIN ) , which can capture graph structures such as graph structure . The proposed GIN can achieve state-of-the-art performance on node classification , link prediction , and graph classification tasks . However , there is little theoretical understanding of the properties and limitations of GNN , and formal analysis of the representational capacity is limited . Therefore , the design of new GNN based models is mostly based on empirical intuition , heuristics , and experimental trial-anderror .
1_1907.07355 = This paper investigates the performance of BERT on the ARCT dataset . The authors show that the model is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated by adding a copy of each data point with the claim negated and the label inverted . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that BERT ’ s performance can be entirely accounted for in terms of exploiting spurious statistical cues , but they do not provide any evidence to support this claim . 2 .In Table 1 , BERT achieves 77 % test set accuracy with its best run ( Table 1 ) , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question “ what has BERT learned about argument comprehension ? ” 3 .In the experiments , the authors only compare BERT with the SemEval dataset . It would be better if the authors can also compare with the proposed adversarial dataset , which is more challenging .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The proposed method is simple to implement and has little computational overhead . The paper is well written and easy to follow . The authors prove that for quadratic objective functions , SWA can converge to the optimal solution as well as to a smaller asymptotic noise ball than low precision SGD for strongly convex objectives . Empirically , the authors show that training with 8 bit SWA with full precision can match the full precision baseline in deep learning tasks such as ResNet-164 and CIFAR-10/100 datasets . The experimental results show that SWA outperforms the state-of-the-art in terms of generalization performance . Pros : 1 . The idea of using SWA to reduce the quantization noise during training is interesting and novel . 2 .The paper is clearly written . 3 .The experimental results are convincing . 4 .The theoretical analysis is sound . Cons : 1.1 . It is not clear to me why SWA works well with a relatively high learning rate and can tolerate additional noise . It would be better to show the performance of SWA when the learning rate is low . 2.2 .It would be more convincing if the authors can show the convergence rate of the proposed method with a fixed learning rate . 3.3 .It is better to compare the performance with other quantization methods such as quantized SGD . 4.4 .There are some typos and grammatical errors in the paper .
1_1902.04911 = This paper proposes a novel approach to knowledge-grounded dialogue generation . The authors propose to separate the posterior distribution over knowledge from the prior distribution over utterance and response . The proposed model is trained to minimize the KL divergence between the prior and posterior distributions . Then , during the inference process , the model samples knowledge merely based on the prior prior distribution and incorporates the sampled knowledge into response generation . It is proved that this model can effectively learn to generate proper and informative responses by utilizing appropriate knowledge by utilizing knowledge . The paper is well-written and easy to follow . The idea of separating the posterior and prior distributions over knowledge is interesting and novel . However , I have the following concerns . 1 .The authors claim that the proposed method is able to avoid the discrepancy between the posterior distributions over utterances and responses . But , it is not clear to me how the model can avoid this discrepancy . For example , given an utterance , different responses can be generated depending on whether appropriate knowledge is used in the response . If the model is training by selecting wrong knowledge ( e.g. , K2 in R2 ) or knowledge irrelevant to the true response ( i.e .K1 in R1 ) , it can be seen that they are completely useless since they can not provide any helpful information . This kind of discrepancy would stop the model from learning to generating proper responses by using appropriate knowledge . 2 .The proposed approach is not compared with other related works . For instance , [ Dinan et al. , 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 3 .In Table 1 , there is no comparison with [ Ghazvininejad et al .2018 ] , which also uses knowledge selection to guide the knowledge selection . 4 .It is also important to properly incorporate knowledge in response generation , such as K1 and K3 in R4 in the Wizard of Wikipedia dataset . 5 .The paper is not well-motivated . The motivation of this paper is to solve the problem of knowledge-based dialogue generation , which has not been sufficiently studied in the previous work . 6 .There are many typos in the paper . For the example in Section 3.1 , the sentence `` In this dataset , each agent is associated with a persona profile , which is served as knowledge . `` should be removed .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea of the paper is to prune the architecture of the model by optimizing the number of parameters and the pre-training loss . The experiments show that the proposed pruning method can improve the performance of BERT . The paper is well written and easy to follow . However , I have some concerns about the experiments . 1 .The experiments are only conducted on MNLI dataset . It would be more convincing if the authors can conduct experiments on other NLP tasks . 2 .The pruning based architecture search is only applied to BERT architecture . It is not clear how the pruning can be applied to other models . 3 .The authors should compare with other pruning methods , e.g. , [ 1 ] , [ 2 ] and [ 3 ] . 4 .It would be interesting to see the results of pruning BERT on other language modeling tasks , such as NLP , machine translation , etc . 5 .The paper is not well organized . For example , in the introduction , the authors claim that “ The ratio of the architecture design dimensions within a BERT encoder layer can be modified to obtain a layer with better performance . ” .However , the paper does not explain how this can be done . 6 .In the experiments , it would be better to show the results on other tasks . 7 .In Table 1 , it is better to report the accuracy of the pruned BERT models . 8 .The results on Table 2 are not convincing . For instance , the results in Table 1 are not consistent with the results reported in the paper . 9 .In Figure 2 , why the results are not better than the ones reported in [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,47,48,50,49,51,52,53,54,55,56,58,59,60,61,62,63,64,70,72,73,72 ,73,74,75,76,80,83,84,81,88,89,90,91,93,90 ,93,94,94 ,94,96,98,99,99 ,98,101,102,103,103 ,104,104 ,104 ,105,104,105,106 ,107,108 ,108 ,109 ,110 ,111 ,112 ,113 ,114 ,115 ,116 ,117 ,118 ,119 ,122 ,123 , 123 , 128 , 130 , 131 , 128,129 , 130,131 ,131 ,132 ,133 ,134 ,135 ,136 ,137 ,138 ,139 ,140 ,141 ,142 ,143 ,144 ,145 ,150 ,153 ,154 ,155 ,156 ,157 ,163 ,168 ,178 ,179 ,180 ,181 ,183 ,184 ,185 ,186 ,197 ,198 ,199 ,207 ,208 ,207 . 10 .In Section 3.1 , it seems that the authors did not compare with the existing pruning techniques in the literature . It will be better if the author can compare with them . 11 .In section 3.2 , the author claims that ‘ The fully-connected component applied to each token separately plays a much more significant role in the top layers as compared to the bottom layers ’ . This is not true . In fact , the fully connected component is not the same as the attention head . 12 .In table 1 , the result of ‘ tall and narrow ’ architecture is better than ‘ wide and shallow ’ and ‘ shallow and tall ’ architectures . 13 .In figure 2 , the difference between ‘ top and bottom ’ is not very clear . 14 .The author should provide more details about the hyper-parameters of the proposed method . 15 .The proposed method is not compared with the other pruned methods . The author should also compare with these methods .
1_2102.07983 = This paper introduces a new dataset , FEWS ( Few-shot Examples of Word Senses ) , to train and evaluate word sense disambiguation models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary , which contains examples for over 71,000 senses , which is much higher than existing datasets such as SemCor , the largest manually annotated WSD dataset , which only covers about 33,000 . The authors also conduct experiments on knowledge-based approaches and a recent neural biencoder model for WSD . The paper is well-written and easy to follow . The proposed dataset is a good addition to the literature on WSD , and the experiments are well-conducted . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset for training and evaluating WSD models in a few- and zero shot setting . The contribution of the dataset is two-fold : ( 1 ) it exposes models a broad array of senses in a low-shot setting , and ( 2 ) the large evaluation set allows for more robust evaluation of rare senses . 2 .The experiments are conducted on knowledge based approaches and the recent Neural BienCoder model , which are the strongest baselines . However it is not clear whether the proposed dataset can be used as additional training data for other WSD tasks such as transfer learning . It would be better if the authors can show the performance of the proposed datasets on transfer learning tasks .
1_1812.02425 = This paper proposes to use a teacher-student ensembling framework to learn an ensemble of multiple neural networks without incurring any additional testing costs . The idea is to use the outputs of different neural networks as supervisions to guide the target network training . To further improve the robustness of the student networks , the authors introduce an adversarial learning strategy to force the student to generate similar outputs as teachers . The experiments show that MEAL consistently improves the accuracy across a variety of popular network architectures on different datasets . The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a number of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . It is a simple extension of Swapout ( Singh , Hoiem , and Forsyth 2016 ) . 2 .The proposed method can achieve the goal of ensembleling multiple neural network with no additional testing cost . 3 .The experimental results are not convincing . For example , in Table 1 , the performance of Shake-Shake ( Gastaldi 2017 ) and MEAL ( Meals et al. , 2017 ) are not better than Shake-shake and Meals , respectively , in terms of accuracy .
1_1901.06829 = This paper proposes a novel method for grounding natural language descriptions in videos . The proposed method is based on an end-to-end reinforcement learning ( RL ) framework , where the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . At each time step , the agent makes an observation that details the currently selected video clip , and takes an action according to its policy . The environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . In addition , the authors introduce a location feature , which enables the agent to explicitly access the current grounding boundaries . The paper is well-written and easy to follow . The method is novel and well-motivated . The experimental results show that the proposed method outperforms the state-of-the-art methods . However , I have the following concerns : 1 . The novelty of the method is limited . There are several existing methods for video grounding , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) , which can be applied only when all candidates are exhaustively enumerated , as they need to rank all possible clip-sentence pairs to be successful in generating the final grounding result . Moreover , the ranking strategy is specially designed for pre-segmented videos and is not applicable to general cases . 2 .The proposed method can not be applied to general video grounding tasks . 3 .The experiments are not convincing . For example , in Table 1 , the performance of the proposed methods is not better than the baseline methods . 4 .In Table 2 , the results are not consistent with the results reported in the original paper .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a new SAT solver to solve the problem of determining the existence of a projective plane of order 10 . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates were generated with the assistance of the symbolic computation library Traces ( McKay and Piperno 2014 ) , but it is not clear how one can verify these certificates without needing to trust the output of the library . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that their work is the first SAT-based resolution of Lamâs problem with all of the exhaustive search work completed by SAT solvers . I am not sure if this statement is correct . The previous SAT-SAT solvers have become so strong that they are “ the best solution in most cases ” . Even still , they note that some problems such as some problems like Lam’s Problem have only been solved by specialpurpose means . 2 .In the proof of Theorem 3.1 , the authors assume that the code associated with projective planes of order ten must contain words that are referred to as weight 15 , weight 16 , or ‘ weight 19 ’ words . The former two cases were first ruled out by the searches of ( Lam , Thiel , and Swiercz 1989 ) and ( MacWilliams , Sloane , and Thompson ) and were recently settled via SAT-Based Existence certificates ( Bright et al. , 2020a ,b ) . It would be better if the authors can clarify the difference between their work and these previous works . 3 .The proof of theorem 3.2 is not very clear . It seems that the proof is based on the fact that there are 66 possible A1s and A2s , which is not true for the case of A2 . It is better to clarify this point . 4 .The authors should provide more details about the verification procedure . For example , how to verify the correctness of the certificates ? 5 .The paper is not well-organized . For instance , it is hard to follow the description of the proof in Section 4.1 .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and the Weisfeiler-Lehman ( WL ) graph isomorphism test . The authors show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of nodes as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNN is thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . The paper rigorously study several variants of multisett functions and theoretically characterize their discriminativity power . The results show that under certain conditions , GNN-based graph structures that can not be distinguished by popular GNN variants , such as GCN ( Kipf & Welling , 2017 ) and GraphSAGE ( Hamilton et al. , 2017a ) , and show that the power of GIN ( Santoro et al .2017 ) is equal to that of WL .The authors also develop a simple neural architecture , Graph Isomorphism Network ( GIN ) , which can capture graph structures such as graph structure . The proposed GIN can achieve state-of-the-art performance on node classification , link prediction , and graph classification tasks . However , there is little theoretical understanding of the properties and limitations of GNN , and formal analysis of the representational capacity is limited . Therefore , the design of new GNN based models is mostly based on empirical intuition , heuristics , and experimental trial-anderror .
1_1907.07355 = This paper investigates the performance of BERT on the ARCT dataset . The authors show that the model is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated by adding a copy of each data point with the claim negated and the label inverted . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that BERT ’ s performance can be entirely accounted for in terms of exploiting spurious statistical cues , but they do not provide any evidence to support this claim . 2 .In Table 1 , BERT achieves 77 % test set accuracy with its best run ( Table 1 ) , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question “ what has BERT learned about argument comprehension ? ” 3 .In the experiments , the authors only compare BERT with the SemEval dataset . It would be better if the authors can also compare with the proposed adversarial dataset , which is more challenging .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The proposed method is simple to implement and has little computational overhead . The paper is well written and easy to follow . The authors prove that for quadratic objective functions , SWA can converge to the optimal solution as well as to a smaller asymptotic noise ball than low precision SGD for strongly convex objectives . Empirically , the authors show that training with 8 bit SWA with full precision can match the full precision baseline in deep learning tasks such as ResNet-164 and CIFAR-10/100 datasets . The experimental results show that SWA outperforms the state-of-the-art in terms of generalization performance . Pros : 1 . The idea of using SWA to reduce the quantization noise during training is interesting and novel . 2 .The paper is clearly written . 3 .The experimental results are convincing . 4 .The theoretical analysis is sound . Cons : 1.1 . It is not clear to me why SWA works well with a relatively high learning rate and can tolerate additional noise . It would be better to show the performance of SWA when the learning rate is low . 2.2 .It would be more convincing if the authors can show the convergence rate of the proposed method with a fixed learning rate . 3.3 .It is better to compare the performance with other quantization methods such as quantized SGD . 4.4 .There are some typos and grammatical errors in the paper .
1_1902.04911 = This paper proposes a novel approach to knowledge-grounded dialogue generation . The authors propose to separate the posterior distribution over knowledge from the prior distribution over utterance and response . The proposed model is trained to minimize the KL divergence between the prior and posterior distributions . Then , during the inference process , the model samples knowledge merely based on the prior prior distribution and incorporates the sampled knowledge into response generation . It is proved that this model can effectively learn to generate proper and informative responses by utilizing appropriate knowledge by utilizing knowledge . The paper is well-written and easy to follow . The idea of separating the posterior and prior distributions over knowledge is interesting and novel . However , I have the following concerns . 1 .The authors claim that the proposed method is able to avoid the discrepancy between the posterior distributions over utterances and responses . But , it is not clear to me how the model can avoid this discrepancy . For example , given an utterance , different responses can be generated depending on whether appropriate knowledge is used in the response . If the model is training by selecting wrong knowledge ( e.g. , K2 in R2 ) or knowledge irrelevant to the true response ( i.e .K1 in R1 ) , it can be seen that they are completely useless since they can not provide any helpful information . This kind of discrepancy would stop the model from learning to generating proper responses by using appropriate knowledge . 2 .The proposed approach is not compared with other related works . For instance , [ Dinan et al. , 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 3 .In Table 1 , there is no comparison with [ Ghazvininejad et al .2018 ] , which also uses knowledge selection to guide the knowledge selection . 4 .It is also important to properly incorporate knowledge in response generation , such as K1 and K3 in R4 in the Wizard of Wikipedia dataset . 5 .The paper is not well-motivated . The motivation of this paper is to solve the problem of knowledge-based dialogue generation , which has not been sufficiently studied in the previous work . 6 .There are many typos in the paper . For the example in Section 3.1 , the sentence `` In this dataset , each agent is associated with a persona profile , which is served as knowledge . `` should be removed .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea of the paper is to prune the architecture of the model by optimizing the number of parameters and the pre-training loss . The experiments show that the proposed pruning method can improve the performance of BERT . The paper is well written and easy to follow . However , I have some concerns about the experiments . 1 .The experiments are only conducted on MNLI dataset . It would be more convincing if the authors can conduct experiments on other NLP tasks . 2 .The pruning based architecture search is only applied to BERT architecture . It is not clear how the pruning can be applied to other models . 3 .The authors should compare with other pruning methods , e.g. , [ 1 ] , [ 2 ] and [ 3 ] . 4 .It would be interesting to see the results of pruning BERT on other language modeling tasks , such as NLP , machine translation , etc . 5 .The paper is not well organized . For example , in the introduction , the authors claim that “ The ratio of the architecture design dimensions within a BERT encoder layer can be modified to obtain a layer with better performance . ” .However , the paper does not explain how this can be done . 6 .In the experiments , it would be better to show the results on other tasks . 7 .In Table 1 , it is better to report the accuracy of the pruned BERT models . 8 .The results on Table 2 are not convincing . For instance , the results in Table 1 are not consistent with the results reported in the paper . 9 .In Figure 2 , why the results are not better than the ones reported in [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,47,48,50,49,51,52,53,54,55,56,58,59,60,61,62,63,64,70,72,73,72 ,73,74,75,76,80,83,84,81,88,89,90,91,93,90 ,93,94,94 ,94,96,98,99,99 ,98,101,102,103,103 ,104,104 ,104 ,105,104,105,106 ,107,108 ,108 ,109 ,110 ,111 ,112 ,113 ,114 ,115 ,116 ,117 ,118 ,119 ,122 ,123 , 123 , 128 , 130 , 131 , 128,129 , 130,131 ,131 ,132 ,133 ,134 ,135 ,136 ,137 ,138 ,139 ,140 ,141 ,142 ,143 ,144 ,145 ,150 ,153 ,154 ,155 ,156 ,157 ,163 ,168 ,178 ,179 ,180 ,181 ,183 ,184 ,185 ,186 ,197 ,198 ,199 ,207 ,208 ,207 . 10 .In Section 3.1 , it seems that the authors did not compare with the existing pruning techniques in the literature . It will be better if the author can compare with them . 11 .In section 3.2 , the author claims that ‘ The fully-connected component applied to each token separately plays a much more significant role in the top layers as compared to the bottom layers ’ . This is not true . In fact , the fully connected component is not the same as the attention head . 12 .In table 1 , the result of ‘ tall and narrow ’ architecture is better than ‘ wide and shallow ’ and ‘ shallow and tall ’ architectures . 13 .In figure 2 , the difference between ‘ top and bottom ’ is not very clear . 14 .The author should provide more details about the hyper-parameters of the proposed method . 15 .The proposed method is not compared with the other pruned methods . The author should also compare with these methods .
1_2102.07983 = This paper introduces a new dataset , FEWS ( Few-shot Examples of Word Senses ) , to train and evaluate word sense disambiguation models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary , which contains examples for over 71,000 senses , which is much higher than existing datasets such as SemCor , the largest manually annotated WSD dataset , which only covers about 33,000 . The authors also conduct experiments on knowledge-based approaches and a recent neural biencoder model for WSD . The paper is well-written and easy to follow . The proposed dataset is a good addition to the literature on WSD , and the experiments are well-conducted . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset for training and evaluating WSD models in a few- and zero shot setting . The contribution of the dataset is two-fold : ( 1 ) it exposes models a broad array of senses in a low-shot setting , and ( 2 ) the large evaluation set allows for more robust evaluation of rare senses . 2 .The experiments are conducted on knowledge based approaches and the recent Neural BienCoder model , which are the strongest baselines . However it is not clear whether the proposed dataset can be used as additional training data for other WSD tasks such as transfer learning . It would be better if the authors can show the performance of the proposed datasets on transfer learning tasks .
1_1812.02425 = This paper proposes to use a teacher-student ensembling framework to learn an ensemble of multiple neural networks without incurring any additional testing costs . The idea is to use the outputs of different neural networks as supervisions to guide the target network training . To further improve the robustness of the student networks , the authors introduce an adversarial learning strategy to force the student to generate similar outputs as teachers . The experiments show that MEAL consistently improves the accuracy across a variety of popular network architectures on different datasets . The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a number of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . It is a simple extension of Swapout ( Singh , Hoiem , and Forsyth 2016 ) . 2 .The proposed method can achieve the goal of ensembleling multiple neural network with no additional testing cost . 3 .The experimental results are not convincing . For example , in Table 1 , the performance of Shake-Shake ( Gastaldi 2017 ) and MEAL ( Meals et al. , 2017 ) are not better than Shake-shake and Meals , respectively , in terms of accuracy .
1_1901.06829 = This paper proposes a novel method for grounding natural language descriptions in videos . The proposed method is based on an end-to-end reinforcement learning ( RL ) framework , where the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . At each time step , the agent makes an observation that details the currently selected video clip , and takes an action according to its policy . The environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . In addition , the authors introduce a location feature , which enables the agent to explicitly access the current grounding boundaries . The paper is well-written and easy to follow . The method is novel and well-motivated . The experimental results show that the proposed method outperforms the state-of-the-art methods . However , I have the following concerns : 1 . The novelty of the method is limited . There are several existing methods for video grounding , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) , which can be applied only when all candidates are exhaustively enumerated , as they need to rank all possible clip-sentence pairs to be successful in generating the final grounding result . Moreover , the ranking strategy is specially designed for pre-segmented videos and is not applicable to general cases . 2 .The proposed method can not be applied to general video grounding tasks . 3 .The experiments are not convincing . For example , in Table 1 , the performance of the proposed methods is not better than the baseline methods . 4 .In Table 2 , the results are not consistent with the results reported in the original paper .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a new SAT solver to solve the problem of determining the existence of a projective plane of order 10 . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates were generated with the assistance of the symbolic computation library Traces ( McKay and Piperno 2014 ) , but it is not clear how one can verify these certificates without needing to trust the output of the library . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that their work is the first SAT-based resolution of Lamâs problem with all of the exhaustive search work completed by SAT solvers . I am not sure if this statement is correct . The previous SAT-SAT solvers have become so strong that they are “ the best solution in most cases ” . Even still , they note that some problems such as some problems like Lam’s Problem have only been solved by specialpurpose means . 2 .In the proof of Theorem 3.1 , the authors assume that the code associated with projective planes of order ten must contain words that are referred to as weight 15 , weight 16 , or ‘ weight 19 ’ words . The former two cases were first ruled out by the searches of ( Lam , Thiel , and Swiercz 1989 ) and ( MacWilliams , Sloane , and Thompson ) and were recently settled via SAT-Based Existence certificates ( Bright et al. , 2020a ,b ) . It would be better if the authors can clarify the difference between their work and these previous works . 3 .The proof of theorem 3.2 is not very clear . It seems that the proof is based on the fact that there are 66 possible A1s and A2s , which is not true for the case of A2 . It is better to clarify this point . 4 .The authors should provide more details about the verification procedure . For example , how to verify the correctness of the certificates ? 5 .The paper is not well-organized . For instance , it is hard to follow the description of the proof in Section 4.1 .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and the Weisfeiler-Lehman ( WL ) graph isomorphism test . The authors show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of nodes as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNN is thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . The paper rigorously study several variants of multisett functions and theoretically characterize their discriminativity power . The results show that under certain conditions , GNN-based graph structures that can not be distinguished by popular GNN variants , such as GCN ( Kipf & Welling , 2017 ) and GraphSAGE ( Hamilton et al. , 2017a ) , and show that the power of GIN ( Santoro et al .2017 ) is equal to that of WL .The authors also develop a simple neural architecture , Graph Isomorphism Network ( GIN ) , which can capture graph structures such as graph structure . The proposed GIN can achieve state-of-the-art performance on node classification , link prediction , and graph classification tasks . However , there is little theoretical understanding of the properties and limitations of GNN , and formal analysis of the representational capacity is limited . Therefore , the design of new GNN based models is mostly based on empirical intuition , heuristics , and experimental trial-anderror .
1_1907.07355 = This paper investigates the performance of BERT on the ARCT dataset . The authors show that the model is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated by adding a copy of each data point with the claim negated and the label inverted . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that BERT ’ s performance can be entirely accounted for in terms of exploiting spurious statistical cues , but they do not provide any evidence to support this claim . 2 .In Table 1 , BERT achieves 77 % test set accuracy with its best run ( Table 1 ) , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question “ what has BERT learned about argument comprehension ? ” 3 .In the experiments , the authors only compare BERT with the SemEval dataset . It would be better if the authors can also compare with the proposed adversarial dataset , which is more challenging .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The proposed method is simple to implement and has little computational overhead . The paper is well written and easy to follow . The authors prove that for quadratic objective functions , SWA can converge to the optimal solution as well as to a smaller asymptotic noise ball than low precision SGD for strongly convex objectives . Empirically , the authors show that training with 8 bit SWA with full precision can match the full precision baseline in deep learning tasks such as ResNet-164 and CIFAR-10/100 datasets . The experimental results show that SWA outperforms the state-of-the-art in terms of generalization performance . Pros : 1 . The idea of using SWA to reduce the quantization noise during training is interesting and novel . 2 .The paper is clearly written . 3 .The experimental results are convincing . 4 .The theoretical analysis is sound . Cons : 1.1 . It is not clear to me why SWA works well with a relatively high learning rate and can tolerate additional noise . It would be better to show the performance of SWA when the learning rate is low . 2.2 .It would be more convincing if the authors can show the convergence rate of the proposed method with a fixed learning rate . 3.3 .It is better to compare the performance with other quantization methods such as quantized SGD . 4.4 .There are some typos and grammatical errors in the paper .
1_1902.04911 = This paper proposes a novel approach to knowledge-grounded dialogue generation . The authors propose to separate the posterior distribution over knowledge from the prior distribution over utterance and response . The proposed model is trained to minimize the KL divergence between the prior and posterior distributions . Then , during the inference process , the model samples knowledge merely based on the prior prior distribution and incorporates the sampled knowledge into response generation . It is proved that this model can effectively learn to generate proper and informative responses by utilizing appropriate knowledge by utilizing knowledge . The paper is well-written and easy to follow . The idea of separating the posterior and prior distributions over knowledge is interesting and novel . However , I have the following concerns . 1 .The authors claim that the proposed method is able to avoid the discrepancy between the posterior distributions over utterances and responses . But , it is not clear to me how the model can avoid this discrepancy . For example , given an utterance , different responses can be generated depending on whether appropriate knowledge is used in the response . If the model is training by selecting wrong knowledge ( e.g. , K2 in R2 ) or knowledge irrelevant to the true response ( i.e .K1 in R1 ) , it can be seen that they are completely useless since they can not provide any helpful information . This kind of discrepancy would stop the model from learning to generating proper responses by using appropriate knowledge . 2 .The proposed approach is not compared with other related works . For instance , [ Dinan et al. , 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 3 .In Table 1 , there is no comparison with [ Ghazvininejad et al .2018 ] , which also uses knowledge selection to guide the knowledge selection . 4 .It is also important to properly incorporate knowledge in response generation , such as K1 and K3 in R4 in the Wizard of Wikipedia dataset . 5 .The paper is not well-motivated . The motivation of this paper is to solve the problem of knowledge-based dialogue generation , which has not been sufficiently studied in the previous work . 6 .There are many typos in the paper . For the example in Section 3.1 , the sentence `` In this dataset , each agent is associated with a persona profile , which is served as knowledge . `` should be removed .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea of the paper is to prune the architecture of the model by optimizing the number of parameters and the pre-training loss . The experiments show that the proposed pruning method can improve the performance of BERT . The paper is well written and easy to follow . However , I have some concerns about the experiments . 1 .The experiments are only conducted on MNLI dataset . It would be more convincing if the authors can conduct experiments on other NLP tasks . 2 .The pruning based architecture search is only applied to BERT architecture . It is not clear how the pruning can be applied to other models . 3 .The authors should compare with other pruning methods , e.g. , [ 1 ] , [ 2 ] and [ 3 ] . 4 .It would be interesting to see the results of pruning BERT on other language modeling tasks , such as NLP , machine translation , etc . 5 .The paper is not well organized . For example , in the introduction , the authors claim that “ The ratio of the architecture design dimensions within a BERT encoder layer can be modified to obtain a layer with better performance . ” .However , the paper does not explain how this can be done . 6 .In the experiments , it would be better to show the results on other tasks . 7 .In Table 1 , it is better to report the accuracy of the pruned BERT models . 8 .The results on Table 2 are not convincing . For instance , the results in Table 1 are not consistent with the results reported in the paper . 9 .In Figure 2 , why the results are not better than the ones reported in [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,47,48,50,49,51,52,53,54,55,56,58,59,60,61,62,63,64,70,72,73,72 ,73,74,75,76,80,83,84,81,88,89,90,91,93,90 ,93,94,94 ,94,96,98,99,99 ,98,101,102,103,103 ,104,104 ,104 ,105,104,105,106 ,107,108 ,108 ,109 ,110 ,111 ,112 ,113 ,114 ,115 ,116 ,117 ,118 ,119 ,122 ,123 , 123 , 128 , 130 , 131 , 128,129 , 130,131 ,131 ,132 ,133 ,134 ,135 ,136 ,137 ,138 ,139 ,140 ,141 ,142 ,143 ,144 ,145 ,150 ,153 ,154 ,155 ,156 ,157 ,163 ,168 ,178 ,179 ,180 ,181 ,183 ,184 ,185 ,186 ,197 ,198 ,199 ,207 ,208 ,207 . 10 .In Section 3.1 , it seems that the authors did not compare with the existing pruning techniques in the literature . It will be better if the author can compare with them . 11 .In section 3.2 , the author claims that ‘ The fully-connected component applied to each token separately plays a much more significant role in the top layers as compared to the bottom layers ’ . This is not true . In fact , the fully connected component is not the same as the attention head . 12 .In table 1 , the result of ‘ tall and narrow ’ architecture is better than ‘ wide and shallow ’ and ‘ shallow and tall ’ architectures . 13 .In figure 2 , the difference between ‘ top and bottom ’ is not very clear . 14 .The author should provide more details about the hyper-parameters of the proposed method . 15 .The proposed method is not compared with the other pruned methods . The author should also compare with these methods .
1_2102.07983 = This paper introduces a new dataset , FEWS ( Few-shot Examples of Word Senses ) , to train and evaluate word sense disambiguation models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary , which contains examples for over 71,000 senses , which is much higher than existing datasets such as SemCor , the largest manually annotated WSD dataset , which only covers about 33,000 . The authors also conduct experiments on knowledge-based approaches and a recent neural biencoder model for WSD . The paper is well-written and easy to follow . The proposed dataset is a good addition to the literature on WSD , and the experiments are well-conducted . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset for training and evaluating WSD models in a few- and zero shot setting . The contribution of the dataset is two-fold : ( 1 ) it exposes models a broad array of senses in a low-shot setting , and ( 2 ) the large evaluation set allows for more robust evaluation of rare senses . 2 .The experiments are conducted on knowledge based approaches and the recent Neural BienCoder model , which are the strongest baselines . However it is not clear whether the proposed dataset can be used as additional training data for other WSD tasks such as transfer learning . It would be better if the authors can show the performance of the proposed datasets on transfer learning tasks .
1_1812.02425 = This paper proposes to use a teacher-student ensembling framework to learn an ensemble of multiple neural networks without incurring any additional testing costs . The idea is to use the outputs of different neural networks as supervisions to guide the target network training . To further improve the robustness of the student networks , the authors introduce an adversarial learning strategy to force the student to generate similar outputs as teachers . The experiments show that MEAL consistently improves the accuracy across a variety of popular network architectures on different datasets . The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a number of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . It is a simple extension of Swapout ( Singh , Hoiem , and Forsyth 2016 ) . 2 .The proposed method can achieve the goal of ensembleling multiple neural network with no additional testing cost . 3 .The experimental results are not convincing . For example , in Table 1 , the performance of Shake-Shake ( Gastaldi 2017 ) and MEAL ( Meals et al. , 2017 ) are not better than Shake-shake and Meals , respectively , in terms of accuracy .
1_1901.06829 = This paper proposes a novel method for grounding natural language descriptions in videos . The proposed method is based on an end-to-end reinforcement learning ( RL ) framework , where the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . At each time step , the agent makes an observation that details the currently selected video clip , and takes an action according to its policy . The environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . In addition , the authors introduce a location feature , which enables the agent to explicitly access the current grounding boundaries . The paper is well-written and easy to follow . The method is novel and well-motivated . The experimental results show that the proposed method outperforms the state-of-the-art methods . However , I have the following concerns : 1 . The novelty of the method is limited . There are several existing methods for video grounding , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) , which can be applied only when all candidates are exhaustively enumerated , as they need to rank all possible clip-sentence pairs to be successful in generating the final grounding result . Moreover , the ranking strategy is specially designed for pre-segmented videos and is not applicable to general cases . 2 .The proposed method can not be applied to general video grounding tasks . 3 .The experiments are not convincing . For example , in Table 1 , the performance of the proposed methods is not better than the baseline methods . 4 .In Table 2 , the results are not consistent with the results reported in the original paper .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a SAT solver to solve the projective plane of order 10 problem , which is the most challenging subcase of Lamâs problem . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates were generated with the assistance of the symbolic computation library Traces ( McKay and Piperno 2014 ) , but the authors describe how one can verify the certificate without needing to trust the output of the library . The paper is well written and easy to follow . However , I have the following concerns : 1 . It is not clear to me how to verify the correctness of the certificates generated by the Traces library . For example , how to check whether the certificates are valid or not ? 2 .In the proof of Theorem 3.1 , the authors claim that the certificate verifier is more verifiable because it can check the certificates for themselves and ( once they believe in the encoding ) be convinced in the nonexistence of a projective planes of order ten without having to trust a search procedure . But I am not sure if this is the case . 3 .The proof of theorem 3.2 is not correct . The proof is based on the fact that there is no proof of the existence of the plane of the order of 10 . 4 .In Theorem 4.2 , it is claimed that there are 66 possible A1s and A2s , but there are only 66 possibilities for the A1 case . I am wondering if the authors can provide more details on how to choose the number of possibilities . 5 .I am not convinced about Theorem 5.1 . It seems that the proof is only valid for the case of A2 and A1 , but not for the other two cases . 6 .In theorem 5.3 , it seems that it is not possible to prove that the certificates of the certificate are valid .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and the Weisfeiler-Lehman graph isomorphism test ( WL ) test . The authors show that GNN can be as powerful as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to study the power of different GNN variants in learning to represent and distinguish between different graph structures . The key insight is that a GNN has as large discriminative power as WL if the aggregation scheme can be highly expressive . The analysis is based on the assumption that the set of feature vectors of a given node 's neighbors is represented as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNN is thought of as an aggregation function over the multisets . Hence , to have strong representational power , the aggregation function must be able to aggregate different multi-sets into different representations . 2 .Theorem 3.1 shows that GIN is as powerful in distinguishing different graphs as the graph isomorphic test . But , it is not clear why GIN can not distinguish the graph structures such as GCNSAGE and GraphSAGE . 3 .The experimental results are not convincing . In Table 1 , the performance of GNN-GIN is not better than WL-WL test . It is hard to see the advantage of GIN . 4 .In Table 2 , the results of GCN-SAGE are not comparable to the results in Table 1 . 5 .In the experiments , the number of parameters in GIN seems to be too small . It would be better if the authors can provide more details .
1_1907.07355 = In this paper , the authors investigate the performance of BERT on the ARCT SemEval dataset . They find that BERT is able to exploit the presence of spurious statistical cues in the warrant , especially `` not `` . They show that the major problem can be eliminated in ARCT by adding a copy of each data point with the claim negated and the label inverted . This means that the distribution of statistical cue in the warrants will be mirrored over both labels , eliminating the signal . On this adversarial dataset all models perform randomly , with BERT achieving a maximum test set accuracy of 53 % . The adversarial data provides a more robust evaluation of argument comprehension and should be adopted as the standard in future work on this dataset . The paper is well written and easy to follow . However , I have the following concerns : 1 . The authors claim that the reason for BERT ’ s surprising performance can be entirely accounted for in terms of exploiting spurious statistical cue , however , they do not provide any evidence to support this claim . 2 .In Table 1 , BERT achieves 77 % accuracy with its best run , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well .
1_1904.11943 = This paper proposes a new method for low-precision training of deep neural networks . The proposed method is based on stochastic weight averaging ( SWALP ) , where all the numbers except the gradient accumulator are quantized to 8 bits without changing the network structure . The authors prove that SWA converges to the optimal solution for quadratic objectives with no loss of accuracy from the quantization noise . For strongly convex objectives , the proposed method converges a smaller asymptotic noise ball than that of low precision SGD . The paper is well written and easy to follow . The method is simple to implement and has little computational overhead . The experimental results show that 8-bit SWA can match the full precision training of SGD on CIFAR-10/100 and VGG-16 with both VGG and PreResNet-164 datasets .
1_1902.04911 = This paper proposes a novel approach for dialogue generation based on knowledge-grounded generative models . The authors propose to separate the prior distribution over knowledge and the posterior distribution over responses . The posterior distribution is inferred from both the utterance and the response , while the prior is inferred only from the response . The model is trained to minimize the KL divergence between the prior and posterior distributions so that the model can approximate the posterior . Then , during the inference process , the model samples knowledge merely based on the prior distributions and incorporates the sampled knowledge into response generation . The paper is well written and easy to follow . The proposed approach is novel and the experiments are convincing . However , I have some concerns about the novelty of the proposed approach . 1 .The proposed approach seems to be a straightforward combination of existing approaches . It is not clear to me why the authors chose to use the knowledge graph as the prior instead of the ground-truth knowledge . 2 .In the experiments , the authors only compare with the baselines on the Wizard-of-Wikipedia dataset . It would be interesting to see the performance of the approach on the Persona-chat dataset as well . 3 .It would be good to see how the approach compares with other approaches such as the commonsense model proposed by Dinan et al . [ ? ] .4 .The authors claim that the approach is able to generate more informative responses by incorporating knowledge into the response generation , but it is not shown in the experiments .
1_2005.06628 = This paper proposes a pruning-based architecture search method to improve the performance of the BERT model by pruning the number of parameters and reducing the size of the architecture design dimensions . The pruning method is based on the idea of pruning each layer of BERT by five different dimensions , instead of two . The authors show that the ratio of the pruning dimensions within a BERT encoder layer can be modified to obtain a layer with better perfor- Design dimensions . This paper is well-written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me why the authors chose to prune the design dimensions instead of parameter sharing across layers and factorizing the embedding layers . 2 .The pruning based architecture search technique is only applied to the pre-training loss and not to the model parameters . It would be better if the authors can show the results on the training loss and model parameters in the experiments . 3 .The authors should compare with the results of ALBERT ( https : //arxiv.org/abs/1711.06570 ) and RoBERTa ( http : //proceedings.mlr.press/v48/roberta.pdf ) . 4 .In the experiments , the authors only compare the performance on MNLI and MRPC SST-2 , which are not state-of-the-art . The results on SQuAD v1.1 and v2.0 should be reported .
1_2102.07983 = This paper introduces a new dataset , FEWS ( Few-shot Examples of Word Senses ) , which is used to train and evaluate Word Sense Disambiguation ( WSD ) models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary definitions of 71,000 senses , which are used as training data for WSD models . The paper is well-written and easy to follow . The idea of using Wikipedia definitions of senses for training and evaluation of WSD is interesting , and the dataset is a good addition to the literature . However , I have the following concerns : 1 . It is not clear to me why the authors did not compare with existing datasets such as SemCor ( SemCor is the largest manually annotated WSD dataset ) . 2 .The authors claim that the proposed dataset is better than SemCor because it has more senses , but it is unclear to me how it compares with SemCor in terms of the number of senses . 3 .It is also not clear how the authors compare the performance of the proposed datasets with the state-of-the-art baselines such as the biencoder and knowledge-based approaches . 4 .The transfer learning experiments are not convincing . The transfer learning results are not very convincing . For example , in Table 2 , the transfer learning performance of BienCoder is not better than human annotators , while in Table 3 , it is worse than human . It would be better if the authors can provide more explanations .
1_1812.02425 = This paper proposes to use a teacher-student ensembling framework to learn an ensemble of multiple neural networks without incurring any additional testing costs . The idea is to use the combination of diverse outputs from different neural networks as supervisions to guide the target network training . To further improve the robustness of student networks , the authors introduce an adversarial learning strategy to force the student to generate similar outputs as teachers . The experiments show that MEAL consistently improves the accuracy across a variety of popular network architectures on different datasets . The paper is well written and easy to follow . The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a number of existing network architectures . However , I have the following concerns : 1 . The authors claim that the proposed method can achieve the goal of ensembleling multiple neural network with no additional testing cost . But , in this paper , the learning-based ensemble method is not compared with the traditional ensemble method . 2 ) Ensemble is always large and slow : Ensemble requires more computing operations than an individual network , which makes it unusable for applications with limited memory , storage space , or computational power such as desktop , mobile and even embedded devices , and for applications in which real-time predictions are needed . To address the aforementioned shortcomings , I suggest the authors to conduct an end-to-end framework with adversarial training .
1_1901.06829 = This paper proposes a novel method for grounding natural language descriptions in videos . The proposed method is based on an observation network that takes visual and textual information as well as current temporal boundaries into consideration . Given the observation , the decision making policy is modeled by a policy network that outputs the probability distribution over all possible actions , and then adjusts the grounding boundary according to the policy . The policy network is implemented using a recurrent neural network in order to leverage the feature ground truth for temporal boundary regression as well at each step . Experimental results show that the proposed method achieves state-of-the-art performance on two well-known datasets and validate the superiority of the proposed model . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of this paper is limited . The main contribution of this work is the observation network and the policy network , which are not novel . 2 .The proposed method can be trained end-to-end for natural language grounding within several steps of glimpses ( observations ) and temporal boundaries ( actions ) . 3 .The experimental results are not convincing . In Table 1 , the performance of Charades and ActivityNet DenseCaption is not better than the baseline method . 4 .In Table 2 , the results of the multi-task learning is not clear . It is better to show the results for the multi task learning . 5 .In Figure 3 , it is hard to see how the performance is improved by the use of the location feature . It would be better to include the results with the results from the original Charades dataset .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a new SAT solver to solve the problem of determining the existence of a projective plane of order 10 . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates were generated with the assistance of the symbolic computation library Traces ( McKay and Piperno 2014 ) , but the authors describe how one can verify the certificate without needing to trust the output of the library . The paper is well written and easy to follow . However , I have the following concerns : 1 . It is not clear to me how to verify the correctness of the certificates generated by the Traces library . For example , how to check whether the certificates are valid ? 2 .The authors claim that the certificate is more verifiable because it is more reliable than writing special-purpose search code , but I do not see how the certificate verifier can verify it . 3 .In the proof of Theorem 3.1 , the authors state that the proof is based on the fact that the code associated with a hypothetical projective planes of order ten must contain that are weight 16 , weight 19 , or `` primitive `` weight 19 words . I am not sure if this is correct . 4 .The proof of theorem 3.2 is not correct . The authors should check the proof in the supplementary material . 5 .In Theorem 4.2 , the proof should be more precise . 6 .In theorem 4.3 , the definition of the number of points in the plane is not consistent with the definition in Theorem 2.1 .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and the Weisfeiler-Lehman ( WL ) graph isomorphism test . The authors show that GNN can be as powerful as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to show that the power of GNN is equal to that of WL , but it is not clear why this is the case . For example , in the proof of Theorem 3.1 , the authors claim that the GIN model is as expressive as WL if the aggregation function is injective . But in the experiments , the performance of GIN is not better than WL . 2 .The experimental results are not convincing . In Table 1 , the results of GCNSAGE and GIN are not comparable . The reason for this is that the authors did not compare with other GNN variants . 3 .In Table 2 , the comparison with WL is not fair . It is better to compare with more recent GNN-based models such as Graph Isomorphism Network ( GIN ) . 4 .In the experiment , the number of parameters in GIN seems to be too small . It would be better if the authors can provide more details .
1_1907.07355 = This paper presents an analysis of the performance of BERT on the argument reasoning mining task ( ARCT ) . The authors show that BERT is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated in ARCT by adding a copy of each data point with the claim negated and the label inverted . This means that the distribution of statistical cues in the warrants will be mirrored over both labels , eliminating the signal . On this adversarial dataset all models perform randomly , with BERT achieving a maximum test set accuracy of 53 % . This paper provides a more robust evaluation of argument comprehension and should be adopted as the standard in future work on this dataset . The paper is well written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me how to interpret the results in Table 1 . In Table 1 , BERT achieves 77 % accuracy with its best run , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question : what has BERT learned about argument comprehension ? To investigate BERT ’ s decision making , the authors looked at data points it finds easy to classify over multiple runs . 2 .In Table 2 , the results are not consistent with the results of Habernal et al . ( 2018b ) , which shows BERT exploits “ cue words ” in the ARCT SemEval submissions , and consistent with their results . It would be better if the authors can provide more explanation about this result .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The paper is well written and easy to follow . The proposed method is simple to implement and has little computational overhead . For quadratic objective , the proposed method converges to the optimal solution with no loss of accuracy from the quantization . For strongly convex objective , it can converge to a smaller asymptotic noise ball than SGD for low precision training . Empirical results show that training with 8-bit SWA can match the full precision SGD baseline in deep learning tasks such as training ResNet and CIFAR-10 and CKrizhevsky & HIF-100 datasets . I think this paper makes the following contributions : 1 . It shows that SWA is able to reduce the performance gap between low precision and full precision training and shows that it can significantly reduce the training time between the two . 2 .It shows that the proposed SWA method can achieve better generalization performance compared to the state-of-the-art methods . 3 .The proposed method can also reduce the computational cost compared to SGD . 4 .The experimental results are convincing .
1_1902.04911 = This paper proposes a method for knowledge-grounded dialogue generation . The authors propose to use knowledge selection mechanism to separate the prior distribution over knowledge from the posterior distribution over the response distribution . The model is trained to minimize the distance between the prior and posterior distributions . Then , the model samples knowledge based on the prior prior distribution and incorporates the sampled knowledge into response generation . Experiments are conducted on Persona-chat and Wizard of Wikipedia dataset . The paper is well written and easy to follow . The idea of knowledge selection is interesting and novel . However , I have some concerns about the experiments . 1 .The authors claim that the proposed method is better than the baselines . But , there is no comparison with baselines in Table 1 . It would be better if the authors can show the performance of baselines on the same dataset . 2 .In Table 1 , the authors should show the results of the model without knowledge selection . It is better to compare with the model with knowledge selection and model without prior distribution . 3 .It is better if authors can provide more details about the training procedure . For example , how many iterations are used in the training process ? 4 .In the experiments , it would be more convincing if the author can show results on more datasets . 5 .The author should provide more explanations about the results in Table 2 .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea is to prune the architecture of the model by minimizing both the pre-training loss and the number of model parameters . The paper is well written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me what is the advantage of the proposed pruning method compared to other pruning methods . For example , the pruning of the attention heads in ALBERT ( Lan et al. , 2019 ) . 2 .The authors claim that the proposed method can reduce the model parameters while increasing the latency , but the experiments do not show the effect of pruning on the model performance . 3 .The experiments are only conducted on the MNLI task . It would be better if the authors can conduct experiments on other NLP tasks , e.g. , WMT , NLI , etc . .
1_2102.07983 = This paper introduces a new dataset called FEWS ( Few-shot Examples of Word Senses ) , which is a dataset for evaluating word sense disambiguation models in few-shot and zero-shot settings . The dataset is built on top of the Wiktionary dataset , which contains 71,000 senses . The authors compare the performance of knowledge-based approaches and a recent neural biencoder model for WSD on the new dataset . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset to evaluate WSD models in the low-shot setting , but it is not clear to me why this dataset is better than existing datasets such as SemCor and SemNet . 2 .The authors claim that the dataset is more comprehensive than SemCor , but I am not sure why this is the case . SemCor is the largest manually annotated WSD dataset , while the dataset in this paper only contains 71 ,000 senses , which makes it hard to compare with SemCor . 3 .In Table 1 , the authors mention that the BienCoder outperforms human annotators , but there is no comparison with the state-of-the-art on the dataset . 4 .The transfer learning experiment is interesting , but the authors do not provide any results on transfer learning . 5 .The paper is not well-organized . For example , in the introduction , it is hard to understand what is the purpose of Table 1 and Table 2 . 6 .In Section 3.2 , the paper claims that the proposed dataset will be used as additional training data for other WSD tasks , but no results are provided .
1_1812.02425 = This paper proposes a method for ensembling multiple neural networks with no additional testing cost . The proposed method is based on the teacher-student learning paradigm , where the teacher network is pre-trained and the student network is trained using adversarial learning . The authors show that the proposed method consistently improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a variety of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of this paper is limited . The idea of using the soft labels as supervision is not new . It has been proposed in [ 1 ] , [ 2 ] and [ 3 ] . 2 .The proposed method can achieve the goal of ensembled multiple neural network with no extra testing cost , but the performance gain is not significant . 3 .The experiments are not convincing . For example , in Table 1 , the performance of Shake-Shake-MEAL is not better than the baseline method , while the performance improvement of MEAL is small . 4 .In Table 2 , it is not clear why the performance is better than Shake-shake-MeAL . 5 .The authors should compare the performance with other ensembles , e.g. , [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,44,47,48,50,49,51,52,54,55,56,58,59,60,61,63,64,70,72,73,72 ,73,74,75,76,77,78,80,81,82,83,84,88,89,90,91,93,94,93 ,94,98,99,99 ,100,100,103,104 ,104,104,105,106,107,108 ,108,108,111,113,114,115,116,117,118,119,120,121,122,122 ,128,128,129,131,128 ,128 ,129,129 ,131,132,131 ,131 ,132,133,136,137,136 ,128-129 ,136-136 ,137-137 ,138-138 ,139-139 ,144-140 . [ 1 , 1 ] https : //arxiv.org/abs/1806.059062 [ 2 , 2 ] http : //proceedings.mlr.press/v48/papers/1605.06570 [ 3 , 3 ] https: //arXiv:1705.09737.03780 [ 4 ] https://arxives.cc/abs-1605/1606.03880 [ 5 ] https //ar xiv.cc /abs/1607.03902 [ 6 ] httpsÂ Arxiv:1611.06470 [ 7 ] https { Arxives : https :Â ArXiv.pdf ] https = = = > https : > ArXives.org [ 8 ] [ 9 ]
1_1901.06829 = This paper proposes an end-to-end reinforcement learning ( RL ) based framework for grounding natural language descriptions in videos . The paper is well-written and easy to follow . The proposed method is novel and well-motivated . However , I have the following concerns : 1 . The novelty of the method is limited . The method is a simple combination of existing methods . For example , the proposed method can not be compared with the state-of-the-art methods for action localization and video grounding . 2 .The method is not compared with other methods for video grounding , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) .3 .The proposed method does not outperform the baselines . 4 .The experimental results are not convincing . In Table 1 , the performance of the proposed methods is not better than the baseline methods . The authors should compare the performance with the baseline methods . 5 .In Table 2 , the authors should show the results of the baseline method .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a SAT solver to solve the projective plane of order 10 problem , which is the most challenging subcase of Lam 's problem . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates are generated with the assistance of the symbolic computation library Traces . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that the certificates are more verifiable , but they do not provide a formal proof of the nonexistence of projective planes of order ten because they rely on results that currently have no formal computerverifiable proofs . 2 .It is not clear to me how the certificates can be verified by a third party . 3 .In the proof of Theorem 3.1 , the authors use a special-purpose solver , but I do not know how to verify the correctness of the certificates . 4 .The proof of theorem 3.2 is not correct . The proof is based on the fact that the error correcting code must contain words that are called `` primitive `` or `` weight `` 19 words , which are the first two cases that are first ruled out by the search of ( MacWilliams , Slane , and Thompson 1973 ) and were recently settled via SAT-based nonexistence certificates ( Bright et al. , 2020a ,b ) . 5 .In Theorem 4.2 , the proof is not complete . It is not possible to prove the existence of the plane .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and shows that they are at most as powerful as the Weisfeiler-Lehman ( WL ) graph isomorphism test in distinguishing graph structures . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of feature vectors of a given node ‘ s neighbors as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNNS can be thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . 2 .The paper also shows that the most powerful GNN by our theory , ie., Graph Isomorphism Network ( GIN ) , also empirically has high representation power as it fits the data perfectly . 3 .The experimental results show that the proposed GIN can not distinguish graph structures that can not be distinguished by popular GNN variants , GCN ( Kipf & Welling , 2017 ) and GraphSA ( Hamilton et al. , 2017a ) and GIN ( Velickovic et .al. , 2018 ) . 4 .In the experiments , the authors compare the performance of GIN with various aggregation functions with various graph aggregation functions . It would be better if the authors can compare the results with the results of GNN with different aggregation functions and graph-level pooling schemes .
1_1907.07355 = This paper investigates the reason why BERT performs so well on the argument reasoning mining task . The authors show that BERT is able to exploit the presence of statistical cues in the warrants in the ARCT task . They also show that the major problem can be eliminated in ARCT by adding a copy of each data point with the claim negated and the label inverted . The paper is well written and easy to follow . However , I have the following concerns : 1 . In the experiments , the authors only compare BERT with the unsupervised baseline . It would be better if the authors can compare with the state-of-the-art . 2 .In Table 1 , the performance of BERT on the adversarial dataset is much lower than the human performance . It will be better to compare the performance with the human baseline . 3 .The authors should provide more details about the dataset used in the experiments . For example , what is the size of the dataset ? What is the number of training examples ? How many training examples were used in each experiment ? 4 .In the experiment , it is better to show the performance on the full dataset .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The paper is well written and easy to follow . The proposed method is simple to implement and has little computational overhead . For quadratic objective , the proposed method converges to a noise ball that is asymptotically smaller than that of low precision SGD . For strongly convex objective , it can converge to the optimal solution with no loss of accuracy from the quantization . Empirical results show that 8-bit SWA can match the full precision training with SGD baseline in deep learning tasks such as training ResNet-164 on Cifar-10 and CIFAR-100 datasets . Pros : 1 . The idea of using SWA to reduce the performance gap between full precision and low precision training is interesting . 2 .The proposed method can reduce the computational cost of training DNNs more efficiently and with less memory becomes increasingly important . 3 .The paper is easy to read . Cons : 1.1 . It is not clear to me why SWA works well with a relatively high learning rate and can tolerate additional quantization noise during training , while SGD tends to underperform when the learning rate is low . 2.2 .In the experiments , the authors only compare the performance of SWA and SGD with VGG16 and VGG-16 with both VGG 16 and Pre-16 . It would be better if the authors can also compare the results with the state-of-the-art . 3.3 .It is better to show the performance difference between SWA with and without quantization in the experiments .
1_1902.04911 = This paper proposes a method for knowledge-grounded dialogue generation . The authors propose to use knowledge selection as a prior distribution over the posterior distribution over knowledge , which is inferred from both the utterance and the response . The model is trained to minimize the KL divergence between the prior and posterior distributions . Then , the model samples knowledge from the prior distribution and incorporates the sampled knowledge into response generation . Experiments are conducted on the Persona-chat dataset and Wizardof-Wikipedia dataset to demonstrate the effectiveness of the proposed method . The paper is well-written and easy to follow . The proposed method is novel and interesting . However , I have the following concerns . 1 .The proposed method seems to be very similar to [ Dinan et al. , 2018 ] , which also uses knowledge selection to guide the knowledge selection . The only difference is that the authors use the knowledge distribution instead of the semantic similarity between the utterances and the responses . 2 .It is not clear to me why the authors chose to use the proposed knowledge selection method instead of using the ground-truth knowledge . 3 .The authors should compare the performance of their method with the baselines in Table 1 and Table 2 . It would be better if the authors can provide more details about the baseline methods . 4 .In Table 2 , the authors should provide more information about the number of parameters of the model . For example , how many parameters are used in the model ? 5 .In the experiments , it would be more convincing if the author can show the results on more datasets .
1_2005.06628 = This paper proposes a pruning-based architecture search method for reducing the number of parameters in the Transformer-based BERT model . The paper is well-written and easy to follow . The experiments are well-conducted and the results are convincing . However , I have the following concerns : 1 . The authors claim that the top layers of BERT are more important than the bottom layers , but they do not provide any evidence to support this claim . For example , in Table 1 , it is shown that the fully connected component applied to each token separately plays a much more significant role in top layers as compared to bottom layers . 2 .In Table 2 , the performance of the pruned model is not significantly better than the original model . It would be better if the authors can provide some explanation why this is the case . 3 .It is not clear to me how the pruning based architecture search can be applied to BERT . For instance , the authors should provide more details on how to prune the size of the attention heads .
1_2102.07983 = This paper introduces a new dataset called FEWS ( Few-shot Examples of Word Senses ) , which is a dataset for evaluating Word Sense Disambiguation ( WSD ) models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary definitions of 71,000 senses , which are used as training data for WSD models . The authors also conduct experiments to compare the performance of knowledge-based approaches and a recent neural biencoder model on the new dataset . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset to evaluate the WSD model in the few-and-zero-shot setting , but it is not clear to me how significant this contribution is . 2 .The experimental results are not convincing . For example , in Table 1 , the results of the biencoders are not compared to human annotators , which makes it hard to judge the significance of the results . 3 .The transfer learning experiments are not well-conducted . It would be better to conduct the transfer learning experiment to see if the proposed dataset can be used as additional training data to improve the performance on other WSD tasks .
1_1812.02425 = This paper proposes a method for ensembling multiple neural networks with no additional testing cost . The proposed method is based on the teacher-student learning paradigm , where the teacher network is pre-trained and the student network is trained using adversarial learning . The authors show that the proposed method consistently improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a variety of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of this paper is limited . The idea of using the soft labels as supervision is not new . It has been proposed in [ 1 ] , [ 2 ] and [ 3 ] . 2 .The proposed method can achieve the goal of ensembled multiple neural network with no extra testing cost , but the performance gain is not significant . 3 .The experiments are not convincing . For example , in Table 1 , the performance of Shake-Shake-MEAL is not better than the baseline method , while the performance improvement of MEAL is small . 4 .In Table 2 , it is not clear why the performance is better than Shake-shake-MeAL . 5 .The authors should compare the performance with other ensembles , e.g. , [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,44,47,48,50,49,51,52,54,55,56,58,59,60,61,63,64,70,72,73,72 ,73,74,75,76,77,78,80,81,82,83,84,88,89,90,91,93,94,93 ,94,98,99,99 ,100,100,103,104 ,104,104,105,106,107,108 ,108,108,111,113,114,115,116,117,118,119,120,121,122,122 ,128,128,129,131,128 ,128 ,129,129 ,131,132,131 ,131 ,132,133,136,137,136 ,128-129 ,136-136 ,137-137 ,138-138 ,139-139 ,144-140 . [ 1 , 1 ] https : //arxiv.org/abs/1806.059062 [ 2 , 2 ] http : //proceedings.mlr.press/v48/papers/1605.06570 [ 3 , 3 ] https: //arXiv:1705.09737.03780 [ 4 ] https://arxives.cc/abs-1605/1606.03880 [ 5 ] https //ar xiv.cc /abs/1607.03902 [ 6 ] httpsÂ Arxiv:1611.06470 [ 7 ] https { Arxives : https :Â ArXiv.pdf ] https = = = > https : > ArXives.org [ 8 ] [ 9 ]
1_1901.06829 = This paper proposes an end-to-end reinforcement learning framework for grounding natural language descriptions in videos . The paper is well-written and easy to follow . The proposed method is novel and well-motivated . However , I have some concerns about the novelty of the proposed method . 1 .The proposed method seems to be a direct application of existing methods for video grounding . The novelty of this paper is limited . The method is a combination of several existing methods . For example , the method of Gao et al . ( 2017 ) , which is used in this paper , is not compared with . 2 .The method of Hendricks et al. , 2017 is not applicable to general cases . 3 .The results are not convincing . The performance of the method is not better than the state-of-the-art methods . 4 .The paper is not well-organized . There are many typos and grammatical errors in the paper . I suggest the authors to proofread the paper carefully .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a SAT solver to solve the projective plane of order 10 problem , which is the most challenging subcase of Lam 's problem . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates are generated with the assistance of the symbolic computation library Traces . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that the certificates are more verifiable , but they do not provide a formal proof of the nonexistence of projective planes of order ten because they rely on results that currently have no formal computerverifiable proofs . 2 .It is not clear to me how the certificates can be verified by a third party . 3 .In the proof of Theorem 3.1 , the authors use a special-purpose solver , but I do not know how to verify the correctness of the certificates . 4 .The proof of theorem 3.2 is not correct . The proof is based on the fact that the error correcting code must contain words that are called `` primitive `` or `` weight `` 19 words , which are the first two cases that are first ruled out by the search of ( MacWilliams , Slane , and Thompson 1973 ) and were recently settled via SAT-based nonexistence certificates ( Bright et al. , 2020a ,b ) . 5 .In Theorem 4.2 , the proof is not complete . It is not possible to prove the existence of the plane .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and shows that they are at most as powerful as the Weisfeiler-Lehman ( WL ) graph isomorphism test in distinguishing graph structures . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of feature vectors of a given node ‘ s neighbors as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNNS can be thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . 2 .The paper also shows that the most powerful GNN by our theory , ie., Graph Isomorphism Network ( GIN ) , also empirically has high representation power as it fits the data perfectly . 3 .The experimental results show that the proposed GIN can not distinguish graph structures that can not be distinguished by popular GNN variants , GCN ( Kipf & Welling , 2017 ) and GraphSA ( Hamilton et al. , 2017a ) and GIN ( Velickovic et .al. , 2018 ) . 4 .In the experiments , the authors compare the performance of GIN with various aggregation functions with various graph aggregation functions . It would be better if the authors can compare the results with the results of GNN with different aggregation functions and graph-level pooling schemes .
1_1907.07355 = This paper investigates the reason why BERT performs so well on the argument reasoning mining task . The authors show that BERT is able to exploit the presence of statistical cues in the warrants in the ARCT task . They also show that the major problem can be eliminated in ARCT by adding a copy of each data point with the claim negated and the label inverted . The paper is well written and easy to follow . However , I have the following concerns : 1 . In the experiments , the authors only compare BERT with the unsupervised baseline . It would be better if the authors can compare with the state-of-the-art . 2 .In Table 1 , the performance of BERT on the adversarial dataset is much lower than the human performance . It will be better to compare the performance with the human baseline . 3 .The authors should provide more details about the dataset used in the experiments . For example , what is the size of the dataset ? What is the number of training examples ? How many training examples were used in each experiment ? 4 .In the experiment , it is better to show the performance on the full dataset .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) to reduce the performance gap between low-precision and full precision training in deep learning by quantizing all numbers except the gradient accumulator to 8 bits without changing the network structure . The proposed method is simple to implement and has little computational overhead . The experimental results show that the proposed method can match the full precision SGD baseline on Cifar-10 and CIFAR-100 with both VGG16 and VGG-1616 . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The motivation of the paper is not very clear . The authors claim that SWA has been shown to lead to wider optima and better generalization performance in the context of low precision training , but it is not clear to me why SWA can boost the performance of low precision training and that performance improvement is more significant than in the case of SWA applied to full precision . 2 .In the experiments , the authors only compare with the state-of-the-art results of Wang et al . ( 2018 ) . It would be better if the authors can compare with more recent results , e.g. , [ 1 ] , [ 2 ] and [ 3 ] . 3 .The authors should compare with other quantization methods such as [ 4 ] . 4 .The experiments are not convincing enough . For example , in Table 1 , it is better to compare with [ 5 ] . 5 .In Table 2 , the comparison with [ 6 ] is missing . 6 .In Section 4.2 , it would be more convincing if the author can show the results of [ 7 ] . 7 .In Figure 3 , it seems that the authors did not compare the results with [ 8 ] . It will be better to show the result of [ 9 ] .
1_1902.04911 = This paper proposes a method for dialogue generation based on knowledge-based models . The key idea is to separate the prior distribution over knowledge and the posterior distribution over responses . The prior distribution is inferred from both utterance and response , while the posterior is inferred based on the actual knowledge used in the response . The authors propose to minimize the KL divergence between the prior and posterior distributions during the training process . Then , during the inference process , the model samples knowledge without any posterior information ( i.e. , without any prior information ) and incorporates the sampled knowledge into response generation . Experiments are conducted on the Persona-chat dataset and Wizard of Wikipedia dataset . The paper is well-written and easy to follow . The proposed method is novel and interesting . However , I have the following concerns . 1 .The authors claim that the proposed method can generate more informative responses by utilizing appropriate knowledge . But , it is not clear to me how the model can generate informative responses . For example , if the model is trained by selecting wrong knowledge ( e.g. , K2 in R2 ) or knowledge irrelevant to the true response , it can be seen that they are completely useless since they can not provide any helpful information . 2 .The proposed model is not compared with other related works . For instance , the authors should compare with the commonsense model proposed in [ Zhou et al. , 2018 ] , which also uses knowledge as a prior distribution to guide the knowledge selection . 3 .In Table 1 , the performance of the proposed approach is not better than the baselines . It would be better if the authors can compare with other baselines such as [ Dinan et al .2018 ] and [ Ghazvininejad et al 2018 ] . 4 .The paper is not well-organized . It is hard to understand the motivation of the paper . The motivation of this paper is to learn a model that is able to generate informative and appropriate responses by incorporating knowledge in response generation , but the paper does not explain the motivation behind the proposed model . 5 .The experiments are not convincing . In Table 2 , the results are not very convincing . It seems that the model does not work well when the response is generated by using knowledge based on K1 and K3 . In addition , there are many typos in the paper , such as “ R1 utilizes no knowledge and thus ends up in a less informative response ” .
1_2005.06628 = This paper proposes a pruning-based architecture search method to improve the performance of the BERT model . The paper is well written and easy to follow . However , I have the following concerns : 1 . It is not clear to me what is the advantage of the proposed pruning method compared to other pruning methods . For example , the pruning of the attention heads in ALBERT ( Lan et al. , 2019 ) . 2 .It is unclear to me why the proposed method is better than the other two pruning techniques . 3 .The experiments are not convincing . In Table 1 and Table 2 , the results of the pruned BERT are not better than other pruned models . The authors should compare the results with other prune methods . 4 .In Table 3 , the authors should show the results for different pruning strategies . 5 .The authors should provide more details about the hyper-parameters used for pruning . 6 .The paper is not well organized . For instance , in Section 3.1 , it is hard to follow the algorithm . 7 .In Section 4.2 , the author should provide the details of the architecture search algorithm .
1_2102.07983 = This paper introduces a new dataset called FEWS ( Few-shot Examples of Word Senses ) , which is a dataset for evaluating Word Sense Disambiguation ( WSD ) models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary definitions of 71,000 senses , which are used as training data for WSD models . The authors also conduct experiments to compare the performance of knowledge-based approaches and a recent neural biencoder model on the new dataset . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset to evaluate the WSD model in the few-and-zero-shot setting , but it is not clear to me how significant this contribution is . 2 .The experimental results are not convincing . For example , in Table 1 , the results of the biencoders are not compared to human annotators , which makes it hard to judge the significance of the results . 3 .The transfer learning experiments are not well-conducted . It would be better to conduct the transfer learning experiment to see if the proposed dataset can be used as additional training data to improve the performance on other WSD tasks .
1_1812.02425 = This paper proposes a method for ensembling multiple neural networks with no additional testing cost . The proposed method is based on the teacher-student learning paradigm , where the teacher network is pre-trained and the student network is trained using adversarial learning . The authors show that the proposed method consistently improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a variety of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of this paper is limited . The idea of using the soft labels as supervision is not new . It has been proposed in [ 1 ] , [ 2 ] and [ 3 ] . 2 .The proposed method can achieve the goal of ensembled multiple neural network with no extra testing cost , but the performance gain is not significant . 3 .The experiments are not convincing . For example , in Table 1 , the performance of Shake-Shake-MEAL is not better than the baseline method , while the performance improvement of MEAL is small . 4 .In Table 2 , it is not clear why the performance is better than Shake-shake-MeAL . 5 .The authors should compare the performance with other ensembles , e.g. , [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,44,47,48,50,49,51,52,54,55,56,58,59,60,61,63,64,70,72,73,72 ,73,74,75,76,77,78,80,81,82,83,84,88,89,90,91,93,94,93 ,94,98,99,99 ,100,100,103,104 ,104,104,105,106,107,108 ,108,108,111,113,114,115,116,117,118,119,120,121,122,122 ,128,128,129,131,128 ,128 ,129,129 ,131,132,131 ,131 ,132,133,136,137,136 ,128-129 ,136-136 ,137-137 ,138-138 ,139-139 ,144-140 . [ 1 , 1 ] https : //arxiv.org/abs/1806.059062 [ 2 , 2 ] http : //proceedings.mlr.press/v48/papers/1605.06570 [ 3 , 3 ] https: //arXiv:1705.09737.03780 [ 4 ] https://arxives.cc/abs-1605/1606.03880 [ 5 ] https //ar xiv.cc /abs/1607.03902 [ 6 ] httpsÂ Arxiv:1611.06470 [ 7 ] https { Arxives : https :Â ArXiv.pdf ] https = = = > https : > ArXives.org [ 8 ] [ 9 ]
1_1901.06829 = This paper proposes an end-to-end reinforcement learning framework for grounding natural language descriptions in videos . Specifically , the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . The agent makes an observation that details the currently selected video clip , and takes an action according to its policy , and the environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . The paper is well-written and easy to follow . The proposed method is novel and interesting . However , I have the following concerns : 1 . The novelty of this paper is limited . It is not clear how the proposed method can be applied to other video grounding tasks such as video retrieval and text-oriented video highlight detection . 2 .The experiments are not convincing . The performance of the proposed model is not better than the state-of-the-art baselines . 3 .The proposed method does not outperform the baselines in Table 1 and Table 2 . 4 .The paper is not well-organized . For example , there are many typos and grammatical errors .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a SAT solver to solve the projective plane of order 10 problem , which is the most challenging subcase of Lam 's problem . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates are generated with the assistance of the symbolic computation library Traces . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that the certificates are more verifiable , but they do not provide a formal proof of the nonexistence of projective planes of order ten because they rely on results that currently have no formal computerverifiable proofs . 2 .It is not clear to me how the certificates can be verified by a third party . 3 .In the proof of Theorem 3.1 , the authors use a special-purpose solver , but I do not know how to verify the correctness of the certificates . 4 .The proof of theorem 3.2 is not correct . The proof is based on the fact that the error correcting code must contain words that are called `` primitive `` or `` weight `` 19 words , which are the first two cases that are first ruled out by the search of ( MacWilliams , Slane , and Thompson 1973 ) and were recently settled via SAT-based nonexistence certificates ( Bright et al. , 2020a ,b ) . 5 .In Theorem 4.2 , the proof is not complete . It is not possible to prove the existence of the plane .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and shows that they are at most as powerful as the Weisfeiler-Lehman ( WL ) graph isomorphism test in distinguishing graph structures . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of feature vectors of a given node ‘ s neighbors as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNNS can be thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . 2 .The paper also shows that the most powerful GNN by our theory , ie., Graph Isomorphism Network ( GIN ) , also empirically has high representation power as it fits the data perfectly . 3 .The experimental results show that the proposed GIN can not distinguish graph structures that can not be distinguished by popular GNN variants , GCN ( Kipf & Welling , 2017 ) and GraphSA ( Hamilton et al. , 2017a ) and GIN ( Velickovic et .al. , 2018 ) . 4 .In the experiments , the authors compare the performance of GIN with various aggregation functions with various graph aggregation functions . It would be better if the authors can compare the results with the results of GNN with different aggregation functions and graph-level pooling schemes .
1_1907.07355 = This paper presents an analysis of the performance of BERT on the argument reasoning mining task ( ARCT ) . The authors show that BERT is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated in ARCT by adding a copy of each data point with the claim negated and the label inverted . This means that the distribution of statistical cues in the warrants will be mirrored over both labels , eliminating the signal . On this adversarial dataset all models perform randomly , with BERT achieving a maximum test set accuracy of 53 % . This paper provides a more robust evaluation of argument comprehension and should be adopted as the standard in future work on this dataset . The paper is well written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me how to interpret the results in Table 1 . In Table 1 , BERT achieves 77 % accuracy with its best run , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question : what has BERT learned about argument comprehension ? To investigate BERT ’ s decision making , the authors looked at data points it finds easy to classify over multiple runs . 2 .In Table 2 , the results are not consistent with the results of Habernal et al . ( 2018b ) , which shows BERT exploits “ cue words ” in the ARCT SemEval submissions , and consistent with their results . It would be better if the authors can provide more explanation about this result .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The authors prove that for quadratic and strongly convex objectives , SWA converges to a noise ball that is asymptotically smaller than that of low precision SGD . They also show that their method can significantly reduce the performance gap between low precision and full precision training . This paper is well-written and easy to follow . The idea of using SWA to reduce the quantization noise during training is interesting and novel . However , I have the following concerns : 1 . The paper is not well-motivated . The motivation of SWA is not clear . In the introduction , the authors claim that SWA takes an average of SGD iterates with a modified learning rate schedule and has been shown to lead to wider optima ( Izmailov et al. , 2018a ) . But in the experiments , it seems that the authors only compare SWA with SGD with a low learning rate . 2 .The experiments are not convincing . In Table 1 , the performance of 8-bit SWA does not match the full precision baseline . 3 .In Table 2 , the accuracy of the proposed method is not better than the baseline . 4 .The authors should provide more details about the implementation details . For example , how to choose the number of bits in the quantized grid ? 5 .In the experiment , it is better to compare the performance with the state-of-the-art methods .
1_1902.04911 = This paper proposes a method for knowledge-grounded dialogue generation , where the prior distribution over knowledge is inferred from both utterance and response , and the posterior distribution over the actual knowledge used in the response is considered . The paper is well-written and easy to follow . The idea of using knowledge selection to guide knowledge selection in dialogue generation is interesting . However , I have the following concerns : 1 . The proposed method is based on the KL-divergence between the prior and posterior distributions , which is not a new idea . The authors should compare the proposed method with other related methods . 2 .The authors should also compare the performance of their method with the baselines . 3 .The proposed method seems to be a simple combination of existing methods . It would be better if the authors can provide more details about the motivation of this method . For example , what is the motivation for using the knowledge selection mechanism ? 4 .In the experiments , the authors only compare with two baselines , which are not the state-of-the-art methods . I would like to see the results of other baselines as well .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea is to prune the architecture of the model by minimizing both the pre-training loss and the number of model parameters . Experiments are conducted on GLUE , SQuAD v1.1 and SQuad v2.0 . The paper is well written and easy to follow . However , I have the following concerns : 1 . It is not clear to me how the pruning is done in this paper . The pruning method is based on the idea of pruning the attention heads , which is not a new idea . 2 .The experiments are limited to GLUE . It would be better if the authors can conduct experiments on other NLP tasks . 3 .The authors should compare with other pruning methods , such as pruning based pruning of attention heads in ALBERT ( Lan et al. , 2019 ) . 4 .The results of the experiments are not convincing . For example , in Table 1 , the performance of the pruned BERT is not better than the original BERT .
1_2102.07983 = This paper introduces a new dataset called FEWS ( Few-shot Examples of Word Senses ) , which is a dataset for evaluating Word Sense Disambiguation ( WSD ) models in few-shot and zero-shot settings . The dataset is built on top of the Wiktionary dataset , which contains 71,000 senses . The authors evaluate the performance of different knowledge-based approaches and a recent neural biencoder model for WSD on the new dataset . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset to evaluate WSD models in the low-shot setting , but it is not clear to me how significant this contribution is . 2 .The evaluation of the proposed dataset is only conducted on a few senses . It would be interesting to see the performance on other senses . 3 .It would be good to see how the proposed datasets can be used for transfer learning . 4 .The paper is not well-organized . There are many typos and grammatical errors in the paper .
1_1812.02425 = This paper proposes a method for ensembling multiple neural networks with no additional testing cost . The proposed method is based on the teacher-student learning paradigm , where the teacher network is pre-trained and the student network is trained using adversarial learning . The authors show that the proposed method consistently improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a variety of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of this paper is limited . The idea of using the soft labels as supervision is not new . It has been proposed in [ 1 ] , [ 2 ] and [ 3 ] . 2 .The proposed method can achieve the goal of ensembled multiple neural network with no extra testing cost , but the performance gain is not significant . 3 .The experiments are not convincing . For example , in Table 1 , the performance of Shake-Shake-MEAL is not better than the baseline method , while the performance improvement of MEAL is small . 4 .In Table 2 , it is not clear why the performance is better than Shake-shake-MeAL . 5 .The authors should compare the performance with other ensembles , e.g. , [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,44,47,48,50,49,51,52,54,55,56,58,59,60,61,63,64,70,72,73,72 ,73,74,75,76,77,78,80,81,82,83,84,88,89,90,91,93,94,93 ,94,98,99,99 ,100,100,103,104 ,104,104,105,106,107,108 ,108,108,111,113,114,115,116,117,118,119,120,121,122,122 ,128,128,129,131,128 ,128 ,129,129 ,131,132,131 ,131 ,132,133,136,137,136 ,128-129 ,136-136 ,137-137 ,138-138 ,139-139 ,144-140 . [ 1 , 1 ] https : //arxiv.org/abs/1806.059062 [ 2 , 2 ] http : //proceedings.mlr.press/v48/papers/1605.06570 [ 3 , 3 ] https: //arXiv:1705.09737.03780 [ 4 ] https://arxives.cc/abs-1605/1606.03880 [ 5 ] https //ar xiv.cc /abs/1607.03902 [ 6 ] httpsÂ Arxiv:1611.06470 [ 7 ] https { Arxives : https :Â ArXiv.pdf ] https = = = > https : > ArXives.org [ 8 ] [ 9 ]
1_1901.06829 = This paper proposes an end-to-end reinforcement learning framework for grounding natural language descriptions in videos . The paper is well-written and easy to follow . The proposed method is novel and well-motivated . However , I have some concerns about the novelty of the proposed method . 1 .The proposed method seems to be a direct application of IoU to video grounding . It is not clear to me why the authors chose to use IoU as the grounding method . It seems to me that IoU can be applied to other tasks such as video retrieval or video highlight detection . 2 .In the experiments , the authors only show the results on one video . It would be better to show results on more videos . 3 .The authors should compare their method with the state-of-the-art methods such as [ 1 ] and [ 2 ] . 4 .It would be more convincing if the authors can show the performance of their method on more than one video and compare it with other methods . 5 .The paper is not well-organized . For example , there are many typos and grammatical errors .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a SAT solver to solve the projective plane of order 10 problem , which is the most challenging subcase of Lam 's problem . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete , and verify the certificates with a proof verifier . The certificates are generated with the assistance of the symbolic computation library Traces . The paper is well written and easy to follow . However , I have the following concerns : 1 . It is not clear to me how to verify the correctness of the certificates generated by the Traces library . 2 .The authors claim that the certificates are more verifiable , but they do not provide a formal proof of the nonexistence of the plane . 3 .In the proof of Theorem 3.1 , the authors use a special-purpose solver ( SAT ) instead of a special purpose solver . It would be better if the authors can provide more details about the differences between the special purpose and SAT solvers in the paper . 4 .The proof of theorem 3.2 is not very clear . It seems that the proof is based on the fact that the error-correcting code associated with a hypothetical projective planes of order ten must contain words that are weight 15 or weight 16 , or ‘ primitive weight 19 ’ weight words . The authors should provide more explanation about this . 5 .The paper is not well organized . For example , in the introduction , it is not easy to understand how the certificates can be verified .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and shows that they are at most as powerful as the Weisfeiler-Lehman ( WL ) graph isomorphism test in distinguishing graph structures . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of feature vectors of a given node ‘ s neighbors as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNNS can be thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . 2 .The paper also shows that the most powerful GNN by our theory , ie., Graph Isomorphism Network ( GIN ) , also empirically has high representation power as it fits the data perfectly . 3 .The experimental results show that the proposed GIN can not distinguish graph structures that can not be distinguished by popular GNN variants , GCN ( Kipf & Welling , 2017 ) and GraphSA ( Hamilton et al. , 2017a ) and GIN ( Velickovic et .al. , 2018 ) . 4 .In the experiments , the authors compare the performance of GIN with various aggregation functions with various graph aggregation functions . It would be better if the authors can compare the results with the results of GNN with different aggregation functions and graph-level pooling schemes .
1_1907.07355 = This paper presents an analysis of the performance of BERT on the argument reasoning mining task ( ARCT ) . The authors show that BERT is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated in ARCT by adding a copy of each data point with the claim negated and the label inverted . This means that the distribution of statistical cues in the warrants will be mirrored over both labels , eliminating the signal . On this adversarial dataset all models perform randomly , with BERT achieving a maximum test set accuracy of 53 % . This paper provides a more robust evaluation of argument comprehension and should be adopted as the standard in future work on this dataset . The paper is well written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me how to interpret the results in Table 1 . In Table 1 , BERT achieves 77 % accuracy with its best run , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question : what has BERT learned about argument comprehension ? To investigate BERT ’ s decision making , the authors looked at data points it finds easy to classify over multiple runs . 2 .In Table 2 , the results are not consistent with the results of Habernal et al . ( 2018b ) , which shows BERT exploits “ cue words ” in the ARCT SemEval submissions , and consistent with their results . It would be better if the authors can provide more explanation about this result .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The authors prove that for quadratic and strongly convex objectives , SWA converges to a noise ball that is asymptotically smaller than that of low precision SGD . They also show that their method can significantly reduce the performance gap between low precision and full precision training . This paper is well-written and easy to follow . The idea of using SWA to reduce the quantization noise during training is interesting and novel . However , I have the following concerns : 1 . The paper is not well-motivated . The motivation of SWA is not clear . In the introduction , the authors claim that SWA takes an average of SGD iterates with a modified learning rate schedule and has been shown to lead to wider optima ( Izmailov et al. , 2018a ) . But in the experiments , it seems that the authors only compare SWA with SGD with a low learning rate . 2 .The experiments are not convincing . In Table 1 , the performance of 8-bit SWA does not match the full precision baseline . 3 .In Table 2 , the accuracy of the proposed method is not better than the baseline . 4 .The authors should provide more details about the implementation details . For example , how to choose the number of bits in the quantized grid ? 5 .In the experiment , it is better to compare the performance with the state-of-the-art methods .
1_1902.04911 = This paper proposes to use knowledge selection to guide dialogue generation . The key idea is to separate the prior distribution over knowledge from the posterior distribution over the response . The model is trained to minimize the KL divergence between the prior and posterior distributions . Then , the model samples the knowledge distribution based on the posterior information and incorporates the sampled knowledge into the response generation . Experiments are conducted on the Persona-chat dataset and Wizard of Wikipedia dataset . The paper is well-written and easy to follow . The idea of using knowledge selection for dialogue generation is interesting . However , I have the following concerns : 1 . The proposed model is not compared with any other knowledge-grounded dialogue generation models . For example , [ Dinan et al . [ 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 2 .The proposed model does not outperform the existing models by incorporating more knowledge . It is better to compare the proposed model with the baselines . 3 .The experiments are only conducted on one dataset . It would be better to conduct experiments on more datasets . 4 .The paper is not well-motivated . The motivation of this work is not clear . The authors claim that knowledge selection mechanism is effective for appropriate response generation , but they do not provide any empirical evidence to support this claim . 5 .The authors should provide more details about the model . For instance , what is the architecture of the model ? How many parameters are used for the model and how many parameters for the knowledge selection ? 6 .In the experiments , the authors only compare with the baseline models . It will be better if the authors can show the performance of the proposed method on other models as well .
1_2005.06628 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2102.07983 = This paper introduces a new dataset called FEWS ( Few-shot Examples of Word Senses ) , which is a dataset for evaluating Word Sense Disambiguation ( WSD ) models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary definitions of 71,000 senses , which are used as training data for WSD models . The authors also conduct experiments to compare the performance of knowledge-based approaches and a recent neural biencoder model on the new dataset . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset to evaluate the WSD model in the few-and-zero-shot setting , but it is not clear to me how significant this contribution is . 2 .The experimental results are not convincing . For example , in Table 1 , the results of the biencoders are not compared to human annotators , which makes it hard to judge the significance of the results . 3 .The transfer learning experiments are not well-conducted . It would be better to conduct the transfer learning experiment to see if the proposed dataset can be used as additional training data to improve the performance on other WSD tasks .
1_1812.02425 = This paper proposes a method for ensembling multiple neural networks with no additional testing cost . The proposed method is based on the teacher-student learning paradigm , where the teacher network is pre-trained and the student network is trained using adversarial learning . The authors show that the proposed method consistently improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a variety of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of this paper is limited . The idea of using the soft labels as supervision is not new . It has been proposed in [ 1 ] , [ 2 ] and [ 3 ] . 2 .The proposed method can achieve the goal of ensembled multiple neural network with no extra testing cost , but the performance gain is not significant . 3 .The experiments are not convincing . For example , in Table 1 , the performance of Shake-Shake-MEAL is not better than the baseline method , while the performance improvement of MEAL is small . 4 .In Table 2 , it is not clear why the performance is better than Shake-shake-MeAL . 5 .The authors should compare the performance with other ensembles , e.g. , [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,44,47,48,50,49,51,52,54,55,56,58,59,60,61,63,64,70,72,73,72 ,73,74,75,76,77,78,80,81,82,83,84,88,89,90,91,93,94,93 ,94,98,99,99 ,100,100,103,104 ,104,104,105,106,107,108 ,108,108,111,113,114,115,116,117,118,119,120,121,122,122 ,128,128,129,131,128 ,128 ,129,129 ,131,132,131 ,131 ,132,133,136,137,136 ,128-129 ,136-136 ,137-137 ,138-138 ,139-139 ,144-140 . [ 1 , 1 ] https : //arxiv.org/abs/1806.059062 [ 2 , 2 ] http : //proceedings.mlr.press/v48/papers/1605.06570 [ 3 , 3 ] https: //arXiv:1705.09737.03780 [ 4 ] https://arxives.cc/abs-1605/1606.03880 [ 5 ] https //ar xiv.cc /abs/1607.03902 [ 6 ] httpsÂ Arxiv:1611.06470 [ 7 ] https { Arxives : https :Â ArXiv.pdf ] https = = = > https : > ArXives.org [ 8 ] [ 9 ]
1_1901.06829 = This paper proposes an end-to-end reinforcement learning framework for grounding natural language descriptions in videos . Specifically , the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . The agent makes an observation that details the currently selected video clip , and takes an action according to its policy , and the environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . The paper is well-written and easy to follow . The proposed method is novel and interesting . However , I have the following concerns : 1 . The novelty of this paper is limited . There are many existing methods for action localization and video grounding tasks , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) .The proposed method can be applied only when all candidates are exhaustively enumerated , as they need to rank all possible clip-sentence pairs to be successful in generating the final grounding result . Moreover , the ranking strategy is specially designed for pre-segmented videos and is not applicable to general cases . To effectively detect the temporal boundaries of a given description from a given video , especially from a long , untrimmed video , it is important to make judgments only by several glimpses . 2 .The paper is not well-motivated . In the introduction , the authors claim that it requires to map natural language description to a close set of labels before applying these methods , which inevitably causes information loss . Although there is a growing interest in grounded natural language in videos , i.e. , if the sentence “The parent watches the boy play drums ” is mapped to an action label “play drums , ” , action detectors can hardly tell the start point of the presence of the parent . 3 .The experiments are not convincing . In Table 1 , the performance of the proposed method seems to be worse than the state-of-the-art methods . The authors should compare the performance with other methods .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a SAT solver to solve the projective plane of order 10 problem , which is the most challenging subcase of Lam 's problem . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates are generated with the assistance of the symbolic computation library Traces . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that the certificates are more verifiable , but they do not provide a formal proof of the nonexistence of projective planes of order ten because they rely on results that currently have no formal computerverifiable proofs . 2 .It is not clear to me how the certificates can be verified by a third party . 3 .In the proof of Theorem 3.1 , the authors use a special-purpose solver , but I do not know how to verify the correctness of the certificates . 4 .The proof of theorem 3.2 is not correct . The proof is based on the fact that the error correcting code must contain words that are called `` primitive `` or `` weight `` 19 words , which are the first two cases that are first ruled out by the search of ( MacWilliams , Slane , and Thompson 1973 ) and were recently settled via SAT-based nonexistence certificates ( Bright et al. , 2020a ,b ) . 5 .In Theorem 4.2 , the proof is not complete . It is not possible to prove the existence of the plane .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and shows that they are at most as powerful as the Weisfeiler-Lehman ( WL ) graph isomorphism test in distinguishing graph structures . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of feature vectors of a given node ‘ s neighbors as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNNS can be thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . 2 .The paper also shows that the most powerful GNN by our theory , ie., Graph Isomorphism Network ( GIN ) , also empirically has high representation power as it fits the data perfectly . 3 .The experimental results show that the proposed GIN can not distinguish graph structures that can not be distinguished by popular GNN variants , GCN ( Kipf & Welling , 2017 ) and GraphSA ( Hamilton et al. , 2017a ) and GIN ( Velickovic et .al. , 2018 ) . 4 .In the experiments , the authors compare the performance of GIN with various aggregation functions with various graph aggregation functions . It would be better if the authors can compare the results with the results of GNN with different aggregation functions and graph-level pooling schemes .
1_1907.07355 = This paper investigates the performance of BERT on the argument reasoning mining task . The authors show that BERT is able to exploit the presence of cue words in the warrant , especially “ not ” . They show that this can be eliminated in ARCT by adding a copy of each data point with the claim negated and the label inverted . This means that the distribution of statistical cues in the warrants will be mirrored over both labels , eliminating the signal . The adversarial dataset provides a more robust evaluation of argument comprehension and should be adopted as the standard in future work on this dataset . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that “ BERT achieves 77 % test set accuracy with its best run ( Table 1 ) , only three points below the average ( untrained ) human baseline . ” However , it is not clear to me why this is the case . In Table 1 , the human baseline is only 3 points below BERT ’ s performance . 2 .In Table 2 , the authors claim “ The ARCT SemEval shared task ( Habernal et al. , 2018b ) verified the challenging nature of this task ” , but it is unclear to me how this is verified . 3 .In Figure 3 , it seems that the authors did not compare BERT with the state-of-the-art methods on the ARCT dataset . 4 .In Section 4.2 , it would be better to show the results of the BERT baseline on the same dataset . 5 .It would be more convincing if the authors can provide more details about the dataset .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The authors prove that for quadratic and strongly convex objectives , SWA converges to a noise ball that is asymptotically smaller than that of low precision SGD . They also show that their method can significantly reduce the performance gap between low precision and full precision training . This paper is well-written and easy to follow . The idea of using SWA to reduce the quantization noise during training is interesting and novel . However , I have the following concerns : 1 . The paper is not well-motivated . The motivation of SWA is not clear . In the introduction , the authors claim that SWA takes an average of SGD iterates with a modified learning rate schedule and has been shown to lead to wider optima ( Izmailov et al. , 2018a ) . But in the experiments , it seems that the authors only compare SWA with SGD with a low learning rate . 2 .The experiments are not convincing . In Table 1 , the performance of 8-bit SWA does not match the full precision baseline . 3 .In Table 2 , the accuracy of the proposed method is not better than the baseline . 4 .The authors should provide more details about the implementation details . For example , how to choose the number of bits in the quantized grid ? 5 .In the experiment , it is better to compare the performance with the state-of-the-art methods .
1_1902.04911 = This paper proposes to use knowledge selection to guide dialogue generation . The key idea is to separate the prior distribution over knowledge from the posterior distribution over the response . The model is trained to minimize the KL divergence between the prior and posterior distributions . Then , during the inference process , the model samples knowledge merely based on the prior prior distribution and incorporates the sampled knowledge into response generation . Experiments show that the proposed model outperforms the existing knowledge-based dialogue generation models . The paper is well-written and easy to follow . The idea of using knowledge selection for dialogue generation is interesting and novel . However , I have some concerns about the experiments . 1 .The proposed model is only evaluated on the Persona-chat dataset . It would be interesting to see the results on Wizard of Wikipedia dataset as well . 2 .It would be better if the authors can show the performance of the model on other dialogue generation datasets . 3 .In Table 1 , it would be good to show the results of the knowledge selection in the response generation task . 4 .It is not clear to me why the model is better than the baselines in Table 2 .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea is to prune the architecture of the model by minimizing both the pre-training loss and the number of model parameters . The paper is well written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me what is the advantage of the proposed pruning method compared to other pruning methods . For example , the pruning of the attention heads in ALBERT ( Lan et al. , 2019 ) . 2 .The authors claim that the proposed method can reduce the model parameters while increasing the latency , but the experiments do not show the effect of pruning on the model performance . 3 .The experiments are only conducted on the MNLI task . It would be better if the authors can conduct experiments on other NLP tasks , e.g. , WMT , NLI , etc . .
1_2102.07983 = This paper introduces a new dataset called FEWS ( Few-shot Examples of Word Senses ) , which is a dataset for evaluating Word Sense Disambiguation ( WSD ) models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary definitions of 71,000 senses , which are used as training data for WSD models . The authors also conduct experiments to compare the performance of knowledge-based approaches and a recent neural biencoder model on the new dataset . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset to evaluate the WSD model in the few-and-zero-shot setting , but it is not clear to me how significant this contribution is . 2 .The experimental results are not convincing . For example , in Table 1 , the results of the biencoders are not compared to human annotators , which makes it hard to judge the significance of the results . 3 .The transfer learning experiments are not well-conducted . It would be better to conduct the transfer learning experiment to see if the proposed dataset can be used as additional training data to improve the performance on other WSD tasks .
1_1812.02425 = This paper proposes a method for ensembling multiple neural networks with no additional testing cost . The proposed method is based on the teacher-student learning paradigm , where the teacher network is pre-trained and the student network is trained using adversarial learning . The authors show that the proposed method consistently improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a variety of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of this paper is limited . The idea of using the soft labels as supervision is not new . It has been proposed in [ 1 ] , [ 2 ] and [ 3 ] . 2 .The proposed method can achieve the goal of ensembled multiple neural network with no extra testing cost , but the performance gain is not significant . 3 .The experiments are not convincing . For example , in Table 1 , the performance of Shake-Shake-MEAL is not better than the baseline method , while the performance improvement of MEAL is small . 4 .In Table 2 , it is not clear why the performance is better than Shake-shake-MeAL . 5 .The authors should compare the performance with other ensembles , e.g. , [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,44,47,48,50,49,51,52,54,55,56,58,59,60,61,63,64,70,72,73,72 ,73,74,75,76,77,78,80,81,82,83,84,88,89,90,91,93,94,93 ,94,98,99,99 ,100,100,103,104 ,104,104,105,106,107,108 ,108,108,111,113,114,115,116,117,118,119,120,121,122,122 ,128,128,129,131,128 ,128 ,129,129 ,131,132,131 ,131 ,132,133,136,137,136 ,128-129 ,136-136 ,137-137 ,138-138 ,139-139 ,144-140 . [ 1 , 1 ] https : //arxiv.org/abs/1806.059062 [ 2 , 2 ] http : //proceedings.mlr.press/v48/papers/1605.06570 [ 3 , 3 ] https: //arXiv:1705.09737.03780 [ 4 ] https://arxives.cc/abs-1605/1606.03880 [ 5 ] https //ar xiv.cc /abs/1607.03902 [ 6 ] httpsÂ Arxiv:1611.06470 [ 7 ] https { Arxives : https :Â ArXiv.pdf ] https = = = > https : > ArXives.org [ 8 ] [ 9 ]
1_1901.06829 = This paper proposes an end-to-end reinforcement learning framework for grounding natural language descriptions in videos . The paper is well-written and easy to follow . The proposed method is novel and well-motivated . However , I have some concerns about the novelty of the proposed method . 1 .The proposed method seems to be a direct application of existing methods for video grounding . The novelty of this paper is limited . The method is a combination of several existing methods . For example , the method of Gao et al . ( 2017 ) , which is used in this paper , is not compared with . 2 .The method of Hendricks et al. , 2017 is not applicable to general cases . 3 .The results are not convincing . The performance of the method is not better than the state-of-the-art methods . 4 .The paper is not well-organized . There are many typos and grammatical errors in the paper . I suggest the authors to proofread the paper carefully .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a SAT solver to solve the projective plane of order 10 problem , which is the most challenging subcase of Lam 's problem . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates are generated with the assistance of the symbolic computation library Traces . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that the certificates are more verifiable , but they do not provide a formal proof of the nonexistence of projective planes of order ten because they rely on results that currently have no formal computerverifiable proofs . 2 .It is not clear to me how the certificates can be verified by a third party . 3 .In the proof of Theorem 3.1 , the authors use a special-purpose solver , but I do not know how to verify the correctness of the certificates . 4 .The proof of theorem 3.2 is not correct . The proof is based on the fact that the error correcting code must contain words that are called `` primitive `` or `` weight `` 19 words , which are the first two cases that are first ruled out by the search of ( MacWilliams , Slane , and Thompson 1973 ) and were recently settled via SAT-based nonexistence certificates ( Bright et al. , 2020a ,b ) . 5 .In Theorem 4.2 , the proof is not complete . It is not possible to prove the existence of the plane .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and shows that they are at most as powerful as the Weisfeiler-Lehman ( WL ) graph isomorphism test in distinguishing graph structures . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of feature vectors of a given node ‘ s neighbors as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNNS can be thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . 2 .The paper also shows that the most powerful GNN by our theory , ie., Graph Isomorphism Network ( GIN ) , also empirically has high representation power as it fits the data perfectly . 3 .The experimental results show that the proposed GIN can not distinguish graph structures that can not be distinguished by popular GNN variants , GCN ( Kipf & Welling , 2017 ) and GraphSA ( Hamilton et al. , 2017a ) and GIN ( Velickovic et .al. , 2018 ) . 4 .In the experiments , the authors compare the performance of GIN with various aggregation functions with various graph aggregation functions . It would be better if the authors can compare the results with the results of GNN with different aggregation functions and graph-level pooling schemes .
1_1907.07355 = This paper presents an analysis of the performance of BERT on the argument reasoning mining task ( ARCT ) . The authors show that BERT is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated in ARCT by adding a copy of each data point with the claim negated and the label inverted . This means that the distribution of statistical cues in the warrants will be mirrored over both labels , eliminating the signal . On this adversarial dataset all models perform randomly , with BERT achieving a maximum test set accuracy of 53 % . This paper provides a more robust evaluation of argument comprehension and should be adopted as the standard in future work on this dataset . The paper is well written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me how to interpret the results in Table 1 . In Table 1 , BERT achieves 77 % accuracy with its best run , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question : what has BERT learned about argument comprehension ? To investigate BERT ’ s decision making , the authors looked at data points it finds easy to classify over multiple runs . 2 .In Table 2 , the results are not consistent with the results of Habernal et al . ( 2018b ) , which shows BERT exploits “ cue words ” in the ARCT SemEval submissions , and consistent with their results . It would be better if the authors can provide more explanation about this result .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) to reduce the performance gap between low-precision and full precision training in deep learning by quantizing all numbers except the gradient accumulator to 8 bits without changing the network structure . The proposed method is simple to implement and has little computational overhead . The paper is well written and easy to follow . The main contribution of this paper is to show that SWA can converge to a smaller asymptotic noise ball than SGD for quadratic and strongly convex objectives . The experimental results show that the proposed method can match the full precision SGD baseline in training ResNet-164 on CIFAR-10 and Cifar-100 datasets . However , I have some concerns about this paper . 1 .The proposed method seems to be a straightforward extension of SWA ( Izmailov et al. , 2018a ) . It is not clear to me why SWA is better than SWA applied to full precision . 2 .In the experiments , the authors only compare the performance of 8-bit SWA with SGD . It would be better if the authors can also compare with the state-of-the-art results of Wang et al . ( 2018 ) . 3 .In Table 1 , the comparison between SWA and SGD is not fair . The authors should compare the results with the results of full precision and low precision training .
1_1902.04911 = This paper proposes a method for knowledge-grounded dialogue generation . The authors propose to use a prior distribution over knowledge and a posterior distribution over responses . The prior distribution is learned by minimizing the KL divergence between the prior distribution and the posterior distribution . The posterior distribution is inferred from both the utterance and the response , and the actual knowledge used in the response is considered . The paper is well-written and easy to follow . The proposed method is novel and the experimental results are convincing . However , I have some concerns about the novelty of the proposed method . 1 .The proposed method seems to be a straightforward combination of existing methods . For example , the authors mentioned that the previous work [ Dinan et al. , 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . But the authors did not provide any comparison with this work . 2 .The authors claimed that the proposed model is able to learn the correct knowledge selection mechanism , but it is not clear to me how the model is trained . It seems to me that the authors trained the model by minimizing a KL-divergence between the posterior and the prior . Then , the model samples the knowledge from the prior and uses it to generate the response . It would be better if the authors can provide more details about the training procedure . 3 .In the experiments , it would be more convincing if the author can show the performance of the model on the Persona-chat dataset . 4 .In Table 1 , it is better to show the results on the Wizard of Wikipedia dataset .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea is to prune the architecture of the model by minimizing both the pre-training loss and the number of model parameters . The paper is well written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me what is the advantage of the proposed pruning method compared to other pruning methods . For example , the pruning of the attention heads in ALBERT ( Lan et al. , 2019 ) . 2 .The authors claim that the proposed method can reduce the model parameters while increasing the latency , but the experiments do not show the effect of pruning on the model performance . 3 .The experiments are only conducted on the MNLI task . It would be better if the authors can conduct experiments on other NLP tasks , e.g. , WMT , NLI , etc . .
1_2102.07983 = This paper introduces a new dataset called FEWS ( Few-shot Examples of Word Senses ) , which is a dataset for evaluating Word Sense Disambiguation ( WSD ) models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary definitions of 71,000 senses , which are used as training data for WSD models . The authors also conduct experiments to compare the performance of knowledge-based approaches and a recent neural biencoder model on the new dataset . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset to evaluate the WSD model in the few-and-zero-shot setting , but it is not clear to me how significant this contribution is . 2 .The experimental results are not convincing . For example , in Table 1 , the results of the biencoders are not compared to human annotators , which makes it hard to judge the significance of the results . 3 .The transfer learning experiments are not well-conducted . It would be better to conduct the transfer learning experiment to see if the proposed dataset can be used as additional training data to improve the performance on other WSD tasks .
1_1812.02425 = This paper proposes a method for ensembling multiple neural networks with no additional testing cost . The proposed method is based on the teacher-student learning paradigm , where the teacher network is pre-trained and the student network is trained using adversarial learning . The authors show that the proposed method consistently improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a variety of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of this paper is limited . The idea of using the soft labels as supervision is not new . It has been proposed in [ 1 ] , [ 2 ] and [ 3 ] . 2 .The proposed method can achieve the goal of ensembled multiple neural network with no extra testing cost , but the performance gain is not significant . 3 .The experiments are not convincing . For example , in Table 1 , the performance of Shake-Shake-MEAL is not better than the baseline method , while the performance improvement of MEAL is small . 4 .In Table 2 , it is not clear why the performance is better than Shake-shake-MeAL . 5 .The authors should compare the performance with other ensembles , e.g. , [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,44,47,48,50,49,51,52,54,55,56,58,59,60,61,63,64,70,72,73,72 ,73,74,75,76,77,78,80,81,82,83,84,88,89,90,91,93,94,93 ,94,98,99,99 ,100,100,103,104 ,104,104,105,106,107,108 ,108,108,111,113,114,115,116,117,118,119,120,121,122,122 ,128,128,129,131,128 ,128 ,129,129 ,131,132,131 ,131 ,132,133,136,137,136 ,128-129 ,136-136 ,137-137 ,138-138 ,139-139 ,144-140 . [ 1 , 1 ] https : //arxiv.org/abs/1806.059062 [ 2 , 2 ] http : //proceedings.mlr.press/v48/papers/1605.06570 [ 3 , 3 ] https: //arXiv:1705.09737.03780 [ 4 ] https://arxives.cc/abs-1605/1606.03880 [ 5 ] https //ar xiv.cc /abs/1607.03902 [ 6 ] httpsÂ Arxiv:1611.06470 [ 7 ] https { Arxives : https :Â ArXiv.pdf ] https = = = > https : > ArXives.org [ 8 ] [ 9 ]
1_1901.06829 = This paper proposes an end-to-end reinforcement learning framework for grounding natural language descriptions in videos . Specifically , the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . The agent makes an observation that details the currently selected video clip , and takes an action according to its policy , and the environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . The paper is well-written and easy to follow . The proposed method is novel and interesting . However , I have the following concerns : 1 . The novelty of this paper is limited . It is not clear how the proposed method can be applied to other video grounding tasks such as video retrieval and text-oriented video highlight detection . 2 .The experiments are not convincing . The performance of the proposed model is not better than the state-of-the-art baselines . 3 .The proposed method does not outperform the baselines in Table 1 and Table 2 . 4 .The paper is not well-organized . For example , there are many typos and grammatical errors .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a SAT solver to solve the projective plane of order 10 problem , which is the most challenging subcase of Lam 's problem . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates are generated with the assistance of the symbolic computation library Traces . The paper is well-written and easy to follow . However , I have the following concerns . 1 .The paper does not provide a completely formal proof of the nonexistence of a projective planes of order ten because it relies on results that currently have no formal computerverifiable proofs . 2 .It is not clear how one can verify the certificate without needing to trust the output of the Traces library . 3 .The authors claim that the certificates are more verifiable because a third party can check the certificates for themselves and ( once they believe in the encoding ) be convinced in the existence of the plane without having to trust a search procedure . But I do not see how the certificates can be verified . 4 .In the proof of Theorem 3.1 , the authors use a special-purpose solver ( SAT ) instead of a special purpose solver . I am not sure if this is correct .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and shows that they are at most as powerful as the Weisfeiler-Lehman ( WL ) graph isomorphism test in distinguishing graph structures . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of feature vectors of a given node ‘ s neighbors as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNNS can be thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . 2 .The paper also shows that the most powerful GNN by our theory , ie., Graph Isomorphism Network ( GIN ) , also empirically has high representation power as it fits the data perfectly . 3 .The experimental results show that the proposed GIN can not distinguish graph structures that can not be distinguished by popular GNN variants , GCN ( Kipf & Welling , 2017 ) and GraphSA ( Hamilton et al. , 2017a ) and GIN ( Velickovic et .al. , 2018 ) . 4 .In the experiments , the authors compare the performance of GIN with various aggregation functions with various graph aggregation functions . It would be better if the authors can compare the results with the results of GNN with different aggregation functions and graph-level pooling schemes .
1_1907.07355 = This paper investigates the reason why BERT performs so well on the argument reasoning mining task . The authors show that BERT is able to exploit the presence of statistical cues in the warrants in the ARCT task . They also show that the major problem can be eliminated in ARCT by adding a copy of each data point with the claim negated and the label inverted . The paper is well written and easy to follow . However , I have the following concerns : 1 . In the experiments , the authors only compare BERT with the unsupervised baseline . It would be better if the authors can compare with the state-of-the-art . 2 .In Table 1 , the performance of BERT on the adversarial dataset is much lower than the human performance . It will be better to compare the performance with the human baseline . 3 .The authors should provide more details about the dataset used in the experiments . For example , what is the size of the dataset ? What is the number of training examples ? How many training examples were used in each experiment ? 4 .In the experiment , it is better to show the performance on the full dataset .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The authors prove that for quadratic and strongly convex objectives , SWA converges to a noise ball that is asymptotically smaller than that of low precision SGD . They also show that their method can significantly reduce the performance gap between low precision and full precision training . This paper is well-written and easy to follow . The idea of using SWA to reduce the quantization noise during training is interesting and novel . However , I have the following concerns : 1 . The paper is not well-motivated . The motivation of SWA is not clear . In the introduction , the authors claim that SWA takes an average of SGD iterates with a modified learning rate schedule and has been shown to lead to wider optima ( Izmailov et al. , 2018a ) . But in the experiments , it seems that the authors only compare SWA with SGD with a low learning rate . 2 .The experiments are not convincing . In Table 1 , the performance of 8-bit SWA does not match the full precision baseline . 3 .In Table 2 , the accuracy of the proposed method is not better than the baseline . 4 .The authors should provide more details about the implementation details . For example , how to choose the number of bits in the quantized grid ? 5 .In the experiment , it is better to compare the performance with the state-of-the-art methods .
1_1902.04911 = This paper proposes a method for knowledge-grounded dialogue generation , where the prior distribution over knowledge is inferred from both utterance and response , and the posterior distribution over the actual knowledge used in the response is considered . The paper is well-written and easy to follow . The idea of using knowledge selection to guide knowledge selection in dialogue generation is interesting . However , I have the following concerns : 1 . The proposed method is based on the KL-divergence between the prior and posterior distributions , which is not a new idea . The authors should compare the proposed method with other related methods . 2 .The authors should also compare the performance of their method with the baselines . 3 .In the experiments , the authors only compare with a single baseline . It would be more convincing if the authors can compare with other baselines , such as [ 1 ] , [ 2 ] , and [ 3 ] . 4 .In Table 1 , it would be better to compare with more baselines to show the effectiveness of knowledge selection . 5 .In Figure 2 , it is not clear why the authors did not compare with the knowledge selection method in [ 1 , 2 ] . 6 .The author should compare with [ 2 , 3 ] and [ 4 ] in the experiments . 7 .It would be interesting to see how the model performs when the knowledge is sampled from a knowledge graph .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea is to prune the architecture of the model by minimizing both the pre-training loss and the number of model parameters . The paper is well written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me what is the advantage of the proposed pruning method compared to other pruning methods . For example , the pruning of the attention heads in ALBERT ( Lan et al. , 2019 ) . 2 .The authors claim that the proposed method can reduce the model parameters while increasing the latency , but the experiments do not show the effect of pruning on the model performance . 3 .The experiments are only conducted on the MNLI task . It would be better if the authors can conduct experiments on other NLP tasks , e.g. , WMT , NLI , etc . .
1_2102.07983 = This paper introduces a new dataset called FEWS ( Few-shot Examples of Word Senses ) , which is a dataset for evaluating Word Sense Disambiguation ( WSD ) models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary definitions of 71,000 senses , which are used as training data for WSD models . The authors also conduct experiments to compare the performance of knowledge-based approaches and a recent neural biencoder model on the new dataset . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset to evaluate the WSD model in the few-and-zero-shot setting , but it is not clear to me how significant this contribution is . 2 .The experimental results are not convincing . For example , in Table 1 , the results of the biencoders are not compared to human annotators , which makes it hard to judge the significance of the results . 3 .The transfer learning experiments are not well-conducted . It would be better to conduct the transfer learning experiment to see if the proposed dataset can be used as additional training data to improve the performance on other WSD tasks .
1_1812.02425 = This paper proposes a method for ensembling multiple neural networks with no additional testing cost . The proposed method is based on the teacher-student learning paradigm , where the teacher network is pre-trained and the student network is trained using adversarial learning . The authors show that the proposed method consistently improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a variety of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of this paper is limited . The idea of using the soft labels as supervision is not new . It has been proposed in [ 1 ] , [ 2 ] and [ 3 ] . 2 .The proposed method can achieve the goal of ensembled multiple neural network with no extra testing cost , but the performance gain is not significant . 3 .The experiments are not convincing . For example , in Table 1 , the performance of Shake-Shake-MEAL is not better than the baseline method , while the performance improvement of MEAL is small . 4 .In Table 2 , it is not clear why the performance is better than Shake-shake-MeAL . 5 .The authors should compare the performance with other ensembles , e.g. , [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,44,47,48,50,49,51,52,54,55,56,58,59,60,61,63,64,70,72,73,72 ,73,74,75,76,77,78,80,81,82,83,84,88,89,90,91,93,94,93 ,94,98,99,99 ,100,100,103,104 ,104,104,105,106,107,108 ,108,108,111,113,114,115,116,117,118,119,120,121,122,122 ,128,128,129,131,128 ,128 ,129,129 ,131,132,131 ,131 ,132,133,136,137,136 ,128-129 ,136-136 ,137-137 ,138-138 ,139-139 ,144-140 . [ 1 , 1 ] https : //arxiv.org/abs/1806.059062 [ 2 , 2 ] http : //proceedings.mlr.press/v48/papers/1605.06570 [ 3 , 3 ] https: //arXiv:1705.09737.03780 [ 4 ] https://arxives.cc/abs-1605/1606.03880 [ 5 ] https //ar xiv.cc /abs/1607.03902 [ 6 ] httpsÂ Arxiv:1611.06470 [ 7 ] https { Arxives : https :Â ArXiv.pdf ] https = = = > https : > ArXives.org [ 8 ] [ 9 ]
1_1901.06829 = This paper proposes an end-to-end reinforcement learning framework for grounding natural language descriptions in videos . Specifically , the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . The agent makes an observation that details the currently selected video clip , and takes an action according to its policy , and the environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . The paper is well-written and easy to follow . The proposed method is novel and interesting . However , I have the following concerns : 1 . The novelty of this paper is limited . It is not clear how the proposed method can be applied to other video grounding tasks such as video retrieval and text-oriented video highlight detection . 2 .The experiments are not convincing . The performance of the proposed model is not better than the state-of-the-art baselines . 3 .The proposed method does not outperform the baselines in Table 1 and Table 2 . 4 .The paper is not well-organized . For example , there are many typos and grammatical errors .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a SAT solver to solve the projective plane of order 10 problem , which is the most challenging subcase of Lam 's problem . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates are generated with the assistance of the symbolic computation library Traces . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that the certificates are more verifiable , but they do not provide a complete formal proof of the nonexistence of a projective planes of order ten because they rely on results that currently have no formal computerverifiable proofs . 2 .It is not clear to me how the certificates can be verified by a third party . 3 .In the proof of Theorem 3.1 , the authors use the proof that the error correcting code associated with A1s and A2s must contain words that are weight 15 , weight 16 , or ‘ primitive weight 19 ’ . The former two cases were first ruled out by the first two cases of MacWilliams 1973 and Thompson 1973 , respectively , and the latter cases were recently settled via SAT-based nonexistence certificates ( Bright et al. , 2020a ,b ) . It would be better if the authors can provide more details on how to verify the correctness of the certificates . 4 .The proof of theorem 3.2 is not very clear . It is unclear to me what is the meaning of “ Theorem 2.1 ” and “ 2.2.1.2 ” . 5 .In Theorem 4.2 , it is unclear how to prove that the certificate is valid .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and shows that they are at most as powerful as the Weisfeiler-Lehman ( WL ) graph isomorphism test in distinguishing graph structures . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of feature vectors of a given node ‘ s neighbors as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNNS can be thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . 2 .The paper also shows that the most powerful GNN by our theory , ie., Graph Isomorphism Network ( GIN ) , also empirically has high representation power as it fits the data perfectly . 3 .The experimental results show that the proposed GIN can not distinguish graph structures that can not be distinguished by popular GNN variants , GCN ( Kipf & Welling , 2017 ) and GraphSA ( Hamilton et al. , 2017a ) and GIN ( Velickovic et .al. , 2018 ) . 4 .In the experiments , the authors compare the performance of GIN with various aggregation functions with various graph aggregation functions . It would be better if the authors can compare the results with the results of GNN with different aggregation functions and graph-level pooling schemes .
1_1907.07355 = This paper investigates the reason why BERT performs so well on the argument reasoning mining task . The authors show that BERT is able to exploit the presence of statistical cues in the warrants in the ARCT task . They also show that the major problem can be eliminated in ARCT by adding a copy of each data point with the claim negated and the label inverted . The paper is well written and easy to follow . However , I have the following concerns : 1 . In the experiments , the authors only compare BERT with the unsupervised baseline . It would be better if the authors can compare with the state-of-the-art . 2 .In Table 1 , the performance of BERT on the adversarial dataset is much lower than the human performance . It will be better to compare the performance with the human baseline . 3 .The authors should provide more details about the dataset used in the experiments . For example , what is the size of the dataset ? What is the number of training examples ? How many training examples were used in each experiment ? 4 .In the experiment , it is better to show the performance on the full dataset .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The authors prove that for quadratic and strongly convex objectives , SWA converges to a noise ball that is asymptotically smaller than that of low precision SGD . They also show that their method can significantly reduce the performance gap between low precision and full precision training . This paper is well-written and easy to follow . The idea of using SWA to reduce the quantization noise during training is interesting and novel . However , I have the following concerns : 1 . The paper is not well-motivated . The motivation of SWA is not clear . In the introduction , the authors claim that SWA takes an average of SGD iterates with a modified learning rate schedule and has been shown to lead to wider optima ( Izmailov et al. , 2018a ) . But in the experiments , it seems that the authors only compare SWA with SGD with a low learning rate . 2 .The experiments are not convincing . In Table 1 , the performance of 8-bit SWA does not match the full precision baseline . 3 .In Table 2 , the accuracy of the proposed method is not better than the baseline . 4 .The authors should provide more details about the implementation details . For example , how to choose the number of bits in the quantized grid ? 5 .In the experiment , it is better to compare the performance with the state-of-the-art methods .
1_1902.04911 = This paper proposes to use knowledge selection to guide dialogue generation . The key idea is to separate the prior distribution over knowledge from the posterior distribution over the response . The model is trained to minimize the KL divergence between the prior and posterior distributions . Then , the model samples the knowledge distribution based on the posterior information and incorporates the sampled knowledge into the response generation . Experiments are conducted on the Persona-chat dataset and Wizard of Wikipedia dataset . The paper is well-written and easy to follow . The idea of using knowledge selection for dialogue generation is interesting . However , I have the following concerns : 1 . The proposed model is not compared with any other knowledge-grounded dialogue generation models . For example , [ Dinan et al . [ 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 2 .The proposed model does not outperform the existing models by incorporating more knowledge . It is better to compare the proposed model with the baselines . 3 .The experiments are only conducted on one dataset . It would be better to conduct experiments on more datasets . 4 .The paper is not well-motivated . The motivation of this work is not clear . The authors claim that knowledge selection mechanism is effective for appropriate response generation , but they do not provide any empirical evidence to support this claim . 5 .The authors should provide more details about the model . For instance , what is the architecture of the model ? How many parameters are used for the model and how many parameters for the knowledge selection ? 6 .In the experiments , the authors only compare with the baseline models . It will be better if the authors can show the performance of the proposed method on other models as well .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea is to prune the architecture of the model by minimizing both the pre-training loss and the number of model parameters . The paper is well written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me what is the advantage of the proposed pruning method compared to other pruning methods . For example , the pruning of the attention heads in ALBERT ( Lan et al. , 2019 ) . 2 .The authors claim that the proposed method can reduce the model parameters while increasing the latency , but the experiments do not show the effect of pruning on the model performance . 3 .The experiments are only conducted on the MNLI task . It would be better if the authors can conduct experiments on other NLP tasks , e.g. , WMT , NLI , etc . .
1_2102.07983 = This paper introduces a new dataset called FEWS ( Few-shot Examples of Word Senses ) , which is a dataset for evaluating word sense disambiguation models in few-shot and zero-shot settings . The dataset is built on top of the Wiktionary dataset , which contains 71,000 senses . The authors compare the performance of knowledge-based approaches and a recent neural biencoder model for WSD on the new dataset . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset to evaluate WSD models in the low-shot setting , but it is not clear to me why this dataset is better than existing datasets such as SemCor and SemNet . 2 .The authors claim that the dataset is more comprehensive than SemCor , but I am not sure why this is the case . SemCor is the largest manually annotated WSD dataset , while the dataset in this paper only contains 71 ,000 senses , which makes it hard to compare with SemCor . 3 .In Table 1 , the authors mention that the BienCoder outperforms human annotators , but there is no comparison with the state-of-the-art on the dataset . 4 .The transfer learning experiment is interesting , but the authors do not provide any results on transfer learning . 5 .The paper is not well-organized . For example , in the introduction , it is hard to understand what is the purpose of Table 1 and Table 2 . 6 .In Section 3.2 , the paper claims that the proposed dataset will be used as additional training data for other WSD tasks , but no results are provided .
1_1812.02425 = This paper proposes a method for ensembling multiple neural networks with no additional testing cost . The proposed method is based on the teacher-student learning paradigm , where the teacher network is pre-trained and the student network is trained using adversarial learning . The authors show that the proposed method consistently improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a variety of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of this paper is limited . The idea of using the soft labels as supervision is not new . It has been proposed in [ 1 ] , [ 2 ] and [ 3 ] . 2 .The proposed method can achieve the goal of ensembled multiple neural network with no extra testing cost , but the performance gain is not significant . 3 .The experiments are not convincing . For example , in Table 1 , the performance of Shake-Shake-MEAL is not better than the baseline method , while the performance improvement of MEAL is small . 4 .In Table 2 , it is not clear why the performance is better than Shake-shake-MeAL . 5 .The authors should compare the performance with other ensembles , e.g. , [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,44,47,48,50,49,51,52,54,55,56,58,59,60,61,63,64,70,72,73,72 ,73,74,75,76,77,78,80,81,82,83,84,88,89,90,91,93,94,93 ,94,98,99,99 ,100,100,103,104 ,104,104,105,106,107,108 ,108,108,111,113,114,115,116,117,118,119,120,121,122,122 ,128,128,129,131,128 ,128 ,129,129 ,131,132,131 ,131 ,132,133,136,137,136 ,128-129 ,136-136 ,137-137 ,138-138 ,139-139 ,144-140 . [ 1 , 1 ] https : //arxiv.org/abs/1806.059062 [ 2 , 2 ] http : //proceedings.mlr.press/v48/papers/1605.06570 [ 3 , 3 ] https: //arXiv:1705.09737.03780 [ 4 ] https://arxives.cc/abs-1605/1606.03880 [ 5 ] https //ar xiv.cc /abs/1607.03902 [ 6 ] httpsÂ Arxiv:1611.06470 [ 7 ] https { Arxives : https :Â ArXiv.pdf ] https = = = > https : > ArXives.org [ 8 ] [ 9 ]
1_1901.06829 = This paper proposes an end-to-end reinforcement learning framework for grounding natural language descriptions in videos . Specifically , the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . The agent makes an observation that details the currently selected video clip , and takes an action according to its policy , and the environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . The paper is well-written and easy to follow . The proposed method is novel and interesting . However , I have the following concerns : 1 . The novelty of this paper is limited . There are many existing methods for action localization and video grounding tasks , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) .The proposed method can be applied only when all candidates are exhaustively enumerated , as they need to rank all possible clip-sentence pairs to be successful in generating the final grounding result . Moreover , the ranking strategy is specially designed for pre-segmented videos and is not applicable to general cases . To effectively detect the temporal boundaries of a given description from a given video , especially from a long , untrimmed video , it is important to make judgments only by several glimpses . 2 .The paper is not well-motivated . In the introduction , the authors claim that it requires to map natural language description to a close set of labels before applying these methods , which inevitably causes information loss . Although there is a growing interest in grounded natural language in videos , i.e. , if the sentence “The parent watches the boy play drums ” is mapped to an action label “play drums , ” , action detectors can hardly tell the start point of the presence of the parent . 3 .The experiments are not convincing . In Table 1 , the performance of the proposed method seems to be worse than the state-of-the-art methods . The authors should compare the performance with other methods .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .