1_2012.04715 = In this paper , the authors attempt to solve the projective plane of order 10 problem by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solvers instead of special-purpose search code to produce the certificates . However , it is not clear to me why the authors chose to use the SAT solver instead of the cube-and-conquer method . It would be better if the authors can provide more details about the choice of the solver and how it compares to the cube and conquer method . In addition , it would be interesting to see how the results compare to the results obtained by the original search of Lam , Crossfield , and Thiel 1985 .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) to capture different graph structures . The main contribution of the paper is to analyze the discriminative power of popular GNN variants , such as Graph Convolutional Networks and GraphSAGE , and show that they can not learn to distinguish certain simple graph structures , and develop a simple architecture that is provably the most expressive among the class of GNN and is as powerful as the Weisfeiler-Lehman graph isomorphism test . The paper is well-written and easy to follow . However , I have some concerns about the paper : 1 . Theoretical results are presented in the form of a set of functions over multisets that a GNN can represent .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low precision SGD iterates with a modified learning rate schedule . The authors show that the proposed method can match the performance of full-precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The paper is well written and easy to follow . The proposed method is simple to implement and the theoretical analysis is sound . However , I have some concerns about the experimental results . 1 .In the experiments , the authors only compare with SGD and HALP . It would be more convincing if the authors can compare with other methods such as QSGD and ZipML . 2 .In Table 1 , the comparison with HALP is not fair . It is better to compare with the full precision training method . 3 .In Section 4.2 , it would be better to show the convergence rate of the proposed algorithm . 4 .In Figure 2 , it is not clear what is the difference between SWA update and SWA . 5 .It is better if the author can provide more details about the algorithm .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that employs a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed model is novel and well-motivated . 3 .The experiments are comprehensive and show the effectiveness of the model . 4 .The paper is technically sound . Cons : 1 ) The proposed model seems to be a straightforward combination of two existing models . It would be better if the authors can provide a more detailed analysis of the differences between the two models . 2 ) It is not clear why the authors use the KLDivLoss to measure the proximity between the prior distribution and the posterior distribution . 3 ) The authors should provide more details about the model architecture . 4 ) The experiments are only conducted on the Wizard-of-Wikipedia dataset . It is better to conduct experiments on other datasets .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT model . The main idea is to prune the parameters of each layer of the model by 5 different dimensions . The pruning is done by optimizing the architecture search using the pruning-based architecture search technique . The paper is well written and easy to follow . However , I have the following concerns about this paper : 1 . The motivation of this paper is not clear . The authors claim that the proposed method can be applied towards other objectives such as FLOPs or latency . But the experimental results are not convincing . In Table 1 , the authors only show the results of the pruned model . 2 .In Table 2 , the proposed pruning method is not compared with other pruning methods such as DistilBERT ( Sanh et al. , 2019 ) and RoBERTa ( Liu et al .2019 ) .The authors should compare with these methods to show the effectiveness of the proposed methods . 3 .The proposed method is based on pruning the architecture design dimensions , but the authors do not show the performance of the final model . 4 .In the experiments , the prunable parameters are set to zero for each pruned dimension , but it is not shown how to set them to zero . 5 .The experimental results in Table 2 are not very convincing . The proposed method does not outperform the existing methods .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of few-shot and zero-shot senses extracted from Wikdatasets , which is not new . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low-latency manner , but it is not clear to me why this is the case . For example , in Table 1 , it is mentioned that FEWs contains 1.1 % of the senses that are considered to be toxic language . It would be more convincing if the authors can provide more details about this . 3 .In Table 2 , it would be interesting to see the performance of the model trained on FEWN and BEM . 4 .It would be better to provide more information about the training and evaluation time of the models . 5 .It is unclear to me how to interpret the results in Table 3 . It seems that the biencoder and the MFS baseline are trained on the same training set , but the results are not comparable . 6 .I am not sure why the authors did not report the training time of BEM and MFS in Table 2 .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensembling from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim , Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experimental results are not convincing . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a reinforcement learning based framework for the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed method is novel in that it successfully combines supervised learning with reinforcement learning in a multi-task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The experimental results show that the proposed method outperforms the baselines . Cons : 1 ) The novelty of this paper is limited . It is not clear to me why the proposed model is better than the baseline methods . 2 ) The proposed method can not be applied to other video grounding tasks . 3 ) The experimental results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors tackle the problem of finding a projective plane of order 10 from projective geometry . The authors translate the problem into Boolean logic and use satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solvers instead of special-purpose search code to solve the problem . However , I have the following concerns : 1 . In the proof of the certificate , it is not clear to me how to verify that the certificate is correct . The proof is based on the fact that the certificates are obtained by the SAT solver . It would be better if the authors can provide more details about the proof . 2 .In the proof , it would be more clear if the author can give more details on how to check the correctness of the certificates . For example , what is the error rate of the proof ? 3 .The paper is not well-organized . It is hard to follow the details of the proofs . For instance , the proof is not clearly written .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors show that the proposed method converges arbitrarily close to the optimal solution for quadratic objectives , and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and well-organized . However , I have the following concerns : 1 . In the experiments , it is not clear how to choose the learning rate for SWA and HALP . It would be better if the authors can provide more details about the choice of learning rate . 2 .In the experiment , the authors should compare with other methods such as QSGD , HALP , and Q-SGD . 3 .The authors should provide more analysis on the convergence of SWA to the noise ball . 4 .It is better to provide more theoretical analysis on SWA . 5 .In Section 4.2 , it would be more convincing if the author can provide some analysis on how to select the number of iterations for SWALp .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that uses both prior and posterior distributions over knowledge to facilitate knowledge selection . The paper is well-written and easy to follow . The idea of using the posterior distribution over knowledge is interesting and novel . However , I have the following concerns : 1 . In the prior knowledge module , the prior distribution p ( k|x ) is inferred from both the utterance and the response , while the posterior p ( y|x , y ) is only inferred from the responses . It is not clear to me how the prior p ( x ) can be used to select appropriate knowledge . 2 .In the inference process , the authors use the Kullback-Leibler divergence loss ( KLDivLoss ) to measure the distance between the prior and the posterior distributions . It seems that the authors do not use the KL divergence between the posterior and the prior . 3 .The authors should compare the performance of the proposed model with the baselines on the Persona dataset and Wizard-of-Wikipedia dataset . 4 .The proposed model is not compared with the baseline methods in the literature .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT language model . The main idea is to prune the parameters of each layer of the model in order to reduce its size . The pruning is done by optimizing the design dimensions of the encoder and decoder . The authors show that the pruned model achieves 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the model parameters is interesting . 3 .The experiments are well designed . 4 .The results are convincing . Cons : 1 ) It is not clear to me why the authors chose to use the prunable parameters for each design dimension . It would be better if the authors can provide more details about the pruning procedure . 2 ) The authors should compare the results with other pruning methods such as DistilBERT ( Sanh et al. , 2019 ) and RoBERTa ( Liu et al .2019 ) .3 ) It would also be better to show the performance of the proposed method with different number of layers .
1_2102.07983 = This paper introduces FEWS ( Few-shot Examples of Word Senses ) , a new low-shot WSD dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , this paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is just a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low manner , but it is not clear to me why this is the case . For example , in Table 1 , it seems that there are only 7.96 unique zero-shot senses in SemCor , while in Table 2 , there are 761 unique few-shot and 761 rare senses . 3 .In Table 3 , it is unclear why the biencoder outperforms human annotators . 4 .It is unclear to me how to interpret the results in Table 4 . It seems that the human performance is not as good as that of the neural models . 5 .I am not sure why the authors did not compare the performance of the knowledge based and neural models with human performance . 6 .I do not understand the motivation of the authors in Table 3 .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a reinforcement learning based framework for the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed model is novel in that it successfully combines supervised learning with reinforcement learning in a multi-task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper shows steady performance gains by considering additional supervised boundary information during training . 4 .The experimental results show that the proposed model outperforms the baselines . Cons : The novelty of this paper is limited . 1 .The novelty of the proposed method is limited because it is a simple combination of supervised learning and reinforcement learning . It is not clear whether the proposed approach can be applied to other video grounding tasks , e.g .sentence-based video retrieval methods . The experiments are not convincing enough to show the effectiveness of this proposed approach .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the agent has strong preferences . Finally , the paper shows the computational complexity of finding different optimal assignments for the problem in the special case where the network has path or star structure . This paper is well-written and easy to follow . The main idea of this paper is to study the problem of finding a reachable assignment for each agent in the social network . The problem is interesting and the results are interesting . However , I have the following concerns . 1 .The paper is not well-motivated . The motivation of the paper is based on social networks , but it is not clear to me what is the difference between social networks and traditional fairness notions . 2 .The algorithm is not clearly explained . It seems that the algorithm is a generalization of the 2-Sat algorithm . It would be better if the authors can give more details about the algorithm . 3 .The proof of Theorem 1 and Theorem 2 is not very clear . The proof is not easy to understand . 4 .Theorem 3 and 4 are not well explained . 5 .In the proof of Lemma 2 , it seems that there is a typo .
1_2012.04715 = In this paper , the authors attempt to solve the projective plane of order 10 problem by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to use SAT solvers instead of special-purpose search code to solve projective geometry problem . It would be better if the authors can provide more details on how the SAT solver is used in this paper . For example , how does the solver used in the paper is different from the one used in [ 1 ] and [ 2 ] ? 2 .The authors claim that the proposed method is less error-prone than writing special purpose search code , but it is not clear to me how to verify the correctness of this claim . 3 .In the paper , it is claimed that the authors found consistency issues in both previous search and independent verification in 2011 , but the authors did not provide any explanation on why this is the case . 4 .The paper is not well organized . It is hard to follow the main idea of the paper . In the introduction , the author claims that the original search and the independent verification were performed using highly specialized custom-written code and did not produce a nonexistence certificate . But the authors do not explain how these searches were performed and why they were not able to produce a certificate . The authors should provide more explanation on this point .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) to capture different graph structures . The main contribution of the paper is to analyze the discriminative power of popular GNN variants , such as Graph Convolutional Networks and GraphSAGE , and show that they can not learn to distinguish certain simple graph structures , and develop a simple architecture that is provably the most expressive among the class of GNN and is as powerful as the Weisfeiler-Lehman graph isomorphism test . The paper is well-written and easy to follow . However , I have some concerns about the paper : 1 . Theoretical results are presented in the form of a set of functions over multisets that a GNN can represent .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation with knowledge-grounded responses . The authors propose a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . The paper is well-written and easy to follow . However , I have the following concerns . 1 .The authors claim that their model is the first neural model which incorporates the posterior distribution as guidance , enabling accurate knowledge lookups and high quality response generation . But , it is not clear to me how this is achieved . For example , the prior distribution p ( k|x ) is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution is inferred only from utterances only , so that appropriate knowledge can be selected even without responses during the inference process . Compared with the previous work , our model can better incorporate appropriate knowledge in response generation , which is better than previous work . 2 .In Table 1 , the authors compare the performance of their model with Seq2Seq model with a GRU decoder where knowledge is concatenated . It would be better if the authors can show the results of different ways of incorporating knowledge incorporating knowledge . 3 .It is better to show the effect of different loss functions in Table 1 .
1_2005.06628 = This paper proposes a method to reduce the number of parameters in the BERT model . The main idea is to use a smaller number of layers in the encoder of BERT . The authors show that this can be achieved by reducing the architecture design dimensions rather than reducing the Transformer encoder layers . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that the proposed method can be applied towards other objectives such as FLOPs or latency . But the authors do not provide any experimental results to support this claim . 2 .The experimental results are not convincing . The proposed method is only compared with BERT with three layers . It is not clear whether this method can also be applied to other models such as Transformer-based models . 3 .In Table 1 , the authors only show the results for BERT without three layers , which is not enough to show the effectiveness of this method .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experimental results are not convincing . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper is technically sound . Cons : 1.1 . The novelty of the proposed method is limited . 2.2 .The experimental results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) to capture different graph structures . The main contribution of the paper is to analyze the discriminative power of popular GNN variants , such as Graph Convolutional Networks and GraphSAGE , and show that they can not learn to distinguish certain simple graph structures , and develop a simple architecture that is provably the most expressive among the class of GNN and is as powerful as the Weisfeiler-Lehman graph isomorphism test . The paper is well-written and easy to follow . However , I have the following concerns : 1 . In the abstract , the authors claim that the most powerful GNN , i.e. , Graph Isomorphism Network ( GIN ) , also empirically has high representational power as it almost perfectly fits the training data , whereas the less powerful models often severely underfit the data . But , in the experiments , the GIN only outperforms the state-of-the-art results on a number of graph classification benchmarks , which is not convincing . 2 .In the experimental results , the performance of the proposed GIN is not better than the other GNN models . 3 .The experimental results are not convincing enough . In Table 1 , the results of GIN and GCN are not comparable . 4 .In Table 2 , the result of GCN and GIN are not compared .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that employs a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed model is novel and well-motivated . 3 .The experiments are comprehensive and show the effectiveness of the model . 4 .The paper is technically sound . Cons : 1 ) The proposed model seems to be a straightforward combination of two existing models . It would be better if the authors can provide a more detailed analysis of the differences between the two models . 2 ) It is not clear why the authors use the KLDivLoss to measure the proximity between the prior distribution and the posterior distribution . 3 ) The authors should provide more details about the model architecture . 4 ) The experiments are only conducted on the Wizard-of-Wikipedia dataset . It is better to conduct experiments on other datasets .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT model . The main idea is to prune the parameters of each layer of the model by 5 different dimensions . The pruning method is based on pruning the architecture design dimensions and the pruning-based architecture search technique . Experiments on GLUE and SQuAD datasets show that the pruned model achieves 6.6 % higher average accuracy compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning different layers of BERT is interesting and novel . 3 .The experimental results are convincing . 4 .The pruning based architecture search method can be applied to other objectives such as FLOPs or latency . Cons : 1 ) It is not clear to me why the prunable parameters in Eq . ( 1 ) and ( 2 ) are pruned . 2 ) The pruned parameters are not different from the original BERT parameters . 3 ) The proposed method is not compared with other pruning methods such as DistilBERT and RoBERTa .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experimental results are not convincing . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The experimental results demonstrate the effectiveness of the proposed approach . Cons : 1.1 . The novelty of the approach is limited . 2.2 .The performance gain is not significant .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the 2-Sat problem and the star problem can both be solved by fast solvers . The main ideas of the two algorithms are similar : first characterizing some structural properties of the problem and then reducing the problem to the polynomal time solvable 2-sat problem . In general , the paper is well-written and easy to follow . However , I have the following concerns : 1 . In Section 2.3.1 , it is not clear to me what is the meaning of `` compatible `` and `` non-compatible `` . 2 .In Theorem 1 , the statement of Theorem 2 is not very clear . 3 .In Section 3.2 , the definition of Pareto Efficiency is not well-defined . 4 .Theorem 2 and Theorem 3 are not clear .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that uses both prior and posterior distributions over knowledge to facilitate knowledge selection . The paper is well-written and easy to follow . The idea of using the posterior distribution over knowledge is interesting and novel . However , I have the following concerns : 1 . In the prior knowledge module , the prior distribution p ( k|x ) is inferred from both the utterance and the response , while the posterior p ( y|x , y ) is only inferred from the responses . It is not clear to me how the prior p ( x ) can be used to select appropriate knowledge . 2 .In the inference process , the authors use the Kullback-Leibler divergence loss ( KLDivLoss ) to measure the distance between the prior and the posterior distributions . It seems that the authors do not use the KL divergence between the posterior and the prior . 3 .The authors should compare the performance of the proposed model with the baselines on the Persona dataset and Wizard-of-Wikipedia dataset . 4 .The proposed model is not compared with the baseline methods in the literature .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT model . The main idea is to prune the architecture of the model by using pruning-based architecture search . The pruning is done by optimizing the design dimensions of the encoder layers . The authors show that the pruned model achieves 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoders . Pros : - The paper is well-written and easy to follow . - The idea of pruning the model is interesting . Cons : - It is not clear to me how the pruning can be applied to other objectives such as FLOPs or latency . - There is no comparison with other pruning methods such as DistilBERT ( Sanh et al. , 2019 ) and RoBERTa ( Liu et al .2019 ) .- The experiments are limited to a single task ( MNLI ) . It would be better if the authors can conduct more experiments on more downstream tasks such as speech recognition and language modeling .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper is technically sound . Cons : 1.1 . The novelty of the proposed method is limited . 2.2 .The experimental results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low precision SGD iterates with a modified learning rate schedule . The authors show that the proposed method can match the performance of full-precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The paper is well written and easy to follow . The proposed method is simple to implement and the theoretical analysis is sound . However , I have the following concerns : 1 . In the experiments , it is not clear how to choose the hyperparameters for the experiments . For example , how to set the number of iterations ? 2 .In the experiment , the authors only compare the performance with 8-bit quantized models . It is better to compare with full precision models as well . 3 .The authors should compare with other methods such as HALP , QSGD , and ZipML .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that uses both prior and posterior distributions over knowledge to facilitate knowledge selection . The paper is well-written and easy to follow . The idea of using the posterior distribution over knowledge is interesting and novel . However , I have the following concerns : 1 . In the prior knowledge module , the prior distribution p ( k|x ) is inferred from both the utterance and the response , while the posterior p ( y|x , y ) is only inferred from the responses . It is not clear to me how the prior p ( x ) can be used to select appropriate knowledge . 2 .In the inference process , the authors use the Kullback-Leibler divergence loss ( KLDivLoss ) to measure the distance between the prior and the posterior distributions . It seems that the authors do not use the KL divergence between the posterior and the prior . 3 .The authors should compare the performance of the proposed model with the baselines on the Persona dataset and Wizard-of-Wikipedia dataset . 4 .The proposed model is not compared with the baseline methods in the literature .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT language model . The main idea is to prune the parameters of each layer of BERT by 5 different dimensions . The pruning is done by optimizing the design dimensions of the encoder and decoding layers . The authors show that the pruned model achieves 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number of parameter . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the model parameters is interesting . 3 .The experiments are well designed . 4 .The pruning method is simple and effective . 5 .The paper is easy to read . Cons : 1.1 . The novelty of this paper is limited . The proposed pruning technique is not novel . The idea of using pruning to reduce parameters is not new . 2.2 .The experimental results are not convincing . 3.3 .The proposed method is not compared with other pruning methods such as RoBERTa . 4.4 .The results of the proposed method are not significant .
1_2102.07983 = This paper introduces FEWS ( Few-shot Examples of Word Senses ) , a new low-shot WSD dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , this paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of few-shot and zero-shot senses extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low-cost manner , however , it is not clear how this is achieved . For example , in Table 1 , the number of rare senses is shown to be 1.1 % . It would be better if the authors can provide more details about this number . 3 .In Table 2 , why the biencoder outperforms human annotators on the zero shot senses ?
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a reinforcement learning based framework for video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed model is novel in that it successfully combines supervised learning with reinforcement learning in a multi-task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The experimental results show that the proposed model outperforms the baselines . Cons : 1.1 . The novelty of this paper is limited . It is not clear to me how the proposed method can be applied to other video grounding tasks , e.g. , ( Gao , Sun , and Nevatia 2016 ) . 2.2 .The novelty of the proposed approach is limited to the video grounding task . 3.3 .The performance gain is not significant .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem is polylogarithmic time solvable when the network is a path or a star . The paper shows that when the preferences are weak ( ties among objects are allowed ) , the problem becomes NP-hard . The main idea of the paper is to show that the problem can be reduced to the 2-Sat problem and then it can be solved in polynomially time by using fast solvers for 2-sat problem . In general , this paper is well-written and easy to follow . However , I have the following concerns . 1 .The main ideas of the two algorithms are similar : first characterizing some structural properties of the problem and reducing the problem to the polynomal time solver , then reducing it to 2-sant problem . It would be better if the authors can give more details about the algorithm . 2 .In the proof of Theorem 2 , the authors only give the proof sketches . It will be better to give the full proof in the main paper . 3 .In Theorem 1 , the statement of the result is not clear . The statement is not consistent with the proof sketch .
1_2012.04715 = In this paper , the authors attempt to solve the projective plane of order 10 problem by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to use SAT solvers instead of special-purpose search code to solve projective geometry problem . It would be better if the authors can provide more details on how the SAT solver is used in this paper . For example , how does the solver used in the paper is different from the one used in [ 1 ] and [ 2 ] ? 2 .The authors claim that the proposed method is less error-prone than writing special purpose search code , but it is not clear to me how to verify the correctness of this claim . 3 .In the paper , it is claimed that the authors found consistency issues in both previous search and independent verification in 2011 , but the authors did not provide any explanation on why this is the case . 4 .The paper is not well organized . It is hard to follow the main idea of the paper . In the introduction , the author claims that the original search and the independent verification were performed using highly specialized custom-written code and did not produce a nonexistence certificate . But the authors do not explain how these searches were performed and why they were not able to produce a certificate . The authors should provide more explanation on this point .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) to capture different graph structures . The main contribution of the paper is to analyze the discriminative power of popular GNN variants , such as Graph Convolutional Networks and GraphSAGE , and show that they can not learn to distinguish certain simple graph structures , and develop a simple architecture that is provably the most expressive among the class of GNN and is as powerful as the Weisfeiler-Lehman graph isomorphism test . The paper is well-written and easy to follow . However , I have the following concerns : 1 . In the abstract , the authors claim that the most powerful GNN , i.e. , Graph Isomorphism Network ( GIN ) , also empirically has high representational power as it almost perfectly fits the training data , whereas the less powerful models often severely underfit the data . But , in the experiments , the GIN only outperforms the state-of-the-art results on a number of graph classification benchmarks , which is not convincing . 2 .In the experimental results , the performance of the proposed GIN is not better than the other GNN models . 3 .The experimental results are not convincing enough . In Table 1 , the results of GIN and GCN are not comparable . 4 .In Table 2 , the result of GCN and GIN are not compared .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that uses both prior and posterior distributions over knowledge to facilitate knowledge selection . The paper is well-written and easy to follow . The idea of using the posterior distribution over knowledge is interesting and novel . However , I have the following concerns : 1 . In the prior knowledge module , the prior distribution p ( k|x ) is inferred from both the utterance and the response , while the posterior p ( y|x , y ) is only inferred from the responses . It is not clear to me how the prior p ( x ) can be used to select appropriate knowledge . 2 .In the inference process , the authors use the Kullback-Leibler divergence loss ( KLDivLoss ) to measure the distance between the prior and the posterior distributions . It seems that the authors do not use the KL divergence between the posterior and the prior . 3 .The authors should compare the performance of the proposed model with the baselines on the Persona dataset and Wizard-of-Wikipedia dataset . 4 .The proposed model is not compared with the baseline methods in the literature .
1_2005.06628 = This paper proposes a pruning-based approach to reduce the number of parameters in the BERT language model . The main idea is to prune the architecture of the encoder and the attention head of the Transformer . The pruning is done by optimizing the architecture design dimensions of each layer of the model , and the pruned parameters are then used to train the model in a pre-trained fashion . The authors show that the proposed method can achieve 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the architecture is interesting . 3 .The proposed pruning method is simple and effective . Cons : 1.1 . It is not clear why the authors chose to optimize the design dimensions in the pruning framework rather than launching a pretraining job for each of these choices . 2.2 .The authors should compare the performance of the proposed approach with other pruning methods such as RoBERTa ( Liu et al. , 2019 ) and DistilBERT ( Sanh et al .2019 ) .
1_2102.07983 = This paper introduces FEWS ( Few-shot Examples of Word Senses ) , a new low-shot WSD dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , this paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of few-shot and zero-shot senses extracted from Wikdatasets , which is not a new dataset . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low-cost manner , but it is not clear to me why this is the case . It would be better if the authors can provide more explanation on this point . 3 .In Table 1 , it is unclear to me what is the performance of the biencoder model on the zero shot senses . Is it the same as the baseline model ? 4 .The evaluation set is not very comprehensive . It seems that the authors did not compare with the SemCor dataset , which contains 761 unique zero-shots and 761 few-shots senses . It will be better to provide more details on the evaluation set . 5 .The paper is not well-organized . For example , the abstract and introduction should be moved to the main paper . 6 .The experiments are not convincing . The results are not compared with the state-of-the-art .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensembling from multi-original models ; ( 3 ) the teacher network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of this paper is limited . There is a large body of previous work ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim , 2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experimental results are not convincing . In Table 1 , the performance of the proposed method is not better than that of the baselines . 3 .In Table 2 , the results of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper is technically sound . Cons : 1.1 . The novelty of the proposed method is limited . 2.2 .The experimental results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that incorporates knowledge into the response generation process . The authors propose a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . The paper is well-written and easy to follow . The idea of incorporating knowledge into dialogue generation is interesting and novel . However , I have the following concerns : 1 . In the prior knowledge module , the authors define a conditional probability distribution over knowledge , denoted by p ( k|x ) = exp ( exp ( k ) ) . It is not clear to me what is the relationship between the prior distribution and the posterior distribution . 2 .In the inference process , how to choose the correct knowledge K_1 , K_2 , and K_3 ? How to choose K_4 ? 3 .How to select K_5 ? 4 .The authors claim that the proposed method can better incorporate appropriate knowledge in response generation . But the authors did not provide any experimental results to support this claim . 5 .In Table 1 , the performance of R3 and R2 is not compared with the baseline methods .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT language model . The main idea is to prune the parameters of the model in order to reduce its size . The paper is well written and easy to follow . However , I have the following concerns : 1 . The proposed method is based on pruning the architecture design dimensions of BERT , which is not a new idea . The authors should compare with other pruning methods such as DistilBERT ( Sanh et al. , 2019 ) and RoBERTa ( Liu et al .2019 ) .2 .The experiments are limited to GLUE and SQuAD datasets . It would be better if the authors can conduct more experiments on other NLP tasks . 3 .The authors should provide more details about the pruning method . For example , how many prunable parameters are pruned in each pruning step ? 4 .The proposed method can be applied to other objectives such as FLOPs or latency . It is better to show the performance of the proposed method on other tasks .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper is technically sound . Cons : 1.1 . The novelty of the proposed method is limited . 2.2 .The experimental results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors tackle the problem of finding a projective plane of order 10 from projective geometry . The authors translate the problem into Boolean logic and use satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solvers instead of special-purpose search code to solve the problem . However , it is not clear to me why the authors chose to use the SAT solver instead of the cube-and-conquer method . It would be better if the authors can provide more details about the choice of the solver . In particular , it would be interesting to see the performance of the proposed solver compared to the existing solver in terms of speed and accuracy .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that employs a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . The paper is well-written and easy to follow . The proposed method is novel and interesting . However , I have the following concerns : 1 . The authors claim that the prior distribution over knowledge is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution is used to approximate the posterior distribution so that appropriate knowledge can be selected even without responses during the inference process . Compared with the previous work , the proposed method can better incorporate appropriate knowledge in response generation . 2 .It is not clear to me why the authors use the Kullback-Leibler divergence loss ( KLDivLoss ) to measure the proximity between posterior distribution and posterior distribution . 3 .The authors should provide more details about the model architecture . 4 .In the experiments , the authors should compare the performance of different variants of their model with the baselines , such as the hard knowledge-grounded model with a GRU decoder with a knowledge concatenated model .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT language model . The main idea is to prune the parameters of each layer of BERT by 5 different dimensions . The pruning is done by optimizing the design dimensions of the encoder and decoding layers . The authors show that the pruned model achieves 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number of parameter . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the model parameters is interesting . 3 .The experiments are well designed . 4 .The pruning method is simple and effective . 5 .The paper is easy to read . Cons : 1.1 . The novelty of this paper is limited . The proposed pruning technique is not novel . The idea of using pruning to reduce parameters is not new . 2.2 .The experimental results are not convincing . 3.3 .The proposed method is not compared with other pruning methods such as RoBERTa . 4.4 .The results of the proposed method are not significant .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where they define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensembling from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experimental results are not convincing . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The experimental results show that the proposed approach outperforms the baselines . Cons : 1.1 . The novelty of this paper is limited . It is not clear how the proposed method can be applied to other video grounding tasks , e.g .sentence-based video retrieval methods . 2.2 .The performance of the proposed model is not better than the state of the art .
1_1909.07557 = This paper studies the Housing Market problem , where the agents are embedded in a social network to denote the ability to exchange objects between them . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The authors show that when the preferences of the agents is weak ( ties among objects are allowed ) , the problem becomes NP-hard when the network is a path and can be solved in polynomial time when it is a star . It is left as an open problem whether the problem is polynomially-time solvable when the networks is path or star . The paper is well-written and easy to follow . The main ideas of the two algorithms are similar : first characterizing some structural properties of the problem and then reducing the problem to the 2-Sat problem . However , the main idea of the algorithm as delete all agents and objects as well as the initial endowment is not clear to me . It would be better if the authors can give some intuition about the algorithm .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that employs a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed model is novel and well-motivated . 3 .The experiments are comprehensive and show the effectiveness of the model . 4 .The paper is technically sound . Cons : 1 ) The proposed model seems to be a straightforward combination of two existing models . It would be better if the authors can provide a more detailed analysis of the differences between the two models . 2 ) It is not clear why the authors use the KLDivLoss to measure the proximity between the prior distribution and the posterior distribution . 3 ) The authors should provide more details about the model architecture . 4 ) The experiments are only conducted on the Wizard-of-Wikipedia dataset . It is better to conduct experiments on other datasets .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT model . The main idea is to prune the parameters of each layer of the model by 5 different dimensions . The pruning method is based on pruning the architecture design dimensions and the pruning-based architecture search technique . Experiments on GLUE and SQuAD datasets show that the pruned model achieves 6.6 % higher average accuracy compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning different layers of BERT is interesting and novel . 3 .The experimental results are convincing . 4 .The pruning based architecture search method can be applied to other objectives such as FLOPs or latency . Cons : 1 ) It is not clear to me why the prunable parameters in Eq . ( 1 ) and ( 2 ) are pruned . 2 ) The pruned parameters are not different from the original BERT parameters . 3 ) The proposed method is not compared with other pruning methods such as DistilBERT and RoBERTa .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensembling from multi-original models ; ( 3 ) the students can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of this paper is limited . There is a large body of previous work ( Hansen and Salamon 1990 ; Perrone and Cooper 1995 ; Krogh and Vedelsby 1995 ; Dietterich 2000 ; Huang et al.2017a ; Lakshminarayanan , 2017a ; Anil et al .2018 ) , which has applied to many fields such as image generation ( Johnson , Gupta , and Fei-Fei 2018 ) , detection ( Bai et .al.2018 , etc ) , etc . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper is technically sound . Cons : 1.1 . The novelty of the proposed method is limited . 2.2 .The experimental results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
