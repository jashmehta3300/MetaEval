1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low precision SGD iterates with a modified learning rate schedule . The authors show that the proposed method can match the performance of full-precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The paper is well written and easy to follow . The proposed method is simple to implement and the theoretical analysis is sound . However , I have the following concerns : 1 . In the experiments , the authors only compare the performance with 8-bit quantized SGD . It would be more convincing if the authors can compare with other quantized methods such as HALP or QSGD . 2 .The authors should compare with more quantized models such as ResNet-164 . 3 .The proposed method does not seem to work well in practice . For example , in Table 1 , the accuracy of SWA is lower than that of SGD with full precision . It is not clear why this is the case . 4 .In Table 2 , the performance gap between SWA and SGD is very small . It will be better to show the accuracy gap with other methods . 5 .In the experiment , it is better to compare the accuracy with other low precision methods .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation with knowledge-grounded responses . The authors propose a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . The paper is well-written and easy to follow . However , I have the following concerns . 1 .The authors claim that their model is the first neural model which incorporates the posterior distribution as guidance , enabling accurate knowledge lookups and high quality response generation . But , it is not clear to me how this is achieved . For example , the prior distribution p ( k|x ) is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution is inferred only from utterances only , so that appropriate knowledge can be selected even without responses during the inference process . Compared with the previous work , our model can better incorporate appropriate knowledge in response generation , which is better than previous work . 2 .In Table 1 , the authors compare the performance of their model with Seq2Seq model with a GRU decoder where knowledge is concatenated . It would be better if the authors can show the results of different ways of incorporating knowledge incorporating knowledge . 3 .It is better to show the effect of different loss functions in Table 1 .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT model . The main idea is to prune the parameters of each layer of the model by 5 different dimensions . The pruning is done by optimizing the design dimensions in an optimization procedure . The paper is well written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me how the proposed pruning method is different from the existing pruning methods such as DistilBERT ( Sanh et al. , 2019 ) and RoBERTa ( Liu et al .2019 ) .2 .The proposed method is not compared with other pruning techniques such as attention pruning . 3 .The experiments are only conducted on GLUE and SQuAD datasets . It would be better to conduct experiments on other NLP tasks such as CIFAR-10/100/MNIST/etc . 4 .In the experiments , the authors only compare with BERTLARGE and BERT with 3 encoder layers . It will be better if the authors can show the results of the proposed method with more layers .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper is technically sound . Cons : 1.1 . The novelty of the proposed method is limited . 2.2 .The experimental results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to solve the projective plane of order 10 problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solvers instead of special-purpose search code to perform the search . However , the paper does not provide a formal proof of the correctness of the proposed SAT solver . It would be better if the authors can provide the formal proof in the supplementary material . In addition , it is not clear to me how to verify the existence of the certificates . The authors should provide more details about the verification procedure .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) to represent different graph structures . The authors provide a theoretical framework for analyzing GNN ’ s expressive power and show that popular GNN variants , such as Graph Convolutional Networks ( GCN ) and GraphSAGE , can not learn to distinguish simple graph structures such as subtree structures . They then develop a simple architecture that is provably the most expressive among the class of GNN and is as powerful as the Weisfeiler-Lehman graph isomorphism test . The paper is well written and easy to follow . However , I have the following concerns about this paper . 1 .The authors claim that GIN is the most powerful GNN by their theory , but the experimental results do not support this claim . For example , in Table 1 , the performance of GCN and GSAGE is not significantly better than GIN . 2 .In Table 2 , the results of GIN and GCN are very close to each other . It is not clear why GIN performs better than GCN . It would be better if the authors can provide some explanation . 3 .In Section 3.2 , the authors claim GIN can not represent injective multiset functions . But in Section 4.1 , they show that the GIN does not satisfy the conditions in Theorem 3.1 and Theorem 4.2 . I am not sure if this is correct . 4 .In Figure 1 , it seems that GCN does not learn subtree structure . Can the authors provide more explanation about this ? 5 .In the experiments , it is better to provide more details about the architecture of the proposed architecture . For instance , how many layers are used in the proposed GIN ? 6 .The proposed architecture is very similar to Jumping Knowledge Networks ( Xu et al. , 2018 ) . Is it possible to show the performance difference between the two architectures ?
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation with knowledge-grounded responses . The authors propose a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . The paper is well-written and easy to follow . However , I have the following concerns . 1 .The authors claim that their model is the first neural model which incorporates the posterior distribution as guidance , enabling accurate knowledge lookups and high quality response generation . But , it is not clear to me how this is achieved . For example , the prior distribution p ( k|x ) is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution is inferred only from utterances only , so that appropriate knowledge can be selected even without responses during the inference process . Compared with the previous work , our model can better incorporate appropriate knowledge in response generation , which is better than previous work . 2 .In Table 1 , the authors compare the performance of their model with Seq2Seq model with a GRU decoder where knowledge is concatenated . It would be better if the authors can show the results of different ways of incorporating knowledge incorporating knowledge . 3 .It is better to show the effect of different loss functions in Table 1 .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT model . The main idea is to prune the architecture of the model by using pruning-based architecture search . The pruning is done by optimizing the design dimensions of the encoder layers . The authors show that the pruned model achieves 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoders . Pros : - The paper is well-written and easy to follow . - The idea of pruning the model is interesting . Cons : - It is not clear to me how the pruning can be applied to other objectives such as FLOPs or latency . - There is no comparison with other pruning methods such as DistilBERT ( Sanh et al. , 2019 ) and RoBERTa ( Liu et al .2019 ) .- The experiments are limited to a single task ( MNLI ) . It would be better if the authors can conduct more experiments on more downstream tasks such as speech recognition and language modeling .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper is technically sound . Cons : 1.1 . The novelty of the proposed method is limited . 2.2 .The experimental results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to solve the problem of finding a projective plane of order 10 from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solvers for the search of the projective planes , which is an interesting idea . However , I have the following concerns : 1 . The authors claim that the SAT solver is more error-prone than writing special-purpose search code , but it is not clear to me why this is the case . It seems to me that using well-tested solvers is less error prone than writing custom-written code . 2 .In the proof of Theorem 3.1 , it is claimed that the authors found consistency issues in both the original search and the independent verification in 2011 . But the authors did not provide any explanation for this . 3 .In Theorem 4.2 , the author claims that the certificates of the previous search and independent verification are consistent , but the proof is not provided . It would be better to provide the proof in the main text . 4 .The authors should provide more details about the algorithm . For example , how many blocks are used in the search ? What is the complexity of the algorithm ? How many blocks do the authors use in the verification ? 5 .The author should provide the running time of the search and verification .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation with knowledge-grounded responses . The authors propose a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . The paper is well-written and easy to follow . However , I have the following concerns . 1 .The authors claim that their model is the first neural model which incorporates the posterior distribution as guidance , enabling accurate knowledge lookups and high quality response generation . But , it is not clear to me how this is achieved . For example , the prior distribution p ( k|x ) is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution is inferred only from utterances only , so that appropriate knowledge can be selected even without responses during the inference process . Compared with the previous work , our model can better incorporate appropriate knowledge in response generation , which is better than previous work . 2 .In Table 1 , the authors compare the performance of their model with Seq2Seq model with a GRU decoder where knowledge is concatenated . It would be better if the authors can show the results of different ways of incorporating knowledge incorporating knowledge . 3 .It is better to show the effect of different loss functions in Table 1 .
1_2005.06628 = This paper proposes a pruning-based approach to reduce the number of parameters in the BERT language model . The main idea is to prune the architecture of the encoder and the attention head of the Transformer . The pruning is done by optimizing the architecture design dimensions of each layer of the model , and the pruned parameters are then used to train the model in a pre-trained fashion . The authors show that the proposed method can achieve 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the model is interesting . 3 .The proposed pruning method is simple and effective . Cons : 1.1 . It is not clear why the authors chose to optimize the design dimensions in the pruning framework rather than launching a pretraining job for each of these choices . 2.2 .The authors should compare the performance of the proposed pruned model with other pruning methods such as RoBERTa and DistilBERT . 3.3 .The experiments are only conducted on one language modeling task ( GLUE ) . It would be better if the authors can conduct more experiments on other language modeling tasks such as NLP tasks .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper is technically sound . Cons : 1.1 . The novelty of the proposed method is limited . 2.2 .The experimental results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation with knowledge-grounded responses . The authors propose a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . The paper is well-written and easy to follow . However , I have the following concerns . 1 .The authors claim that their model is the first neural model which incorporates the posterior distribution as guidance , enabling accurate knowledge lookups and high quality response generation . But , it is not clear to me how this is achieved . For example , the prior distribution p ( k|x ) is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution is inferred only from utterances only , so that appropriate knowledge can be selected even without responses during the inference process . Compared with the previous work , our model can better incorporate appropriate knowledge in response generation , which is better than previous work . 2 .In Table 1 , the authors compare the performance of their model with Seq2Seq model with a GRU decoder where knowledge is concatenated . It would be better if the authors can show the results of different ways of incorporating knowledge incorporating knowledge . 3 .It is better to show the effect of different loss functions in Table 1 .
1_2005.06628 = This paper proposes a pruning-based approach to reduce the number of parameters in the BERT language model . The main idea is to prune the architecture of the encoder and the attention head of the Transformer . The pruning is done by optimizing the architecture design dimensions of each layer of the model , and the pruned parameters are then used to train the model in a pre-trained fashion . The authors show that the proposed method can achieve 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the model is interesting . 3 .The proposed pruning method is simple and effective . Cons : 1.1 . It is not clear why the authors chose to optimize the design dimensions in the pruning framework rather than launching a pretraining job for each of these choices . 2.2 .The authors should compare the performance of the proposed pruned model with other pruning methods such as RoBERTa and DistilBERT . 3.3 .The experiments are only conducted on one language modeling task ( GLUE ) . It would be better if the authors can conduct more experiments on other language modeling tasks such as NLP tasks .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper is technically sound . Cons : 1.1 . The novelty of the proposed method is limited . 2.2 .The experimental results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem is polylogarithmic time solvable when the network is a path or a star . This paper answers this open problem positively by giving a polynomially-time algorithm . Then , the authors show that when the preferences are weak ( ties among objects are allowed ) , the problem becomes NP-hard in general graphs and can be solved in polinomial time in star . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The main ideas of the two algorithms are similar : first characterizing some structural properties of the problem and then reducing the problem to the 2-Sat problem . It would be better if the authors can give more details about the algorithm . 2 .In the proof of Theorem 2 , the algorithm is based on the main idea of the algorithm of the paper , which is delete all agents and objects on the right side of the initialowment of the agent and move them to the left or the middle of the endowment of agent n ' in the initialization . 3 .In Theorem 1 , it is not clear to me what is the difference between the algorithm and the algorithm proposed in [ 22 ] . 4 .Theorem 2 and Theorem 3 are similar to each other . It will be better to explain the difference . 5 .In Section 2.3.1 , there are some typos .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation with knowledge-grounded responses . The authors propose a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . The paper is well-written and easy to follow . However , I have the following concerns . 1 .The authors claim that their model is the first neural model which incorporates the posterior distribution as guidance , enabling accurate knowledge lookups and high quality response generation . But , it is not clear to me how this is achieved . For example , the prior distribution p ( k|x ) is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution is inferred only from utterances only , so that appropriate knowledge can be selected even without responses during the inference process . Compared with the previous work , our model can better incorporate appropriate knowledge in response generation , which is better than previous work . 2 .In Table 1 , the authors compare the performance of their model with Seq2Seq model with a GRU decoder where knowledge is concatenated . It would be better if the authors can show the results of different ways of incorporating knowledge incorporating knowledge . 3 .It is better to show the effect of different loss functions in Table 1 .
1_2005.06628 = This paper proposes a pruning-based approach to reduce the number of parameters in the BERT language model . The main idea is to prune the architecture of the encoder and the attention head of the Transformer . The pruning is done by optimizing the architecture design dimensions of each layer of the model , and the pruned parameters are then used to train the model in a pre-trained fashion . The authors show that the proposed method can achieve 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the model is interesting . 3 .The proposed pruning method is simple and effective . Cons : 1.1 . It is not clear why the authors chose to optimize the design dimensions in the pruning framework rather than launching a pretraining job for each of these choices . 2.2 .The authors should compare the performance of the proposed pruned model with other pruning methods such as RoBERTa and DistilBERT . 3.3 .The experiments are only conducted on one language modeling task ( GLUE ) . It would be better if the authors can conduct more experiments on other language modeling tasks such as NLP tasks .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensembling from multi-original models ; ( 3 ) the teacher network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are only conducted on ImageNet dataset . It would be better to conduct experiments on other datasets .
1_1901.06829 = This paper proposes a reinforcement learning based framework for the task of video grounding , which aims to temporally localize a natural language description in a video . To alleviate this problem , the authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset and Charades-STA dataset while observing only 10 or less clips per video . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The main contribution of this paper is the multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . 2 .It is not clear why the observation network takes visual and textual information as well as current temporal boundaries into consideration . 3 .The experiments are not convincing . It is better to conduct experiments on more datasets .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to solve the projective plane of order 10 problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to use SAT solvers instead of special-purpose search code to solve this problem . The authors claim that using well-tested SAT solver is less error-prone than writing custom-written code , but it is not clear to me why this is the case . 2 .In the proof of Theorem 2.1 , it is claimed that the certificates are consistent with the original search code . But in the proof , the author only shows that the certificate is consistent with existing search code , which is not true . 3 .The proof of Lemma 2.2 is not correct . The proof is not consistent with Lemma 1.1 . 4 .In Theorem 3.2 , the proof is based on the existing proof of the DRAT proof , but the author does not show that the proof does indeed show the unsatisfiability of the augmented SAT instance . 5 .The paper is not well organized . For example , there are many typos and grammatical errors in the paper .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) to capture different graph structures . The main contribution of the paper is to analyze the discriminative power of popular GNN variants , such as Graph Convolutional Networks and GraphSAGE , and show that they can not learn to distinguish certain simple graph structures , and develop a simple architecture that is provably the most expressive among the class of GNN and is as powerful as the Weisfeiler-Lehman graph isomorphism test . The paper is well-written and easy to follow . However , I have the following concerns : 1 . In the abstract , the authors claim that the most powerful GNN , i.e. , Graph Isomorphism Network ( GIN ) , also empirically has high representational power as it almost perfectly fits the training data , whereas the less powerful models often severely underfit the data . But , in the experiments , the GIN only outperforms the state-of-the-art results on a number of graph classification benchmarks , which is not convincing . 2 .In the experimental results , the performance of the proposed GIN is not better than the other GNN models . 3 .The experimental results are not convincing enough . In Table 1 , the results of GIN and GCN are not comparable . 4 .In Table 2 , the result of GCN and GIN are not compared .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low precision SGD iterates with a modified learning rate schedule . The authors show that it can match the performance of full-precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The paper is well-written and easy to follow . The proposed method is novel and interesting . However , I have the following concerns : 1 . In the experiments , the authors only compare with HALP . It would be better if the authors can also compare with other low precision methods such as QSGD and ZipML . 2 .The authors should compare with the state-of-the-art methods in terms of accuracy . For example , the accuracy of HALP can be asymptotically smaller than that of SGD in strongly convex setting . 3 .In the experiment , it is not clear how to choose the learning rate for SWA update . It is better to use the same learning rate as the full precision method . 4 .The experiments are not convincing . In Table 1 , the results of SWA and HALP are not comparable .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that employs a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed model is novel and well-motivated . 3 .The experiments are comprehensive and show that the proposed method can better incorporate knowledge in response generation . 4 .The paper is technically sound . Cons : 1 ) The proposed method is not compared with other related works . 2 ) It is not clear how the proposed approach can be applied to other dialogue generation tasks such as Persona-chatting and Wizard-of-Wikipedia . 3 ) The paper does not compare with other existing work on dialogue generation , e.g. , [ Zhou et al. , 2018 ] and [ Ghazvininejad et al .2018 ] .It would be better if the authors can compare with them .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT model . The main idea is to prune the parameters of each layer of the model by 5 different dimensions . The pruning method is based on pruning the architecture design dimensions and the pruning-based architecture search technique . Experiments on GLUE and SQuAD datasets show that the pruned model achieves 6.6 % higher average accuracy compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning different layers of BERT is interesting and novel . 3 .The experimental results are convincing . 4 .The pruning based architecture search method can be applied to other objectives such as FLOPs or latency . Cons : 1 ) It is not clear to me why the prunable parameters in Eq . ( 1 ) and ( 2 ) are pruned . 2 ) The pruned parameters are not different from the original BERT parameters . 3 ) The proposed method is not compared with other pruning methods such as DistilBERT and RoBERTa .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper is technically sound . Cons : 1.1 . The novelty of the proposed method is limited . 2.2 .The experimental results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to resolve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem , which was previously solved by Lam , Thiel , and Swiercz ( 1989 ) . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . The authors show that using well-tested SAT solvers is less error-prone than writing special-purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient . The proposed SAT encoding and the mathematical results that the authors rely on inside a formal proof system can be a possible avenue for constructing a formal proofs . In fact , results that were recently proven using SAT Solvers such as the resolution of the Boolean Pythagorean triples problem ( Heule , Kullmann , and Marek 2016 ) and a case of Erdo 's discrepancy conjecture ( Konev and Lisitsa 2014 ) have since had formal proofs generated based on the SAT encoding .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) to capture different graph structures . The main contribution of the paper is to analyze the discriminative power of popular GNN variants , such as Graph Convolutional Networks and GraphSAGE , and show that they can not learn to distinguish certain simple graph structures , and develop a simple architecture that is provably the most expressive among the class of GNN . The paper is well-written and easy to follow . However , I have the following concerns : 1 . In the abstract , the authors claim that the most powerful GNN by their theory , i.e. , Graph Isomorphism Network ( GIN ) , also empirically has high representational power as it almost perfectly fits the training data , whereas the less powerful Gnn variants often severely underfit the data . But , in the experiments , the GIN only achieves state-of-the-art results on a number of graph classification benchmarks , which is not convincing . 2 .In the experimental results , the performance of the proposed GIN model is not better than the state of the art models . 3 .The authors should compare the proposed model with other GNN models , e.g. , GCN ( Kipff & Welling 2017 ) and GIN ( Hamilton et al. , 2017 ) . 4 .In Section 5.2 , it is claimed that GIN is as powerful as the Weisfeiler-Lehman graph isomorphism test , but it is not clear why .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also provide theoretical analysis on the convergence of the proposed method and show that it converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than SGD in strongly convex settings . The paper is well-written and well-organized . However , I have the following concerns : 1 . In the experiments , it is not clear why the authors chose to use BFP instead of fixedpoint because BFP usually has less quantization error caused by overflow and underflow when quantizing DNN models ( Song et al. , 2017 ) . For deep learning experiments , BFP is preferred over fixedpoint as it has less computational overhead . 2 .In the experiment , the authors only compare the performance with 8-bit quantized SGD . It is better to compare with other methods such as HALP , QSGD , and ZipML . 3 .In Table 1 , the comparison with HALP is not fair because HALP uses a fixed-point quantization , while this paper only quantizes all the numbers except the gradients and velocity vectors .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that employs a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed model is novel and well-motivated . 3 .The experiments are comprehensive and show the effectiveness of the model . 4 .The paper is technically sound . Cons : 1 ) The proposed model seems to be a straightforward combination of two existing models . It would be better if the authors can provide a more detailed analysis of the differences between the two models . 2 ) It is not clear why the authors use the KLDivLoss to measure the proximity between the prior distribution and the posterior distribution . 3 ) The authors should provide more details about the model architecture . 4 ) The experiments are only conducted on the Wizard-of-Wikipedia dataset . It is better to conduct experiments on other datasets .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT language model . The main idea is to prune the parameters of each layer of BERT by 5 different dimensions . The pruning is done by optimizing the design dimensions of the encoder and decoding layers . The authors show that the pruned model achieves 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number of parameter . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the model parameters is interesting . 3 .The experiments are well designed . 4 .The pruning method is simple and effective . 5 .The paper is easy to read . Cons : 1.1 . The novelty of this paper is limited . The proposed pruning technique is not novel . The idea of using pruning to reduce parameters is not new . 2.2 .The experimental results are not convincing . 3.3 .The proposed method is not compared with other pruning methods such as RoBERTa . 4.4 .The results of the proposed method are not significant .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensembling from multi-original models ; ( 3 ) the teacher network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are only conducted on ImageNet dataset . It would be better to conduct experiments on other datasets .
1_1901.06829 = This paper proposes a reinforcement learning based framework for the task of video grounding , which aims to temporally localize a natural language description in a video . To alleviate this problem , the authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset and Charades-STA dataset while observing only 10 or less clips per video . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The main contribution of this paper is the multi-task learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 2 .The experimental results are not convincing . The performance of this method is not better than the state of the art methods .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation with knowledge-grounded responses . The authors propose a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . The paper is well-written and easy to follow . However , I have the following concerns . 1 .The authors claim that their model is the first neural model which incorporates the posterior distribution as guidance , enabling accurate knowledge lookups and high quality response generation . But , it is not clear to me how this is achieved . For example , the prior distribution p ( k|x ) is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution is inferred only from utterances only , so that appropriate knowledge can be selected even without responses during the inference process . Compared with the previous work , our model can better incorporate appropriate knowledge in response generation , which is better than previous work . 2 .In Table 1 , the authors compare the performance of their model with Seq2Seq model with a GRU decoder where knowledge is concatenated . It would be better if the authors can show the results of different ways of incorporating knowledge incorporating knowledge . 3 .It is better to show the effect of different loss functions in Table 1 .
1_2005.06628 = This paper proposes a method to reduce the number of parameters in the BERT model . The main idea is to use a smaller number of layers in the encoder of BERT . The authors show that this can be achieved by reducing the architecture design dimensions rather than reducing the Transformer encoder layers . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that the proposed method can be applied towards other objectives such as FLOPs or latency . But the authors do not provide any experimental results to support this claim . 2 .The experimental results are not convincing . The proposed method is only compared with BERT with three layers . It is not clear whether this method can also be applied to other models such as Transformer-based models . 3 .In Table 1 , the authors only show the results for BERT without three layers , which is not enough to show the effectiveness of this method .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper is technically sound . Cons : 1.1 . The novelty of the proposed method is limited . 2.2 .The experimental results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to resolve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solvers instead of special-purpose search code to solve the projective plane of order 10 problem . However , I have some concerns about the correctness of the proposed method . First of all , it is not clear to me why the proposed SAT solver is more error-prone than the one used in the original search by Lam , Crossfield , and Thiel 1985 . The authors should compare the error rates of the two methods . Second , the paper does not provide any theoretical analysis about the proposed approach . It would be better if the authors can provide some theoretical analysis .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) to capture different graph structures . The main contribution of the paper is to analyze the discriminative power of popular GNN variants , such as Graph Convolutional Networks and GraphSAGE , and show that they can not learn to distinguish certain simple graph structures , and develop a simple architecture that is provably the most expressive among the class of GNN . The paper is well-written and easy to follow . However , I have the following concerns : 1 . In the abstract , the authors claim that the most powerful GNN by their theory , i.e. , Graph Isomorphism Network ( GIN ) , also empirically has high representational power as it almost perfectly fits the training data , whereas the less powerful Gnn variants often severely underfit the data . But , in the experiments , the GIN only achieves state-of-the-art results on a number of graph classification benchmarks , which is not convincing . 2 .In the experimental results , the performance of the proposed GIN model is not better than the state of the art models . 3 .The authors should compare the proposed model with other GNN models , e.g. , GCN ( Kipff & Welling 2017 ) and GIN ( Hamilton et al. , 2017 ) . 4 .In Section 5.2 , it is claimed that GIN is as powerful as the Weisfeiler-Lehman graph isomorphism test , but it is not clear why .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives , and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation with knowledge-grounded responses . The authors propose a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . The paper is well-written and easy to follow . However , I have the following concerns . 1 .The authors claim that their model is the first neural model which incorporates the posterior distribution as guidance , enabling accurate knowledge lookups and high quality response generation . But , it is not clear to me how this is achieved . For example , the prior distribution p ( k|x ) is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution is inferred only from utterances only , so that appropriate knowledge can be selected even without responses during the inference process . Compared with the previous work , our model can better incorporate appropriate knowledge in response generation , which is better than previous work . 2 .In Table 1 , the authors compare the performance of their model with Seq2Seq model with a GRU decoder where knowledge is concatenated . It would be better if the authors can show the results of different ways of incorporating knowledge incorporating knowledge . 3 .It is better to show the effect of different loss functions in Table 1 .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT model . The main idea is to prune the parameters of each layer of the model by 5 different dimensions . The pruning method is based on pruning the architecture design dimensions and the pruning-based architecture search technique . Experiments on GLUE and SQuAD datasets show that the pruned model achieves 6.6 % higher average accuracy compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning different layers of BERT is interesting and novel . 3 .The experimental results are convincing . 4 .The pruning based architecture search method can be applied to other objectives such as FLOPs or latency . Cons : 1 ) It is not clear to me why the prunable parameters in Eq . ( 1 ) and ( 2 ) are pruned . 2 ) The pruned parameters are not different from the original BERT parameters . 3 ) The proposed method is not compared with other pruning methods such as DistilBERT and RoBERTa .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a reinforcement learning based framework for video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed method is novel in that it successfully combines supervised learning with reinforcement learning in a multi-task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The experiments show that the proposed method outperforms the baselines . Cons : 1.1 . The novelty of this paper is limited . The main contribution of the paper is to formulate the task of video grounding as a sequential decision-making problem . However , it is not clear to me how this problem can be solved by the proposed approach . 2.2 .The experimental results are not convincing . The performance of the proposed model is not better than the state of the art methods . 3.3 .There are some typos in the paper .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to solve the projective plane of order 10 problem by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to use SAT solvers instead of special-purpose search code to solve projective geometry problem . It would be better if the authors can provide more details on how the SAT solver is used in this paper . For example , how many blocks are used in the search ? 2 .The authors claim that the proposed method is less error-prone than writing special purpose search code , but it is not clear to me how to verify the correctness of this claim . 3 .In the proof of Theorem 3.1 , it is claimed that the proof is based on the SAT encoding , but there is no proof in the paper . It is better to provide the proof in a more formal way .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation with knowledge-grounded responses . The authors propose a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . The paper is well-written and easy to follow . However , I have the following concerns . 1 .The authors claim that their model is the first neural model which incorporates the posterior distribution as guidance , enabling accurate knowledge lookups and high quality response generation . But , it is not clear to me how this is achieved . For example , the prior distribution p ( k|x ) is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution is inferred only from utterances only , so that appropriate knowledge can be selected even without responses during the inference process . Compared with the previous work , our model can better incorporate appropriate knowledge in response generation , which is better than previous work . 2 .In Table 1 , the authors compare the performance of their model with Seq2Seq model with a GRU decoder where knowledge is concatenated . It would be better if the authors can show the results of different ways of incorporating knowledge incorporating knowledge . 3 .It is better to show the effect of different loss functions in Table 1 .
1_2005.06628 = This paper proposes a pruning-based approach to reduce the number of parameters in the BERT language model . The main idea is to prune the architecture of the encoder and the attention head of the Transformer . The pruning is done by optimizing the architecture design dimensions of each layer of the model , and the pruned parameters are then used to train the model in a pre-trained fashion . The authors show that the proposed method can achieve 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the model is interesting . 3 .The proposed pruning method is simple and effective . Cons : 1.1 . It is not clear why the authors chose to optimize the design dimensions in the pruning framework rather than launching a pretraining job for each of these choices . 2.2 .The authors should compare the performance of the proposed pruned model with other pruning methods such as RoBERTa and DistilBERT . 3.3 .The experiments are only conducted on one language modeling task ( GLUE ) . It would be better if the authors can conduct more experiments on other language modeling tasks such as NLP tasks .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper is technically sound . Cons : 1.1 . The novelty of the proposed method is limited . 2.2 .The experimental results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .