\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{QSGD}
Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M.
\newblock {QSGD}: Communication-efficient {SGD} via gradient quantization and
  encoding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1707--1718, 2017.

\bibitem[Aojun~Zhou(2017)]{incremental-quant}
Aojun~Zhou, Anbang~Yao, Y. G. L. X. Y.~C.
\newblock Incremental network quantization: Towards lossless cnns with
  low-precision weights.
\newblock In \emph{International Conference on Learning
  Representations,ICLR2017}, 2017.

\bibitem[Banner et~al.(2018)Banner, Hubara, Hoffer, and
  Soudry]{scalable8bittraining}
Banner, R., Hubara, I., Hoffer, E., and Soudry, D.
\newblock Scalable methods for 8-bit training of neural networks.
\newblock \emph{arXiv preprint arXiv:1805.11046}, 2018.

\bibitem[Burger(2017)]{projectbrainwave}
Burger, D.
\newblock Microsoft unveils {Project Brainwave} for real-time {AI}.
\newblock
  \url{https://www.microsoft.com/en-us/research/blog/microsoft-unveils-project-brainwave/},
  2017.
\newblock Accessed: 2018-02-08.

\bibitem[Courbariaux et~al.(2014)Courbariaux, Bengio, and
  David]{low-precision-multiply}
Courbariaux, M., Bengio, Y., and David, J.-P.
\newblock Training deep neural networks with low precision multiplications.
\newblock \emph{arXiv preprint arXiv:1412.7024}, 2014.

\bibitem[Courbariaux et~al.(2015)Courbariaux, Bengio, and David]{binaryconnect}
Courbariaux, M., Bengio, Y., and David, J.-P.
\newblock Binaryconnect: Training deep neural networks with binary weights
  during propagations.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3123--3131, 2015.

\bibitem[Das et~al.(2018)Das, Mellempudi, Mudigere, Kalamkar, Avancha,
  Banerjee, Sridharan, Vaidyanathan, Kaul, Georganas, Heinecke, Dubey, Corbal,
  Shustrov, Dubtsov, Fomenko, and Pirogov]{Mixed-precision-training}
Das, D., Mellempudi, N., Mudigere, D., Kalamkar, D.~D., Avancha, S., Banerjee,
  K., Sridharan, S., Vaidyanathan, K., Kaul, B., Georganas, E., Heinecke, A.,
  Dubey, P., Corbal, J., Shustrov, N., Dubtsov, R., Fomenko, E., and Pirogov,
  V.~O.
\newblock Mixed precision training of convolutional neural networks using
  integer operations.
\newblock \emph{ICLR}, 2018.

\bibitem[De~Sa et~al.(2018)De~Sa, Leszczynski, Zhang, Marzoev, Aberger,
  Olukotun, and R{\'{e}}]{HALP}
De~Sa, C., Leszczynski, M., Zhang, J., Marzoev, A., Aberger, C.~R., Olukotun,
  K., and R{\'{e}}, C.
\newblock High-accuracy low-precision training.
\newblock \emph{CoRR}, abs/1803.03383, 2018.
\newblock URL \url{http://arxiv.org/abs/1803.03383}.

\bibitem[Gupta et~al.(2015)Gupta, Agrawal, Gopalakrishnan, and
  Narayanan]{gupta2015deep}
Gupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan, P.
\newblock Deep learning with limited numerical precision.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1737--1746, 2015.

\bibitem[Han et~al.(2015)Han, Mao, and Dally]{deepCompression}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural network with pruning,
  trained quantization and huffman coding.
\newblock \emph{CoRR}, abs/1510.00149, 2015.
\newblock URL \url{http://arxiv.org/abs/1510.00149}.

\bibitem[He et~al.(2015{\natexlab{a}})He, Zhang, Ren, and Sun]{he-init}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  1026--1034, 2015{\natexlab{a}}.

\bibitem[He et~al.(2015{\natexlab{b}})He, Zhang, Ren, and Sun]{resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock \emph{CoRR}, abs/1512.03385, 2015{\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/1512.03385}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{preact-resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{European Conference on Computer Vision}, pp.\  630--645.
  Springer, 2016.

\bibitem[Hubara et~al.(2016)Hubara, Courbariaux, Soudry, El-Yaniv, and
  Bengio]{binary-net}
Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y.
\newblock Binarized neural networks.
\newblock In Lee, D.~D., Sugiyama, M., Luxburg, U.~V., Guyon, I., and Garnett,
  R. (eds.), \emph{Advances in Neural Information Processing Systems 29}, pp.\
  4107--4115. 2016.

\bibitem[Izmailov et~al.(2018{\natexlab{a}})Izmailov, Podoprikhin, Garipov,
  Vetrov, and Wilson]{SWA}
Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., and Wilson, A.~G.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock 2018{\natexlab{a}}.

\bibitem[Izmailov et~al.(2018{\natexlab{b}})Izmailov, Podoprikhin, Garipov,
  Vetrov, and Wilson]{SWA-repo}
Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., and Wilson, A.~G.
\newblock Averaging weights leads to wider optima and better generalization --
  released code.
\newblock 2018{\natexlab{b}}.
\newblock URL \url{https://github.com/timgaripov/swa}.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{SVRG}
Johnson, R. and Zhang, T.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  315--323, 2013.

\bibitem[Jouppi et~al.(2017)Jouppi, Young, Patil, Patterson, Agrawal, Bajwa,
  Bates, Bhatia, Boden, Borchers, et~al.]{jouppi2017datacenter}
Jouppi, N.~P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R.,
  Bates, S., Bhatia, S., Boden, N., Borchers, A., et~al.
\newblock In-datacenter performance analysis of a tensor processing unit.
\newblock In \emph{Proceedings of the 44th Annual International Symposium on
  Computer Architecture}, pp.\  1--12. ACM, 2017.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{large-batch}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{CoRR}, abs/1609.04836, 2016.
\newblock URL \url{http://arxiv.org/abs/1609.04836}.

\bibitem[K{\"o}ster et~al.(2017)K{\"o}ster, Webb, Wang, Nassar, Bansal,
  Constable, Elibol, Gray, Hall, Hornof, et~al.]{flexpoint}
K{\"o}ster, U., Webb, T., Wang, X., Nassar, M., Bansal, A.~K., Constable, W.,
  Elibol, O., Gray, S., Hall, S., Hornof, L., et~al.
\newblock Flexpoint: An adaptive numerical format for efficient training of
  deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1742--1752, 2017.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and Hinton]{CIFAR10}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{MNIST}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Li et~al.(2017)Li, De, Xu, Studer, Samet, and
  Goldstein]{training-quantized-network-deeper-understanding}
Li, H., De, S., Xu, Z., Studer, C., Samet, H., and Goldstein, T.
\newblock Training quantized nets: A deeper understanding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5813--5823, 2017.

\bibitem[Mellempudi et~al.(2017)Mellempudi, Kundu, Das, Mudigere, and
  Kaul]{dyanmic-fixed-point}
Mellempudi, N., Kundu, A., Das, D., Mudigere, D., and Kaul, B.
\newblock Mixed low-precision deep learning inference using dynamic fixed
  point.
\newblock \emph{arXiv preprint arXiv:1701.08978}, 2017.

\bibitem[Miyashita et~al.(2016)Miyashita, Lee, and Murmann]{log-quant}
Miyashita, D., Lee, E.~H., and Murmann, B.
\newblock Convolutional neural networks using logarithmic data representation.
\newblock \emph{arXiv preprint arXiv:1603.01025}, 2016.

\bibitem[Moulines \& Bach(2011)Moulines and Bach]{francisbach}
Moulines, E. and Bach, F.~R.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  451--459, 2011.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{PyTorch}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in {PyTorch}.
\newblock 2017.

\bibitem[Rastegari et~al.(2016)Rastegari, Ordonez, Redmon, and
  Farhadi]{xor-net}
Rastegari, M., Ordonez, V., Redmon, J., and Farhadi, A.
\newblock {XNOR-Net}: Imagenet classification using binary convolutional neural
  networks.
\newblock In \emph{European Conference on Computer Vision}, pp.\  525--542.
  Springer, 2016.

\bibitem[Russakovsky et~al.(2014)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Li]{imagenet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M.~S., Berg, A.~C., and Li, F.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{CoRR}, abs/1409.0575, 2014.
\newblock URL \url{http://arxiv.org/abs/1409.0575}.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{VGG}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{CoRR}, abs/1409.1556, 2014.

\bibitem[Song et~al.(2017)Song, Liu, Wang, and Wang]{error-analysis}
Song, Z., Liu, Z., Wang, C., and Wang, D.
\newblock Computation error analysis of block floating point arithmetic
  oriented convolution neural network accelerator design.
\newblock \emph{arXiv preprint arXiv:1709.07776}, 2017.

\bibitem[Wang et~al.(2018)Wang, Choi, Brand, Chen, and
  Gopalakrishnan]{8bitfloat}
Wang, N., Choi, J., Brand, D., Chen, C.-Y., and Gopalakrishnan, K.
\newblock Training deep neural networks with 8-bit floating point numbers.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  7686--7695, 2018.

\bibitem[Wu et~al.(2018)Wu, Li, Chen, and Shi]{WAGE}
Wu, S., Li, G., Chen, F., and Shi, L.
\newblock Training and inference with integers in deep neural networks.
\newblock \emph{ICLR}, 2018.

\bibitem[Zhang et~al.(2017)Zhang, Li, Kara, Alistarh, Liu, and Zhang]{ZipML}
Zhang, H., Li, J., Kara, K., Alistarh, D., Liu, J., and Zhang, C.
\newblock {Z}ip{ML}: Training linear models with end-to-end low precision, and
  a little bit of deep learning.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pp.\  4035--4043, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\bibitem[Zhou et~al.(2016)Zhou, Wu, Ni, Zhou, Wen, and Zou]{dorefa-net}
Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., and Zou, Y.
\newblock {DoReFa-Net}: Training low bitwidth convolutional neural networks
  with low bitwidth gradients.
\newblock \emph{arXiv preprint arXiv:1606.06160}, 2016.

\bibitem[Zhu et~al.(2016)Zhu, Han, Mao, and Dally]{ternary-quant}
Zhu, C., Han, S., Mao, H., and Dally, W.~J.
\newblock Trained ternary quantization.
\newblock \emph{arXiv preprint arXiv:1612.01064}, 2016.

\end{thebibliography}
