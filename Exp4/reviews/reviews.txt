1_2012.04715 = In this paper , the authors study the problem of finding a projective plane of order 10 from projective geometry . The authors translate the problem into Boolean logic and use satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to use SAT solvers instead of special-purpose search code to solve the problem . The reason is that using well-tested SAT solver is less error-prone than writing special purpose search code . But it is not clear to me how to verify the correctness of the proposed method . 2 .The authors should provide more details about the algorithm . For example , what is the complexity of the algorithm ? 3 .How to choose the number of blocks in each instance ? 4 .What is the time complexity of solving the problem ? 5 .The paper is not well organized . There are many typos and grammatical errors .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The paper is well written and easy to follow . The proposed method is novel and interesting . However , I have the following concerns : 1 . In the experiments , it is not clear how to choose the number of quantization bits in the 8-bit case . It is better to use the same number as the full precision SGD . 2 .In Table 1 and Table 2 , the performance of SWA and HALP are very close to each other . It would be better to show the performance gap between SWA/HALP and full precision training . 3 .In the experiment , it would be more convincing if the authors can compare the performance with other quantization methods such as QSGD and ZipML .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation with knowledge-grounded responses . The authors propose a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . The paper is well-written and easy to follow . However , I have the following concerns . 1 .The authors claim that their model is the first neural model which incorporates the posterior distribution as guidance , enabling accurate knowledge lookups and high quality response generation . But , it is not clear to me how this is achieved . For example , the prior distribution p ( k|x ) is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution is inferred only from utterances only , so that appropriate knowledge can be selected even without responses during the inference process . Compared with the previous work , our model can better incorporate appropriate knowledge in response generation , which is better than previous work . 2 .In Table 1 , the authors compare the performance of their model with Seq2Seq model with a GRU decoder where knowledge is concatenated . It would be better if the authors can show the results of different ways of incorporating knowledge incorporating knowledge . 3 .It is better to show the effect of different loss functions in Table 1 .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT model . The main idea is to prune the parameters of each layer of the model by 5 different dimensions , and then pre-train multiple variants of BERT with different values chosen for these dimensions by applying pruning-based architecture search technique . Experiments on GLUE and SQuAD datasets show that the proposed method achieves 6.6 % higher average accuracy compared to BERT without three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the model parameters is interesting . 3 .The proposed method is simple and effective . 4 .The experimental results are convincing . Cons : 1 ) It is not clear to me why the proposed pruning method is better than other pruning methods such as RoBERTa ( Liu et al. , 2019 ) and DistilBERT ( Sanh et al .2019 ) . 2 ) The proposed method can be applied to other objectives such as FLOPs or latency . 3 ) The experiments are limited to GLUE dataset .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed method achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed method is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors tackle the problem of finding a projective plane of order 10 from projective geometry . The authors translate the problem into Boolean logic and use satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of the paper is to use SAT solvers instead of special-purpose search code to solve the problem . The reason is that using well-tested SAT solver is less error-prone than writing custom-written code . But the paper does not provide a formal proof of the correctness of the proposed SAT encoding . 2 .The authors claim that the proposed method can be used to solve large satisfiability instances when the cubing solver can generate many cubes encoding subproblems of approximately equal difficulty . But it is not clear to me how the authors can use the SAT encoding to solve such large instances . 3 .The paper is not well organized . For example , it is hard to follow the description of the algorithm . 4 .It is unclear to me what is the purpose of using the SAT encoder in the paper . It seems that the authors only use it to solve a small satisfiability instance .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that employs a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed model is novel and well-motivated . 3 .The experiments are comprehensive and thorough . 4 .The paper is easy to read and follow . Cons : 1.1 . It is not clear to me why the prior distribution over knowledge is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . 2.2 .The prior distribution p ( k|x , y ) can be effectively utilized to sample appropriate knowledge so as to generate proper responses . 3.3 .It is unclear to me what is the role of the Kullback-Leibler divergence loss ( KLDivLoss ) to measure the proximity between the prior and the posterior distribution . 4.4 .It would be better if the authors can provide more details about the model architecture and training procedure . 5 .It will be better to provide more analysis about the performance of the model .
1_2005.06628 = This paper proposes a pruning-based approach to reduce the number of parameters in the BERT language model . The main idea is to prune the architecture of the encoder and the attention head of the Transformer . The pruning is done by optimizing the architecture design dimensions of each layer of the model , and the pruned parameters are then used to train the model in a pre-trained fashion . The authors show that the proposed method can achieve 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the model is interesting . 3 .The proposed pruning method is simple and effective . Cons : 1.1 . It is not clear why the authors chose to optimize the design dimensions in the pruning framework rather than launching a pretraining job for each of these choices . 2.2 .The authors should compare the performance of the proposed pruned model with other pruning methods such as RoBERTa and DistilBERT . 3.3 .The experiments are only conducted on one language modeling task ( GLUE ) . It would be better if the authors can conduct more experiments on other language modeling tasks such as NLP tasks .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a reinforcement learning based framework for the task of video grounding , which aims to temporally localize a natural language description in a video . To alleviate this problem , the authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . The paper is well written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The main contribution of this paper is the multi-task learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 2 .The experimental results are not convincing . The performance of this method is not better than the state of the art methods .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman ( WL ) graph isomorphism test . In particular , the authors show that the most powerful GNN , i.e. , Graph Isomorphism Network ( GIN ) , also empirically has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The authors then develop a simple architecture that is provably the most expressive among the class of GNN and is as powerful as the WL test . The paper is well-written and easy to follow . However , I have the following concerns . 1 .The authors claim that GNN can not learn to distinguish simple graph structures , but they do not provide any theoretical or experimental evidence to support this claim . 2 .Theorem 3.1 shows that GIN can not represent injective multiset functions , but it is not clear how the authors can prove it . 3 .In Section 5.2.1 , it is claimed that the GIN is able to learn to classify simple graphs , but the authors do not give any experimental results to support the claim . 4 .The experimental results are not convincing . In Table 1 , the performance of the proposed method is not better than the baseline methods .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that uses both prior and posterior distributions over knowledge to facilitate knowledge selection . The paper is well-written and easy to follow . The idea of using the posterior distribution over knowledge is interesting and novel . However , I have the following concerns : 1 . In the prior knowledge module , the prior distribution p ( k|x ) is inferred from both the utterance and the response , while the posterior p ( y|x , y ) is only inferred from the responses . It is not clear to me how the prior p ( x ) can be used to select appropriate knowledge . 2 .In the inference process , the authors use the Kullback-Leibler divergence loss ( KLDivLoss ) to measure the distance between the prior and the posterior distributions . It seems that the authors do not use the KL divergence between the posterior and the prior . 3 .The authors should compare the performance of the proposed model with the baselines on the Persona dataset and Wizard-of-Wikipedia dataset . 4 .The proposed model is not compared with the baseline methods in the literature .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT model . The main idea is to prune the parameters of each layer of the model by 5 different dimensions . The pruning method is based on pruning the architecture design dimensions and the pruning-based architecture search technique . Experiments on GLUE and SQuAD datasets show that the pruned model achieves 6.6 % higher average accuracy compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning different layers of BERT is interesting and novel . 3 .The experimental results are convincing . 4 .The pruning based architecture search method can be applied to other objectives such as FLOPs or latency . Cons : 1 ) It is not clear to me why the prunable parameters in Eq . ( 1 ) and ( 2 ) are pruned . 2 ) The pruned parameters are not different from the original BERT parameters . 3 ) The proposed method is not compared with other pruning methods such as DistilBERT and RoBERTa .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper is technically sound . Cons : 1.1 . The novelty of the proposed method is limited . 2.2 .The experimental results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that employs a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed model is novel and well-motivated . 3 .The experiments are comprehensive and show the effectiveness of the model . 4 .The paper is technically sound . Cons : 1 ) The proposed model seems to be a straightforward combination of two existing models . It would be better if the authors can provide a more detailed analysis of the differences between the two models . 2 ) It is not clear why the authors use the KLDivLoss to measure the proximity between the prior distribution and the posterior distribution . 3 ) The authors should provide more details about the model architecture . 4 ) The experiments are only conducted on the Wizard-of-Wikipedia dataset . It is better to conduct experiments on other datasets .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT model . The main idea is to prune the parameters of each layer of the model by 5 different dimensions . The pruning method is based on pruning the architecture design dimensions and the pruning-based architecture search technique . Experiments on GLUE and SQuAD datasets show that the pruned model achieves 6.6 % higher average accuracy compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning different layers of BERT is interesting and novel . 3 .The experimental results are convincing . 4 .The pruning based architecture search method can be applied to other objectives such as FLOPs or latency . Cons : 1 ) It is not clear to me why the prunable parameters in Eq . ( 1 ) and ( 2 ) are pruned . 2 ) The pruned parameters are not different from the original BERT parameters . 3 ) The proposed method is not compared with other pruning methods such as DistilBERT and RoBERTa .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experimental results are not convincing . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a method for video grounding , which aims to temporally localize a natural language description in a video . The authors propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed method achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed method is simple and effective . 3 .The experimental results show that the proposed method outperforms the baselines . Cons : 1.1 . The novelty of this paper is limited . The main contribution of the paper is the reinforcement learning framework , which is based on the actor-critic algorithm ( Sutton and Barto 2011 ) . However , it is not clear whether the proposed framework can be applied to other video grounding tasks , e.g .sentence-based video retrieval methods . 2.2 .The novelty of the method is limited , as the proposed approach is not novel . 3.3 .The performance gain is not significant . 4 .The experiments are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem is polylogarithmic time solvable when the network is a path or a star . The paper shows that when the preferences are weak ( ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem can be solved in polynomially time when the networks is path or star . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of the proposed algorithm is to delete all agents and objects on the right of agent n ' in the initial endowment , and there is no guarantee that each object o_i will be moved to either the left or the right side of its original position in a reachable assignment ( i.e. , the most possible position for the left and one possible side for the right ) . 2 .In the proof of Theorem 2 , it is not clear to me how to prove that two relations oa i ob and ob i oa together imply that oa and ob are equivalent for agent i , denoted by ob =i oa . 3 .In Theorem 1 , it seems that the number of feasible swaps in an instance may increase . This may increase the searching space of the problem dramatically and make the problem harder to search for a solution .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than SGD in strongly convex settings . The paper is well-written and well-organized . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in Table 2 . 2 .In Table 1 , it would be more convincing if SWA is compared with HALP in terms of accuracy . 3 .In the experiment , the authors should compare with other quantization methods such as QSGD and ZipML . 4 .In Section 4.2 , it will be better to show the convergence rate of SWA with respect to the number of iterations of SGD .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation with knowledge-grounded responses . The authors propose a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . The paper is well-written and easy to follow . However , I have the following concerns . 1 .The authors claim that their model is the first neural model which incorporates the posterior distribution as guidance , enabling accurate knowledge lookups and high quality response generation . But , it is not clear to me how this is achieved . For example , the prior distribution p ( k|x ) is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution is inferred only from utterances only , so that appropriate knowledge can be selected even without responses during the inference process . Compared with the previous work , our model can better incorporate appropriate knowledge in response generation , which is better than previous work . 2 .In Table 1 , the authors compare the performance of their model with Seq2Seq model with a GRU decoder where knowledge is concatenated . It would be better if the authors can show the results of different ways of incorporating knowledge incorporating knowledge . 3 .It is better to show the effect of different loss functions in Table 1 .
1_2005.06628 = This paper proposes a pruning-based approach to reduce the number of parameters in the BERT language model . The main idea is to prune the architecture of the encoder and the attention head of the Transformer . The pruning is done by optimizing the architecture design dimensions of each layer of the model , and the pruned parameters are then used to train the model in a pre-trained fashion . The authors show that the proposed method can achieve 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the model is interesting . 3 .The proposed pruning method is simple and effective . Cons : 1.1 . It is not clear why the authors chose to optimize the design dimensions in the pruning framework rather than launching a pretraining job for each of these choices . 2.2 .The authors should compare the performance of the proposed pruned model with other pruning methods such as RoBERTa and DistilBERT . 3.3 .The experiments are only conducted on one language modeling task ( GLUE ) . It would be better if the authors can conduct more experiments on other language modeling tasks such as NLP tasks .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experimental results are not convincing . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a reinforcement learning based framework for the task of video grounding , which aims to temporally localize a natural language description in a video . To alleviate this problem , the authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . The proposed method achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset and Charades-STA dataset while observing only 10 or less clips per video . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The main contribution of this paper is the multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . 2 .The experiments are not convincing . The performance of this method is not better than the baselines . For example , the results in Table 1 and Table 2 are not comparable . 3 .In Table 1 , the performance of the baseline method is lower than the proposed one . It is not clear why this is the case .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem is polylogarithmic time solvable when the network is a path or a star . The paper shows that when the preferences are weak ( ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors give a polynomially time algorithm for solving the 2-Sat problem . Overall , the paper is well written and easy to follow . However , I have the following concerns : 1 . The main idea of the algorithm is to check whether an agent has access to all the objects in the initial endowment of the agent , and then delete all objects on the right of agent n and delete all agents on the left of agent 's right . This is the main idea behind the algorithm , but it is not clear to me how the algorithm works in practice . 2 .In the proof of Theorem 2 , it is unclear to me what is the difference between the algorithm in the paper and the one in [ 22 ] . 3 .In Theorem 1 , it seems to me that there is a typo in the definition of O ( o_i^ { i-1 } ) . 4 .Theorem 2 and Theorem 3 seem to be the same , but the proof is different .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that employs a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed model is novel and well-motivated . 3 .The experiments are comprehensive and show the effectiveness of the model . 4 .The paper is technically sound . Cons : 1 ) The proposed model seems to be a straightforward combination of two existing models . It would be better if the authors can provide a more detailed analysis of the differences between the two models . 2 ) It is not clear why the authors use the KLDivLoss to measure the proximity between the prior distribution and the posterior distribution . 3 ) The authors should provide more details about the model architecture . 4 ) The experiments are only conducted on the Wizard-of-Wikipedia dataset . It is better to conduct experiments on other datasets .
1_2005.06628 = This paper proposes a pruning-based approach to reduce the number of parameters in the BERT language model . The main idea is to prune the architecture of the encoder and the attention head of the Transformer . The pruning is done by optimizing the architecture design dimensions of each layer of the model , and the pruned parameters are then used to train the model in a pre-trained fashion . The authors show that the proposed method can achieve 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the model is interesting . 3 .The proposed pruning method is simple and effective . Cons : 1.1 . It is not clear why the authors chose to optimize the design dimensions in the pruning framework rather than launching a pretraining job for each of these choices . 2.2 .The authors should compare the performance of the proposed pruned model with other pruning methods such as RoBERTa and DistilBERT . 3.3 .The experiments are only conducted on one language modeling task ( GLUE ) . It would be better if the authors can conduct more experiments on other language modeling tasks such as NLP tasks .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensembling from multi-original models ; ( 3 ) the teacher network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are only conducted on ImageNet dataset . It would be better to conduct experiments on other datasets .
1_1901.06829 = This paper proposes a reinforcement learning based framework for the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well written and easy to follow . 2 .The proposed method is novel in that it successfully combines supervised learning with reinforcement learning in a multi-task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper shows steady performance gains by considering additional supervised boundary information during training . 4 .The experimental results show that the proposed method outperforms the baselines . Cons : 1.1 . The novelty of this paper is limited . The main contribution of this work is to formulate the video grounding as a sequential decision-making problem . However , it is not clear to me how this problem can be solved by the proposed approach . 2.2 .The novelty of the proposed model is limited to the observation network and the policy . 3.3 .The experiment results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem is polylogarithmic time solvable when the network is a path or a star . The paper shows that when the preferences are weak ( ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem can be solved in polynomially time when the networks is path or star . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of the algorithm is to delete all agents and objects on the right of the agent n ’ in the initial endowment , and there is no guarantee that the algorithm can find the agent with the most possible position . 2 .In the proof of Theorem 2 , it is not clear to me how to prove that each object o_i will be moved to either the left or the right side of its original position in a reachable assignment ( i.e. , there is only one possible position for the left and one for the right ) . 3 .In Theorem 1 , the algorithm requires that every trade is performed between neighbors in social network G. 4 . In Theorem 3 , the number of feasible swaps in an instance may increase .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) to capture different graph structures . The main contribution of the paper is to analyze the discriminative power of popular GNN variants , such as Graph Convolutional Networks and GraphSAGE , and show that they can not learn to distinguish certain simple graph structures , and develop a simple architecture that is provably the most expressive among the class of GNN . The paper is well-written and easy to follow . However , I have the following concerns : 1 . In the abstract , the authors claim that the most powerful GNN by their theory , i.e. , Graph Isomorphism Network ( GIN ) , also empirically has high representational power as it almost perfectly fits the training data , whereas the less powerful Gnn variants often severely underfit the data . But , in the experiments , the GIN only achieves state-of-the-art results on a number of graph classification benchmarks , which is not convincing . 2 .In the experimental results , the performance of the proposed GIN model is not better than the state of the art models . 3 .The authors should compare the proposed model with other GNN models , e.g. , GCN ( Kipff & Welling 2017 ) and GIN ( Hamilton et al. , 2017 ) . 4 .In Section 5.2 , it is claimed that GIN is as powerful as the Weisfeiler-Lehman graph isomorphism test , but it is not clear why .
1_1907.07355 = This paper presents an interesting analysis of BERT 's performance on the argument reasoning comprehension task . The authors show that the model is exploiting spurious statistical cues in the dataset . They analyze the nature of these cues and demonstrate that a range of models all exploit them . This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The results are interesting . However , I have the following concerns . 1 .The authors claim that BERT ’ s peak performance of 77 % on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline . This result is entirely accounted for by exploitation of spurious statistics in the data . It would be better if the authors can provide more explanation about this phenomenon . 2 .The paper is not well-motivated . It is not clear to me why BERT is able to perform so well on this task . It seems to me that it is because of the fact that the authors do not provide any explanation about why the model performs so well . 3 .In the experiments , the authors only compare BERT with the best model of Botschen et al . ( 2018 ) and the SemEval winner GIST ( Choi and Lee , 2018 ) . I would like to see the performance of other models , such as ESIM and InferSentConneau ( 2017 ) , as well as the state-of-the-art models .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low precision SGD iterates with a modified learning rate schedule . The authors show that the proposed method can match the performance of full-precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The paper is well-written and easy to follow . The proposed method is novel and interesting . However , I have the following concerns : 1 . In the experiments , the authors did not compare with other methods . For example , HALP ( De Sa et al. , 2018 ) has the ability to produce such arbitrarily close solutions ( for general convex objectives ) by dynamically scaling its low-pre precision numbers and using variance reduction . 2 .The authors should compare with HALP in the experiments . 3 .It is not clear to me how to choose the number of blocks in the proposed algorithm . 4 .In the experiment , it is better to compare with the state-of-the-art methods such as QSGD and ZipML .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that uses both prior and posterior distributions over knowledge to facilitate knowledge selection . The paper is well-written and easy to follow . The idea of using the posterior distribution over knowledge is interesting and novel . However , I have the following concerns : 1 . In the prior knowledge module , the prior distribution p ( k|x ) is inferred from both the utterance and the response , while the posterior p ( y|x , y ) is only inferred from the responses . It is not clear to me how the prior p ( x ) can be used to select appropriate knowledge . 2 .In the inference process , the authors use the Kullback-Leibler divergence loss ( KLDivLoss ) to measure the distance between the prior and the posterior distributions . It seems that the authors do not use the KL divergence between the posterior and the prior . 3 .The authors should compare the performance of the proposed model with the baselines on the Persona dataset and Wizard-of-Wikipedia dataset . 4 .The proposed model is not compared with the baseline methods in the literature .
1_2005.06628 = This paper proposes a pruning-based approach to reduce the number of parameters in the BERT language model . The main idea is to prune the architecture of the encoder and the attention head of the Transformer . The pruning is done by optimizing the architecture design dimensions of each layer of the model , and the pruned parameters are then used to train the model in a pre-trained fashion . The authors show that the proposed method can achieve 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the model is interesting . 3 .The proposed pruning method is simple and effective . Cons : 1.1 . It is not clear why the authors chose to optimize the design dimensions in the pruning framework rather than launching a pretraining job for each of these choices . 2.2 .The authors should compare the performance of the proposed pruned model with other pruning methods such as RoBERTa and DistilBERT . 3.3 .The experiments are only conducted on one language modeling task ( GLUE ) . It would be better if the authors can conduct more experiments on other language modeling tasks such as NLP tasks .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where they define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . On ImageNet , our ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original model by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper is technically sound . Cons : 1.1 . The novelty of the proposed method is limited . 2.2 .The experimental results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper presents an analysis of BERT 's performance on the argument comprehension task . The authors show that BERT is able to reach a peak performance of 77 % on the task by exploiting spurious statistical cues in the dataset . They also show that a range of models all exploit these cues and propose to use an adversarial dataset on which all models achieve random accuracy . The paper is well written and easy to follow . The analysis is interesting and provides a new perspective on the BERT model . I think this paper should be accepted to ICLR .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that employs a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed model is novel and well-motivated . 3 .The experiments are comprehensive and show the effectiveness of the model . 4 .The paper is technically sound . Cons : 1 ) The proposed model seems to be a straightforward combination of two existing models . It would be better if the authors can provide a more detailed analysis of the differences between the two models . 2 ) It is not clear why the authors use the KLDivLoss to measure the proximity between the prior distribution and the posterior distribution . 3 ) The authors should provide more details about the model architecture . 4 ) The experiments are only conducted on the Wizard-of-Wikipedia dataset . It is better to conduct experiments on other datasets .
1_2005.06628 = This paper proposes a pruning-based approach to reduce the number of parameters in BERT by pruning the architecture design dimensions . The pruning is done by optimizing the design dimensions in a joint optimization procedure . The authors show that the proposed method achieves 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed pruning method is simple and effective . 3 .The experimental results are convincing . 4 .The paper is easy to read . Cons : 1 ) The proposed method is not very novel . It is a combination of existing pruning methods . 2 ) It is not clear how the pruning can be applied to other objectives such as FLOPs or latency . 3 ) The experiments are limited to a single task ( GLUE ) .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors propose a reinforcement learning based framework improved by multi-task learning and show steady performance gains by considering additional supervised boundary information during training . The proposed method achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well written and easy to follow . 2 .The proposed method is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper is technically sound . Cons : 1.1 . The novelty of the proposed approach is limited . The main contribution of this paper is to introduce a location feature , which enables the agent to explicitly access where the current grounding boundaries are . 2.2 .The experimental results are not convincing . It is not clear why the proposed method outperforms the baselines .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper presents a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures . The results characterize the discriminative power of popular GNN variants , such as Graph Convolutional Networks and GraphSAGE , and show that they can not learn to distinguish certain simple graph structure . The authors then develop a simple architecture that is provably the most expressive among the class of graph neural networks and is as powerful as the Weisfeiler-Lehman graph isomorphism test . The experimental results show that the proposed model achieves state-of-the-art performance on several graph classification benchmarks .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full-precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives , and to a noise ball asymptotically smaller than low precision low-probability SGD in strongly convex settings . The paper is well-written and well-organized . However , I have the following concerns : 1 . In the experiments , it is not clear how to choose the number of quantization bits in the 8-bit SWA update . It would be better if the authors can provide more details about the choice of the quantization bit size . 2 .In the experiment section , the authors should provide more explanation about the results in Table 1 and Table 2 . 3 .In Table 2 , why SWA and HALP do not outperform SGD on CIFAR-10/100 ?
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that employs a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed model is novel and well-motivated . 3 .The experiments are comprehensive and show the effectiveness of the model . 4 .The paper is technically sound . Cons : 1 ) The proposed model seems to be a straightforward combination of two existing models . It would be better if the authors can provide a more detailed analysis of the differences between the two models . 2 ) It is not clear why the authors use the KLDivLoss to measure the proximity between the prior distribution and the posterior distribution . 3 ) The authors should provide more details about the model architecture . 4 ) The experiments are only conducted on the Wizard-of-Wikipedia dataset . It is better to conduct experiments on other datasets .
1_2005.06628 = This paper proposes a method to reduce the number of parameters in the BERT model . The main idea is to use a smaller number of layers in the encoder of BERT . The authors show that this can be achieved by reducing the architecture design dimensions rather than reducing the Transformer encoder layers . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that the proposed method can be applied towards other objectives such as FLOPs or latency . But the authors do not provide any experimental results to support this claim . 2 .The experimental results are not convincing . The proposed method is only compared with BERT with three layers . It is not clear whether this method can also be applied to other models such as Transformer-based models . 3 .In Table 1 , the authors only show the results for BERT without three layers , which is not enough to show the effectiveness of this method .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensembling from multi-original models ; ( 3 ) the teacher network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are only conducted on ImageNet dataset . It would be better to conduct experiments on other datasets .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The experimental results show that the proposed approach outperforms the baselines . Cons : 1.1 . The novelty of this paper is limited . It is not clear to me how the proposed method can be applied to other video grounding tasks , e.g. , ( Gao , Sun , and Nevatia 2016 ) . 2.2 .The paper is not well-motivated . 3.3 .The novelty of the proposed model is limited to the video grounding task .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) to represent different graph structures . The authors provide a theoretical framework for analyzing the power of popular GNN variants , such as Graph Convolutional Networks and GraphSAGE , and show that they can not learn to distinguish certain simple graph structures , and develop a simple architecture that is provably the most expressive among the class of GNN and is as powerful as the Weisfeiler-Lehman graph isomorphism test . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The theoretical analysis is based on the assumption that the graph is a set of multisets of bounded size , which is not true in practice . For example , for a multiset of size R^n , there exists a function f : X to Rn so that for infinitely many choices of f , including all irrational numbers , h ( c , X ) = ( 1 + ) · f ( c ) + f ( x ) + \sum_ { x_i } _ { ij } ( x_j ) , where f is a function of the set X_i and X_j , and f is an embedding of X . The proof of Theorem 3.1 assumes that f is unique for each pair ( c,X ) , but in practice , f is not unique for any pair of pairs ( c and X ) . 2 .Theorem 4.1 shows that GIN is the most powerful GNN by the theoretical analysis , but it is not clear why GIN has high representational power as it almost perfectly fits the training data , whereas the less powerful models often severely underfit the data . 3 .The experimental results show that the proposed model achieves state-of-the-art results on a number of graph classification benchmarks , but there is no comparison with other GNN models . 4 .In the experimental results , the authors only compare with the GIN model . It would be better if the authors can show the performance of the proposed GNN model on other graph classification tasks as well .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors show that the proposed method converges arbitrarily close to the optimal solution for quadratic objectives , and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . Empirically , the authors demonstrate that training with 8-bit SWA outperforms the full precision training with SGD baseline in deep learning tasks such as training Preactivation ResNet-164 ( He et al. , 2016 ) on CIFAR-10 and Cifar-100 datasets ( Krizhevsky & Hinton , 2009 ) . Overall , this paper is well-written and well-motivated . However , I have the following concerns : 1 . It is not clear to me why the authors chose to use BFP instead of fixedpoint because BFP usually has less quantization error caused by overflow and underflow when quantizing DNN models ( Song et al .2017 ) . For deep learning experiments , BFP is preferred over fixedpoint as it usually has more quantization errors . 2 .In the experiments , it is better to compare with other methods such as HALP , QSGD , and ZipML . 3 .The authors should provide more details about the implementation details of SWA and HALP .
1_1902.04911 = This paper proposes an end-to-end neural model which employs a novel knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Specifically , a posterior distribution over knowledge is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution is used to approximate the posterior distribution so that appropriate knowledge can be selected even without responses during the inference process . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The proposed model is not novel . The prior distribution p ( k|x ) can be effectively utilized to sample appropriate knowledge so as to generate proper responses . 2 .It is not clear to me why the KLDivLoss is introduced to measure the proximity between the prior distribution and posterior distribution . 3 .In Table 2 , the authors claim that the proposed method can better incorporate appropriate knowledge in response generation . But it is not convincing to me . In Table 1 , the results are not convincing . The results are better than the baselines , but the improvement is not significant .
1_2005.06628 = This paper proposes a pruning-based approach to reduce the number of parameters in the BERT language model . The main idea is to prune the architecture of the encoder and the attention head of the Transformer . The pruning is done by optimizing the architecture design dimensions of each layer of the model , and the pruned parameters are then used to train the model in a pre-trained fashion . The authors show that the proposed method can achieve 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the model is interesting . 3 .The proposed pruning method is simple and effective . Cons : 1.1 . It is not clear why the authors chose to optimize the design dimensions in the pruning framework rather than launching a pretraining job for each of these choices . 2.2 .The authors should compare the performance of the proposed pruned model with other pruning methods such as RoBERTa and DistilBERT . 3.3 .The experiments are only conducted on one language modeling task ( GLUE ) . It would be better if the authors can conduct more experiments on other language modeling tasks such as NLP tasks .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experimental results are not convincing . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a reinforcement learning based framework for the task of video grounding , which aims to temporally localize a natural language description in a video . To alleviate this problem , the authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset and Charades-STA dataset while observing only 10 or less clips per video . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The main contribution of this paper is the multi-task learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 2 .The experimental results are not convincing . The performance of this method is not better than the state of the art methods .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .