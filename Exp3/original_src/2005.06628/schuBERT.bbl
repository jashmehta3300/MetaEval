\begin{thebibliography}{30}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Anwar et~al.(2017)Anwar, Hwang, and Sung}]{anwar2017structured}
Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. 2017.
\newblock Structured pruning of deep convolutional neural networks.
\newblock \emph{ACM Journal on Emerging Technologies in Computing Systems
  (JETC)}, 13(3):32.

\bibitem[{Bowman et~al.(2015)Bowman, Angeli, Potts, and
  Manning}]{bowman2015large}
Samuel~R Bowman, Gabor Angeli, Christopher Potts, and Christopher~D Manning.
  2015.
\newblock A large annotated corpus for learning natural language inference.
\newblock \emph{arXiv preprint arXiv:1508.05326}.

\bibitem[{Dai and Le(2015)}]{dai2015semi}
Andrew~M Dai and Quoc~V Le. 2015.
\newblock Semi-supervised sequence learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  3079--3087.

\bibitem[{Devlin et~al.(2018)Devlin, Chang, Lee, and
  Toutanova}]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}.

\bibitem[{Han et~al.(2015)Han, Mao, and Dally}]{han2015deep}
Song Han, Huizi Mao, and William~J Dally. 2015.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}.

\bibitem[{Hassibi and Stork(1993)}]{hassibi1993second}
Babak Hassibi and David~G Stork. 1993.
\newblock Second order derivatives for network pruning: Optimal brain surgeon.
\newblock In \emph{Advances in neural information processing systems}, pages
  164--171.

\bibitem[{Howard and Ruder(2018)}]{howard2018universal}
Jeremy Howard and Sebastian Ruder. 2018.
\newblock Universal language model fine-tuning for text classification.
\newblock \emph{arXiv preprint arXiv:1801.06146}.

\bibitem[{Hubara et~al.(2017)Hubara, Courbariaux, Soudry, El-Yaniv, and
  Bengio}]{hubara2017quantized}
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
  Bengio. 2017.
\newblock Quantized neural networks: Training neural networks with low
  precision weights and activations.
\newblock \emph{The Journal of Machine Learning Research}, 18(1):6869--6898.

\bibitem[{Kim and Rush(2016)}]{kim2016sequence}
Yoon Kim and Alexander~M Rush. 2016.
\newblock Sequence-level knowledge distillation.
\newblock \emph{arXiv preprint arXiv:1606.07947}.

\bibitem[{Lan et~al.(2019)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut}]{lan2019albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut. 2019.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock \emph{arXiv preprint arXiv:1909.11942}.

\bibitem[{LeCun et~al.(1990)LeCun, Denker, and Solla}]{lecun1990optimal}
Yann LeCun, John~S Denker, and Sara~A Solla. 1990.
\newblock Optimal brain damage.
\newblock In \emph{Advances in neural information processing systems}, pages
  598--605.

\bibitem[{Li et~al.(2016)Li, Kadav, Durdanovic, Samet, and
  Graf}]{li2016pruning}
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans~Peter Graf. 2016.
\newblock Pruning filters for efficient convnets.
\newblock \emph{arXiv preprint arXiv:1608.08710}.

\bibitem[{Liu et~al.(2018)Liu, Simonyan, and Yang}]{liu2018darts}
Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018.
\newblock Darts: Differentiable architecture search.
\newblock \emph{arXiv preprint arXiv:1806.09055}.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov}]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}.

\bibitem[{Michel et~al.(2019)Michel, Levy, and Neubig}]{michel2019sixteen}
Paul Michel, Omer Levy, and Graham Neubig. 2019.
\newblock Are sixteen heads really better than one?
\newblock \emph{arXiv preprint arXiv:1905.10650}.

\bibitem[{Molchanov et~al.(2016)Molchanov, Tyree, Karras, Aila, and
  Kautz}]{molchanov2016pruning}
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. 2016.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock \emph{arXiv preprint arXiv:1611.06440}.

\bibitem[{Murray and Chiang(2015)}]{murray2015auto}
Kenton Murray and David Chiang. 2015.
\newblock Auto-sizing neural networks: With applications to n-gram language
  models.
\newblock \emph{arXiv preprint arXiv:1508.05051}.

\bibitem[{Ott et~al.(2018)Ott, Edunov, Grangier, and Auli}]{ott2018scaling}
Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. 2018.
\newblock Scaling neural machine translation.
\newblock \emph{arXiv preprint arXiv:1806.00187}.

\bibitem[{Peters et~al.(2018)Peters, Neumann, Iyyer, Gardner, Clark, Lee, and
  Zettlemoyer}]{peters2018deep}
Matthew~E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
  Kenton Lee, and Luke Zettlemoyer. 2018.
\newblock Deep contextualized word representations.
\newblock \emph{arXiv preprint arXiv:1802.05365}.

\bibitem[{Pham et~al.(2018)Pham, Guan, Zoph, Le, and Dean}]{pham2018efficient}
Hieu Pham, Melody~Y Guan, Barret Zoph, Quoc~V Le, and Jeff Dean. 2018.
\newblock Efficient neural architecture search via parameter sharing.
\newblock \emph{arXiv preprint arXiv:1802.03268}.

\bibitem[{Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever}]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018.
\newblock Improving language understanding with unsupervised learning.
\newblock Technical report, Technical report, OpenAI.

\bibitem[{Rastegari et~al.(2016)Rastegari, Ordonez, Redmon, and
  Farhadi}]{rastegari2016xnor}
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. 2016.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In \emph{European conference on computer vision}, pages 525--542.
  Springer.

\bibitem[{Sanh et~al.(2019)Sanh, Debut, Chaumond, and
  Wolf}]{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock \emph{arXiv preprint arXiv:1910.01108}.

\bibitem[{See et~al.(2016)See, Luong, and Manning}]{see2016compression}
Abigail See, Minh-Thang Luong, and Christopher~D Manning. 2016.
\newblock Compression of neural machine translation models via pruning.
\newblock \emph{arXiv preprint arXiv:1606.09274}.

\bibitem[{Singh et~al.(2019)Singh, Khetan, and Karnin}]{singh2019darc}
Shashank Singh, Ashish Khetan, and Zohar Karnin. 2019.
\newblock Darc: Differentiable architecture compression.
\newblock \emph{arXiv preprint arXiv:1905.08170}.

\bibitem[{Strubell et~al.(2018)Strubell, Verga, Andor, Weiss, and
  McCallum}]{strubell2018linguistically}
Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, and Andrew McCallum.
  2018.
\newblock Linguistically-informed self-attention for semantic role labeling.
\newblock \emph{arXiv preprint arXiv:1804.08199}.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pages
  5998--6008.

\bibitem[{Williams et~al.(2017)Williams, Nangia, and
  Bowman}]{williams2017broad}
Adina Williams, Nikita Nangia, and Samuel~R Bowman. 2017.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock \emph{arXiv preprint arXiv:1704.05426}.

\bibitem[{Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le}]{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,
  and Quoc~V Le. 2019.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1906.08237}.

\bibitem[{Zhu et~al.(2016)Zhu, Han, Mao, and Dally}]{zhu2016trained}
Chenzhuo Zhu, Song Han, Huizi Mao, and William~J Dally. 2016.
\newblock Trained ternary quantization.
\newblock \emph{arXiv preprint arXiv:1612.01064}.

\end{thebibliography}
