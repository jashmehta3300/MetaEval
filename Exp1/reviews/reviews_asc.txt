1_2012.04715 = This paper proposes a new SAT solver to solve the problem of determining the existence of a projective plane of order 10 . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates were generated with the assistance of the symbolic computation library Traces ( McKay and Piperno 2014 ) , but it is not clear how one can verify these certificates without needing to trust the output of the library . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that their work is the first SAT-based resolution of Lamâs problem with all of the exhaustive search work completed by SAT solvers . I am not sure if this statement is correct . The previous SAT-SAT solvers have become so strong that they are “ the best solution in most cases ” . Even still , they note that some problems such as some problems like Lam’s Problem have only been solved by specialpurpose means . 2 .In the proof of Theorem 3.1 , the authors assume that the code associated with projective planes of order ten must contain words that are referred to as weight 15 , weight 16 , or ‘ weight 19 ’ words . The former two cases were first ruled out by the searches of ( Lam , Thiel , and Swiercz 1989 ) and ( MacWilliams , Sloane , and Thompson ) and were recently settled via SAT-Based Existence certificates ( Bright et al. , 2020a ,b ) . It would be better if the authors can clarify the difference between their work and these previous works . 3 .The proof of theorem 3.2 is not very clear . It seems that the proof is based on the fact that there are 66 possible A1s and A2s , which is not true for the case of A2 . It is better to clarify this point . 4 .The authors should provide more details about the verification procedure . For example , how to verify the correctness of the certificates ? 5 .The paper is not well-organized . For instance , it is hard to follow the description of the proof in Section 4.1 .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and the Weisfeiler-Lehman ( WL ) graph isomorphism test . The authors show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of nodes as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNN is thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . The paper rigorously study several variants of multisett functions and theoretically characterize their discriminativity power . The results show that under certain conditions , GNN-based graph structures that can not be distinguished by popular GNN variants , such as GCN ( Kipf & Welling , 2017 ) and GraphSAGE ( Hamilton et al. , 2017a ) , and show that the power of GIN ( Santoro et al .2017 ) is equal to that of WL .The authors also develop a simple neural architecture , Graph Isomorphism Network ( GIN ) , which can capture graph structures such as graph structure . The proposed GIN can achieve state-of-the-art performance on node classification , link prediction , and graph classification tasks . However , there is little theoretical understanding of the properties and limitations of GNN , and formal analysis of the representational capacity is limited . Therefore , the design of new GNN based models is mostly based on empirical intuition , heuristics , and experimental trial-anderror .
1_1907.07355 = This paper investigates the performance of BERT on the ARCT dataset . The authors show that the model is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated by adding a copy of each data point with the claim negated and the label inverted . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that BERT ’ s performance can be entirely accounted for in terms of exploiting spurious statistical cues , but they do not provide any evidence to support this claim . 2 .In Table 1 , BERT achieves 77 % test set accuracy with its best run ( Table 1 ) , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question “ what has BERT learned about argument comprehension ? ” 3 .In the experiments , the authors only compare BERT with the SemEval dataset . It would be better if the authors can also compare with the proposed adversarial dataset , which is more challenging .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The proposed method is simple to implement and has little computational overhead . The paper is well written and easy to follow . The authors prove that for quadratic objective functions , SWA can converge to the optimal solution as well as to a smaller asymptotic noise ball than low precision SGD for strongly convex objectives . Empirically , the authors show that training with 8 bit SWA with full precision can match the full precision baseline in deep learning tasks such as ResNet-164 and CIFAR-10/100 datasets . The experimental results show that SWA outperforms the state-of-the-art in terms of generalization performance . Pros : 1 . The idea of using SWA to reduce the quantization noise during training is interesting and novel . 2 .The paper is clearly written . 3 .The experimental results are convincing . 4 .The theoretical analysis is sound . Cons : 1.1 . It is not clear to me why SWA works well with a relatively high learning rate and can tolerate additional noise . It would be better to show the performance of SWA when the learning rate is low . 2.2 .It would be more convincing if the authors can show the convergence rate of the proposed method with a fixed learning rate . 3.3 .It is better to compare the performance with other quantization methods such as quantized SGD . 4.4 .There are some typos and grammatical errors in the paper .
1_1902.04911 = This paper proposes a novel approach to knowledge-grounded dialogue generation . The authors propose to separate the posterior distribution over knowledge from the prior distribution over utterance and response . The proposed model is trained to minimize the KL divergence between the prior and posterior distributions . Then , during the inference process , the model samples knowledge merely based on the prior prior distribution and incorporates the sampled knowledge into response generation . It is proved that this model can effectively learn to generate proper and informative responses by utilizing appropriate knowledge by utilizing knowledge . The paper is well-written and easy to follow . The idea of separating the posterior and prior distributions over knowledge is interesting and novel . However , I have the following concerns . 1 .The authors claim that the proposed method is able to avoid the discrepancy between the posterior distributions over utterances and responses . But , it is not clear to me how the model can avoid this discrepancy . For example , given an utterance , different responses can be generated depending on whether appropriate knowledge is used in the response . If the model is training by selecting wrong knowledge ( e.g. , K2 in R2 ) or knowledge irrelevant to the true response ( i.e .K1 in R1 ) , it can be seen that they are completely useless since they can not provide any helpful information . This kind of discrepancy would stop the model from learning to generating proper responses by using appropriate knowledge . 2 .The proposed approach is not compared with other related works . For instance , [ Dinan et al. , 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 3 .In Table 1 , there is no comparison with [ Ghazvininejad et al .2018 ] , which also uses knowledge selection to guide the knowledge selection . 4 .It is also important to properly incorporate knowledge in response generation , such as K1 and K3 in R4 in the Wizard of Wikipedia dataset . 5 .The paper is not well-motivated . The motivation of this paper is to solve the problem of knowledge-based dialogue generation , which has not been sufficiently studied in the previous work . 6 .There are many typos in the paper . For the example in Section 3.1 , the sentence `` In this dataset , each agent is associated with a persona profile , which is served as knowledge . `` should be removed .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea of the paper is to prune the architecture of the model by optimizing the number of parameters and the pre-training loss . The experiments show that the proposed pruning method can improve the performance of BERT . The paper is well written and easy to follow . However , I have some concerns about the experiments . 1 .The experiments are only conducted on MNLI dataset . It would be more convincing if the authors can conduct experiments on other NLP tasks . 2 .The pruning based architecture search is only applied to BERT architecture . It is not clear how the pruning can be applied to other models . 3 .The authors should compare with other pruning methods , e.g. , [ 1 ] , [ 2 ] and [ 3 ] . 4 .It would be interesting to see the results of pruning BERT on other language modeling tasks , such as NLP , machine translation , etc . 5 .The paper is not well organized . For example , in the introduction , the authors claim that “ The ratio of the architecture design dimensions within a BERT encoder layer can be modified to obtain a layer with better performance . ” .However , the paper does not explain how this can be done . 6 .In the experiments , it would be better to show the results on other tasks . 7 .In Table 1 , it is better to report the accuracy of the pruned BERT models . 8 .The results on Table 2 are not convincing . For instance , the results in Table 1 are not consistent with the results reported in the paper . 9 .In Figure 2 , why the results are not better than the ones reported in [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,47,48,50,49,51,52,53,54,55,56,58,59,60,61,62,63,64,70,72,73,72 ,73,74,75,76,80,83,84,81,88,89,90,91,93,90 ,93,94,94 ,94,96,98,99,99 ,98,101,102,103,103 ,104,104 ,104 ,105,104,105,106 ,107,108 ,108 ,109 ,110 ,111 ,112 ,113 ,114 ,115 ,116 ,117 ,118 ,119 ,122 ,123 , 123 , 128 , 130 , 131 , 128,129 , 130,131 ,131 ,132 ,133 ,134 ,135 ,136 ,137 ,138 ,139 ,140 ,141 ,142 ,143 ,144 ,145 ,150 ,153 ,154 ,155 ,156 ,157 ,163 ,168 ,178 ,179 ,180 ,181 ,183 ,184 ,185 ,186 ,197 ,198 ,199 ,207 ,208 ,207 . 10 .In Section 3.1 , it seems that the authors did not compare with the existing pruning techniques in the literature . It will be better if the author can compare with them . 11 .In section 3.2 , the author claims that ‘ The fully-connected component applied to each token separately plays a much more significant role in the top layers as compared to the bottom layers ’ . This is not true . In fact , the fully connected component is not the same as the attention head . 12 .In table 1 , the result of ‘ tall and narrow ’ architecture is better than ‘ wide and shallow ’ and ‘ shallow and tall ’ architectures . 13 .In figure 2 , the difference between ‘ top and bottom ’ is not very clear . 14 .The author should provide more details about the hyper-parameters of the proposed method . 15 .The proposed method is not compared with the other pruned methods . The author should also compare with these methods .
1_2102.07983 = This paper introduces a new dataset , FEWS ( Few-shot Examples of Word Senses ) , to train and evaluate word sense disambiguation models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary , which contains examples for over 71,000 senses , which is much higher than existing datasets such as SemCor , the largest manually annotated WSD dataset , which only covers about 33,000 . The authors also conduct experiments on knowledge-based approaches and a recent neural biencoder model for WSD . The paper is well-written and easy to follow . The proposed dataset is a good addition to the literature on WSD , and the experiments are well-conducted . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset for training and evaluating WSD models in a few- and zero shot setting . The contribution of the dataset is two-fold : ( 1 ) it exposes models a broad array of senses in a low-shot setting , and ( 2 ) the large evaluation set allows for more robust evaluation of rare senses . 2 .The experiments are conducted on knowledge based approaches and the recent Neural BienCoder model , which are the strongest baselines . However it is not clear whether the proposed dataset can be used as additional training data for other WSD tasks such as transfer learning . It would be better if the authors can show the performance of the proposed datasets on transfer learning tasks .
1_1812.02425 = This paper proposes to use a teacher-student ensembling framework to learn an ensemble of multiple neural networks without incurring any additional testing costs . The idea is to use the outputs of different neural networks as supervisions to guide the target network training . To further improve the robustness of the student networks , the authors introduce an adversarial learning strategy to force the student to generate similar outputs as teachers . The experiments show that MEAL consistently improves the accuracy across a variety of popular network architectures on different datasets . The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a number of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . It is a simple extension of Swapout ( Singh , Hoiem , and Forsyth 2016 ) . 2 .The proposed method can achieve the goal of ensembleling multiple neural network with no additional testing cost . 3 .The experimental results are not convincing . For example , in Table 1 , the performance of Shake-Shake ( Gastaldi 2017 ) and MEAL ( Meals et al. , 2017 ) are not better than Shake-shake and Meals , respectively , in terms of accuracy .
1_1901.06829 = This paper proposes a novel method for grounding natural language descriptions in videos . The proposed method is based on an end-to-end reinforcement learning ( RL ) framework , where the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . At each time step , the agent makes an observation that details the currently selected video clip , and takes an action according to its policy . The environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . In addition , the authors introduce a location feature , which enables the agent to explicitly access the current grounding boundaries . The paper is well-written and easy to follow . The method is novel and well-motivated . The experimental results show that the proposed method outperforms the state-of-the-art methods . However , I have the following concerns : 1 . The novelty of the method is limited . There are several existing methods for video grounding , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) , which can be applied only when all candidates are exhaustively enumerated , as they need to rank all possible clip-sentence pairs to be successful in generating the final grounding result . Moreover , the ranking strategy is specially designed for pre-segmented videos and is not applicable to general cases . 2 .The proposed method can not be applied to general video grounding tasks . 3 .The experiments are not convincing . For example , in Table 1 , the performance of the proposed methods is not better than the baseline methods . 4 .In Table 2 , the results are not consistent with the results reported in the original paper .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . It also shows that it is NP hard in the special case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved for the general case . This paper is well-written and easy to follow . The problem is interesting and the results are interesting . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , it is not clear to me which one is more important . 2 .The paper is not well-motivated . It is hard to understand the motivation of this paper . The motivation of the paper is to study the assignment problems in the housing market setting , but the paper does not provide any motivation . 3 .In the experiments , there is no comparison with the existing literature . 4 .The experiments are not convincing . The experiments are only conducted on synthetic data .
1_2012.04715 = This paper proposes a new SAT solver to solve the problem of determining the existence of a projective plane of order 10 . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates were generated with the assistance of the symbolic computation library Traces ( McKay and Piperno 2014 ) , but it is not clear how one can verify these certificates without needing to trust the output of the library . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that their work is the first SAT-based resolution of Lamâs problem with all of the exhaustive search work completed by SAT solvers . I am not sure if this statement is correct . The previous SAT-SAT solvers have become so strong that they are “ the best solution in most cases ” . Even still , they note that some problems such as some problems like Lam’s Problem have only been solved by specialpurpose means . 2 .In the proof of Theorem 3.1 , the authors assume that the code associated with projective planes of order ten must contain words that are referred to as weight 15 , weight 16 , or ‘ weight 19 ’ words . The former two cases were first ruled out by the searches of ( Lam , Thiel , and Swiercz 1989 ) and ( MacWilliams , Sloane , and Thompson ) and were recently settled via SAT-Based Existence certificates ( Bright et al. , 2020a ,b ) . It would be better if the authors can clarify the difference between their work and these previous works . 3 .The proof of theorem 3.2 is not very clear . It seems that the proof is based on the fact that there are 66 possible A1s and A2s , which is not true for the case of A2 . It is better to clarify this point . 4 .The authors should provide more details about the verification procedure . For example , how to verify the correctness of the certificates ? 5 .The paper is not well-organized . For instance , it is hard to follow the description of the proof in Section 4.1 .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and the Weisfeiler-Lehman ( WL ) graph isomorphism test . The authors show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of nodes as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNN is thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . The paper rigorously study several variants of multisett functions and theoretically characterize their discriminativity power . The results show that under certain conditions , GNN-based graph structures that can not be distinguished by popular GNN variants , such as GCN ( Kipf & Welling , 2017 ) and GraphSAGE ( Hamilton et al. , 2017a ) , and show that the power of GIN ( Santoro et al .2017 ) is equal to that of WL .The authors also develop a simple neural architecture , Graph Isomorphism Network ( GIN ) , which can capture graph structures such as graph structure . The proposed GIN can achieve state-of-the-art performance on node classification , link prediction , and graph classification tasks . However , there is little theoretical understanding of the properties and limitations of GNN , and formal analysis of the representational capacity is limited . Therefore , the design of new GNN based models is mostly based on empirical intuition , heuristics , and experimental trial-anderror .
1_1907.07355 = This paper investigates the performance of BERT on the ARCT dataset . The authors show that the model is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated by adding a copy of each data point with the claim negated and the label inverted . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that BERT ’ s performance can be entirely accounted for in terms of exploiting spurious statistical cues , but they do not provide any evidence to support this claim . 2 .In Table 1 , BERT achieves 77 % test set accuracy with its best run ( Table 1 ) , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question “ what has BERT learned about argument comprehension ? ” 3 .In the experiments , the authors only compare BERT with the SemEval dataset . It would be better if the authors can also compare with the proposed adversarial dataset , which is more challenging .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The proposed method is simple to implement and has little computational overhead . The paper is well written and easy to follow . The authors prove that for quadratic objective functions , SWA can converge to the optimal solution as well as to a smaller asymptotic noise ball than low precision SGD for strongly convex objectives . Empirically , the authors show that training with 8 bit SWA with full precision can match the full precision baseline in deep learning tasks such as ResNet-164 and CIFAR-10/100 datasets . The experimental results show that SWA outperforms the state-of-the-art in terms of generalization performance . Pros : 1 . The idea of using SWA to reduce the quantization noise during training is interesting and novel . 2 .The paper is clearly written . 3 .The experimental results are convincing . 4 .The theoretical analysis is sound . Cons : 1.1 . It is not clear to me why SWA works well with a relatively high learning rate and can tolerate additional noise . It would be better to show the performance of SWA when the learning rate is low . 2.2 .It would be more convincing if the authors can show the convergence rate of the proposed method with a fixed learning rate . 3.3 .It is better to compare the performance with other quantization methods such as quantized SGD . 4.4 .There are some typos and grammatical errors in the paper .
1_1902.04911 = This paper proposes a novel approach to knowledge-grounded dialogue generation . The authors propose to separate the posterior distribution over knowledge from the prior distribution over utterance and response . The proposed model is trained to minimize the KL divergence between the prior and posterior distributions . Then , during the inference process , the model samples knowledge merely based on the prior prior distribution and incorporates the sampled knowledge into response generation . It is proved that this model can effectively learn to generate proper and informative responses by utilizing appropriate knowledge by utilizing knowledge . The paper is well-written and easy to follow . The idea of separating the posterior and prior distributions over knowledge is interesting and novel . However , I have the following concerns . 1 .The authors claim that the proposed method is able to avoid the discrepancy between the posterior distributions over utterances and responses . But , it is not clear to me how the model can avoid this discrepancy . For example , given an utterance , different responses can be generated depending on whether appropriate knowledge is used in the response . If the model is training by selecting wrong knowledge ( e.g. , K2 in R2 ) or knowledge irrelevant to the true response ( i.e .K1 in R1 ) , it can be seen that they are completely useless since they can not provide any helpful information . This kind of discrepancy would stop the model from learning to generating proper responses by using appropriate knowledge . 2 .The proposed approach is not compared with other related works . For instance , [ Dinan et al. , 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 3 .In Table 1 , there is no comparison with [ Ghazvininejad et al .2018 ] , which also uses knowledge selection to guide the knowledge selection . 4 .It is also important to properly incorporate knowledge in response generation , such as K1 and K3 in R4 in the Wizard of Wikipedia dataset . 5 .The paper is not well-motivated . The motivation of this paper is to solve the problem of knowledge-based dialogue generation , which has not been sufficiently studied in the previous work . 6 .There are many typos in the paper . For the example in Section 3.1 , the sentence `` In this dataset , each agent is associated with a persona profile , which is served as knowledge . `` should be removed .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea of the paper is to prune the architecture of the model by optimizing the number of parameters and the pre-training loss . The experiments show that the proposed pruning method can improve the performance of BERT . The paper is well written and easy to follow . However , I have some concerns about the experiments . 1 .The experiments are only conducted on MNLI dataset . It would be more convincing if the authors can conduct experiments on other NLP tasks . 2 .The pruning based architecture search is only applied to BERT architecture . It is not clear how the pruning can be applied to other models . 3 .The authors should compare with other pruning methods , e.g. , [ 1 ] , [ 2 ] and [ 3 ] . 4 .It would be interesting to see the results of pruning BERT on other language modeling tasks , such as NLP , machine translation , etc . 5 .The paper is not well organized . For example , in the introduction , the authors claim that “ The ratio of the architecture design dimensions within a BERT encoder layer can be modified to obtain a layer with better performance . ” .However , the paper does not explain how this can be done . 6 .In the experiments , it would be better to show the results on other tasks . 7 .In Table 1 , it is better to report the accuracy of the pruned BERT models . 8 .The results on Table 2 are not convincing . For instance , the results in Table 1 are not consistent with the results reported in the paper . 9 .In Figure 2 , why the results are not better than the ones reported in [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,47,48,50,49,51,52,53,54,55,56,58,59,60,61,62,63,64,70,72,73,72 ,73,74,75,76,80,83,84,81,88,89,90,91,93,90 ,93,94,94 ,94,96,98,99,99 ,98,101,102,103,103 ,104,104 ,104 ,105,104,105,106 ,107,108 ,108 ,109 ,110 ,111 ,112 ,113 ,114 ,115 ,116 ,117 ,118 ,119 ,122 ,123 , 123 , 128 , 130 , 131 , 128,129 , 130,131 ,131 ,132 ,133 ,134 ,135 ,136 ,137 ,138 ,139 ,140 ,141 ,142 ,143 ,144 ,145 ,150 ,153 ,154 ,155 ,156 ,157 ,163 ,168 ,178 ,179 ,180 ,181 ,183 ,184 ,185 ,186 ,197 ,198 ,199 ,207 ,208 ,207 . 10 .In Section 3.1 , it seems that the authors did not compare with the existing pruning techniques in the literature . It will be better if the author can compare with them . 11 .In section 3.2 , the author claims that ‘ The fully-connected component applied to each token separately plays a much more significant role in the top layers as compared to the bottom layers ’ . This is not true . In fact , the fully connected component is not the same as the attention head . 12 .In table 1 , the result of ‘ tall and narrow ’ architecture is better than ‘ wide and shallow ’ and ‘ shallow and tall ’ architectures . 13 .In figure 2 , the difference between ‘ top and bottom ’ is not very clear . 14 .The author should provide more details about the hyper-parameters of the proposed method . 15 .The proposed method is not compared with the other pruned methods . The author should also compare with these methods .
1_2102.07983 = This paper introduces a new dataset , FEWS ( Few-shot Examples of Word Senses ) , to train and evaluate word sense disambiguation models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary , which contains examples for over 71,000 senses , which is much higher than existing datasets such as SemCor , the largest manually annotated WSD dataset , which only covers about 33,000 . The authors also conduct experiments on knowledge-based approaches and a recent neural biencoder model for WSD . The paper is well-written and easy to follow . The proposed dataset is a good addition to the literature on WSD , and the experiments are well-conducted . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset for training and evaluating WSD models in a few- and zero shot setting . The contribution of the dataset is two-fold : ( 1 ) it exposes models a broad array of senses in a low-shot setting , and ( 2 ) the large evaluation set allows for more robust evaluation of rare senses . 2 .The experiments are conducted on knowledge based approaches and the recent Neural BienCoder model , which are the strongest baselines . However it is not clear whether the proposed dataset can be used as additional training data for other WSD tasks such as transfer learning . It would be better if the authors can show the performance of the proposed datasets on transfer learning tasks .
1_1812.02425 = This paper proposes to use a teacher-student ensembling framework to learn an ensemble of multiple neural networks without incurring any additional testing costs . The idea is to use the outputs of different neural networks as supervisions to guide the target network training . To further improve the robustness of the student networks , the authors introduce an adversarial learning strategy to force the student to generate similar outputs as teachers . The experiments show that MEAL consistently improves the accuracy across a variety of popular network architectures on different datasets . The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a number of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . It is a simple extension of Swapout ( Singh , Hoiem , and Forsyth 2016 ) . 2 .The proposed method can achieve the goal of ensembleling multiple neural network with no additional testing cost . 3 .The experimental results are not convincing . For example , in Table 1 , the performance of Shake-Shake ( Gastaldi 2017 ) and MEAL ( Meals et al. , 2017 ) are not better than Shake-shake and Meals , respectively , in terms of accuracy .
1_1901.06829 = This paper proposes a novel method for grounding natural language descriptions in videos . The proposed method is based on an end-to-end reinforcement learning ( RL ) framework , where the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . At each time step , the agent makes an observation that details the currently selected video clip , and takes an action according to its policy . The environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . In addition , the authors introduce a location feature , which enables the agent to explicitly access the current grounding boundaries . The paper is well-written and easy to follow . The method is novel and well-motivated . The experimental results show that the proposed method outperforms the state-of-the-art methods . However , I have the following concerns : 1 . The novelty of the method is limited . There are several existing methods for video grounding , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) , which can be applied only when all candidates are exhaustively enumerated , as they need to rank all possible clip-sentence pairs to be successful in generating the final grounding result . Moreover , the ranking strategy is specially designed for pre-segmented videos and is not applicable to general cases . 2 .The proposed method can not be applied to general video grounding tasks . 3 .The experiments are not convincing . For example , in Table 1 , the performance of the proposed methods is not better than the baseline methods . 4 .In Table 2 , the results are not consistent with the results reported in the original paper .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a new SAT solver to solve the problem of determining the existence of a projective plane of order 10 . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates were generated with the assistance of the symbolic computation library Traces ( McKay and Piperno 2014 ) , but it is not clear how one can verify these certificates without needing to trust the output of the library . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that their work is the first SAT-based resolution of Lamâs problem with all of the exhaustive search work completed by SAT solvers . I am not sure if this statement is correct . The previous SAT-SAT solvers have become so strong that they are “ the best solution in most cases ” . Even still , they note that some problems such as some problems like Lam’s Problem have only been solved by specialpurpose means . 2 .In the proof of Theorem 3.1 , the authors assume that the code associated with projective planes of order ten must contain words that are referred to as weight 15 , weight 16 , or ‘ weight 19 ’ words . The former two cases were first ruled out by the searches of ( Lam , Thiel , and Swiercz 1989 ) and ( MacWilliams , Sloane , and Thompson ) and were recently settled via SAT-Based Existence certificates ( Bright et al. , 2020a ,b ) . It would be better if the authors can clarify the difference between their work and these previous works . 3 .The proof of theorem 3.2 is not very clear . It seems that the proof is based on the fact that there are 66 possible A1s and A2s , which is not true for the case of A2 . It is better to clarify this point . 4 .The authors should provide more details about the verification procedure . For example , how to verify the correctness of the certificates ? 5 .The paper is not well-organized . For instance , it is hard to follow the description of the proof in Section 4.1 .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and the Weisfeiler-Lehman ( WL ) graph isomorphism test . The authors show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of nodes as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNN is thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . The paper rigorously study several variants of multisett functions and theoretically characterize their discriminativity power . The results show that under certain conditions , GNN-based graph structures that can not be distinguished by popular GNN variants , such as GCN ( Kipf & Welling , 2017 ) and GraphSAGE ( Hamilton et al. , 2017a ) , and show that the power of GIN ( Santoro et al .2017 ) is equal to that of WL .The authors also develop a simple neural architecture , Graph Isomorphism Network ( GIN ) , which can capture graph structures such as graph structure . The proposed GIN can achieve state-of-the-art performance on node classification , link prediction , and graph classification tasks . However , there is little theoretical understanding of the properties and limitations of GNN , and formal analysis of the representational capacity is limited . Therefore , the design of new GNN based models is mostly based on empirical intuition , heuristics , and experimental trial-anderror .
1_1907.07355 = This paper investigates the performance of BERT on the ARCT dataset . The authors show that the model is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated by adding a copy of each data point with the claim negated and the label inverted . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that BERT ’ s performance can be entirely accounted for in terms of exploiting spurious statistical cues , but they do not provide any evidence to support this claim . 2 .In Table 1 , BERT achieves 77 % test set accuracy with its best run ( Table 1 ) , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question “ what has BERT learned about argument comprehension ? ” 3 .In the experiments , the authors only compare BERT with the SemEval dataset . It would be better if the authors can also compare with the proposed adversarial dataset , which is more challenging .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The proposed method is simple to implement and has little computational overhead . The paper is well written and easy to follow . The authors prove that for quadratic objective functions , SWA can converge to the optimal solution as well as to a smaller asymptotic noise ball than low precision SGD for strongly convex objectives . Empirically , the authors show that training with 8 bit SWA with full precision can match the full precision baseline in deep learning tasks such as ResNet-164 and CIFAR-10/100 datasets . The experimental results show that SWA outperforms the state-of-the-art in terms of generalization performance . Pros : 1 . The idea of using SWA to reduce the quantization noise during training is interesting and novel . 2 .The paper is clearly written . 3 .The experimental results are convincing . 4 .The theoretical analysis is sound . Cons : 1.1 . It is not clear to me why SWA works well with a relatively high learning rate and can tolerate additional noise . It would be better to show the performance of SWA when the learning rate is low . 2.2 .It would be more convincing if the authors can show the convergence rate of the proposed method with a fixed learning rate . 3.3 .It is better to compare the performance with other quantization methods such as quantized SGD . 4.4 .There are some typos and grammatical errors in the paper .
1_1902.04911 = This paper proposes a novel approach to knowledge-grounded dialogue generation . The authors propose to separate the posterior distribution over knowledge from the prior distribution over utterance and response . The proposed model is trained to minimize the KL divergence between the prior and posterior distributions . Then , during the inference process , the model samples knowledge merely based on the prior prior distribution and incorporates the sampled knowledge into response generation . It is proved that this model can effectively learn to generate proper and informative responses by utilizing appropriate knowledge by utilizing knowledge . The paper is well-written and easy to follow . The idea of separating the posterior and prior distributions over knowledge is interesting and novel . However , I have the following concerns . 1 .The authors claim that the proposed method is able to avoid the discrepancy between the posterior distributions over utterances and responses . But , it is not clear to me how the model can avoid this discrepancy . For example , given an utterance , different responses can be generated depending on whether appropriate knowledge is used in the response . If the model is training by selecting wrong knowledge ( e.g. , K2 in R2 ) or knowledge irrelevant to the true response ( i.e .K1 in R1 ) , it can be seen that they are completely useless since they can not provide any helpful information . This kind of discrepancy would stop the model from learning to generating proper responses by using appropriate knowledge . 2 .The proposed approach is not compared with other related works . For instance , [ Dinan et al. , 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 3 .In Table 1 , there is no comparison with [ Ghazvininejad et al .2018 ] , which also uses knowledge selection to guide the knowledge selection . 4 .It is also important to properly incorporate knowledge in response generation , such as K1 and K3 in R4 in the Wizard of Wikipedia dataset . 5 .The paper is not well-motivated . The motivation of this paper is to solve the problem of knowledge-based dialogue generation , which has not been sufficiently studied in the previous work . 6 .There are many typos in the paper . For the example in Section 3.1 , the sentence `` In this dataset , each agent is associated with a persona profile , which is served as knowledge . `` should be removed .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea of the paper is to prune the architecture of the model by optimizing the number of parameters and the pre-training loss . The experiments show that the proposed pruning method can improve the performance of BERT . The paper is well written and easy to follow . However , I have some concerns about the experiments . 1 .The experiments are only conducted on MNLI dataset . It would be more convincing if the authors can conduct experiments on other NLP tasks . 2 .The pruning based architecture search is only applied to BERT architecture . It is not clear how the pruning can be applied to other models . 3 .The authors should compare with other pruning methods , e.g. , [ 1 ] , [ 2 ] and [ 3 ] . 4 .It would be interesting to see the results of pruning BERT on other language modeling tasks , such as NLP , machine translation , etc . 5 .The paper is not well organized . For example , in the introduction , the authors claim that “ The ratio of the architecture design dimensions within a BERT encoder layer can be modified to obtain a layer with better performance . ” .However , the paper does not explain how this can be done . 6 .In the experiments , it would be better to show the results on other tasks . 7 .In Table 1 , it is better to report the accuracy of the pruned BERT models . 8 .The results on Table 2 are not convincing . For instance , the results in Table 1 are not consistent with the results reported in the paper . 9 .In Figure 2 , why the results are not better than the ones reported in [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,47,48,50,49,51,52,53,54,55,56,58,59,60,61,62,63,64,70,72,73,72 ,73,74,75,76,80,83,84,81,88,89,90,91,93,90 ,93,94,94 ,94,96,98,99,99 ,98,101,102,103,103 ,104,104 ,104 ,105,104,105,106 ,107,108 ,108 ,109 ,110 ,111 ,112 ,113 ,114 ,115 ,116 ,117 ,118 ,119 ,122 ,123 , 123 , 128 , 130 , 131 , 128,129 , 130,131 ,131 ,132 ,133 ,134 ,135 ,136 ,137 ,138 ,139 ,140 ,141 ,142 ,143 ,144 ,145 ,150 ,153 ,154 ,155 ,156 ,157 ,163 ,168 ,178 ,179 ,180 ,181 ,183 ,184 ,185 ,186 ,197 ,198 ,199 ,207 ,208 ,207 . 10 .In Section 3.1 , it seems that the authors did not compare with the existing pruning techniques in the literature . It will be better if the author can compare with them . 11 .In section 3.2 , the author claims that ‘ The fully-connected component applied to each token separately plays a much more significant role in the top layers as compared to the bottom layers ’ . This is not true . In fact , the fully connected component is not the same as the attention head . 12 .In table 1 , the result of ‘ tall and narrow ’ architecture is better than ‘ wide and shallow ’ and ‘ shallow and tall ’ architectures . 13 .In figure 2 , the difference between ‘ top and bottom ’ is not very clear . 14 .The author should provide more details about the hyper-parameters of the proposed method . 15 .The proposed method is not compared with the other pruned methods . The author should also compare with these methods .
1_2102.07983 = This paper introduces a new dataset , FEWS ( Few-shot Examples of Word Senses ) , to train and evaluate word sense disambiguation models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary , which contains examples for over 71,000 senses , which is much higher than existing datasets such as SemCor , the largest manually annotated WSD dataset , which only covers about 33,000 . The authors also conduct experiments on knowledge-based approaches and a recent neural biencoder model for WSD . The paper is well-written and easy to follow . The proposed dataset is a good addition to the literature on WSD , and the experiments are well-conducted . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset for training and evaluating WSD models in a few- and zero shot setting . The contribution of the dataset is two-fold : ( 1 ) it exposes models a broad array of senses in a low-shot setting , and ( 2 ) the large evaluation set allows for more robust evaluation of rare senses . 2 .The experiments are conducted on knowledge based approaches and the recent Neural BienCoder model , which are the strongest baselines . However it is not clear whether the proposed dataset can be used as additional training data for other WSD tasks such as transfer learning . It would be better if the authors can show the performance of the proposed datasets on transfer learning tasks .
1_1812.02425 = This paper proposes to use a teacher-student ensembling framework to learn an ensemble of multiple neural networks without incurring any additional testing costs . The idea is to use the outputs of different neural networks as supervisions to guide the target network training . To further improve the robustness of the student networks , the authors introduce an adversarial learning strategy to force the student to generate similar outputs as teachers . The experiments show that MEAL consistently improves the accuracy across a variety of popular network architectures on different datasets . The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a number of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . It is a simple extension of Swapout ( Singh , Hoiem , and Forsyth 2016 ) . 2 .The proposed method can achieve the goal of ensembleling multiple neural network with no additional testing cost . 3 .The experimental results are not convincing . For example , in Table 1 , the performance of Shake-Shake ( Gastaldi 2017 ) and MEAL ( Meals et al. , 2017 ) are not better than Shake-shake and Meals , respectively , in terms of accuracy .
1_1901.06829 = This paper proposes a novel method for grounding natural language descriptions in videos . The proposed method is based on an end-to-end reinforcement learning ( RL ) framework , where the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . At each time step , the agent makes an observation that details the currently selected video clip , and takes an action according to its policy . The environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . In addition , the authors introduce a location feature , which enables the agent to explicitly access the current grounding boundaries . The paper is well-written and easy to follow . The method is novel and well-motivated . The experimental results show that the proposed method outperforms the state-of-the-art methods . However , I have the following concerns : 1 . The novelty of the method is limited . There are several existing methods for video grounding , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) , which can be applied only when all candidates are exhaustively enumerated , as they need to rank all possible clip-sentence pairs to be successful in generating the final grounding result . Moreover , the ranking strategy is specially designed for pre-segmented videos and is not applicable to general cases . 2 .The proposed method can not be applied to general video grounding tasks . 3 .The experiments are not convincing . For example , in Table 1 , the performance of the proposed methods is not better than the baseline methods . 4 .In Table 2 , the results are not consistent with the results reported in the original paper .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a new SAT solver to solve the problem of determining the existence of a projective plane of order 10 . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates were generated with the assistance of the symbolic computation library Traces ( McKay and Piperno 2014 ) , but it is not clear how one can verify these certificates without needing to trust the output of the library . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that their work is the first SAT-based resolution of Lamâs problem with all of the exhaustive search work completed by SAT solvers . I am not sure if this statement is correct . The previous SAT-SAT solvers have become so strong that they are “ the best solution in most cases ” . Even still , they note that some problems such as some problems like Lam’s Problem have only been solved by specialpurpose means . 2 .In the proof of Theorem 3.1 , the authors assume that the code associated with projective planes of order ten must contain words that are referred to as weight 15 , weight 16 , or ‘ weight 19 ’ words . The former two cases were first ruled out by the searches of ( Lam , Thiel , and Swiercz 1989 ) and ( MacWilliams , Sloane , and Thompson ) and were recently settled via SAT-Based Existence certificates ( Bright et al. , 2020a ,b ) . It would be better if the authors can clarify the difference between their work and these previous works . 3 .The proof of theorem 3.2 is not very clear . It seems that the proof is based on the fact that there are 66 possible A1s and A2s , which is not true for the case of A2 . It is better to clarify this point . 4 .The authors should provide more details about the verification procedure . For example , how to verify the correctness of the certificates ? 5 .The paper is not well-organized . For instance , it is hard to follow the description of the proof in Section 4.1 .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and the Weisfeiler-Lehman ( WL ) graph isomorphism test . The authors show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of nodes as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNN is thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . The paper rigorously study several variants of multisett functions and theoretically characterize their discriminativity power . The results show that under certain conditions , GNN-based graph structures that can not be distinguished by popular GNN variants , such as GCN ( Kipf & Welling , 2017 ) and GraphSAGE ( Hamilton et al. , 2017a ) , and show that the power of GIN ( Santoro et al .2017 ) is equal to that of WL .The authors also develop a simple neural architecture , Graph Isomorphism Network ( GIN ) , which can capture graph structures such as graph structure . The proposed GIN can achieve state-of-the-art performance on node classification , link prediction , and graph classification tasks . However , there is little theoretical understanding of the properties and limitations of GNN , and formal analysis of the representational capacity is limited . Therefore , the design of new GNN based models is mostly based on empirical intuition , heuristics , and experimental trial-anderror .
1_1907.07355 = This paper investigates the performance of BERT on the ARCT dataset . The authors show that the model is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated by adding a copy of each data point with the claim negated and the label inverted . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that BERT ’ s performance can be entirely accounted for in terms of exploiting spurious statistical cues , but they do not provide any evidence to support this claim . 2 .In Table 1 , BERT achieves 77 % test set accuracy with its best run ( Table 1 ) , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question “ what has BERT learned about argument comprehension ? ” 3 .In the experiments , the authors only compare BERT with the SemEval dataset . It would be better if the authors can also compare with the proposed adversarial dataset , which is more challenging .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The proposed method is simple to implement and has little computational overhead . The paper is well written and easy to follow . The authors prove that for quadratic objective functions , SWA can converge to the optimal solution as well as to a smaller asymptotic noise ball than low precision SGD for strongly convex objectives . Empirically , the authors show that training with 8 bit SWA with full precision can match the full precision baseline in deep learning tasks such as ResNet-164 and CIFAR-10/100 datasets . The experimental results show that SWA outperforms the state-of-the-art in terms of generalization performance . Pros : 1 . The idea of using SWA to reduce the quantization noise during training is interesting and novel . 2 .The paper is clearly written . 3 .The experimental results are convincing . 4 .The theoretical analysis is sound . Cons : 1.1 . It is not clear to me why SWA works well with a relatively high learning rate and can tolerate additional noise . It would be better to show the performance of SWA when the learning rate is low . 2.2 .It would be more convincing if the authors can show the convergence rate of the proposed method with a fixed learning rate . 3.3 .It is better to compare the performance with other quantization methods such as quantized SGD . 4.4 .There are some typos and grammatical errors in the paper .
1_1902.04911 = This paper proposes a novel approach to knowledge-grounded dialogue generation . The authors propose to separate the posterior distribution over knowledge from the prior distribution over utterance and response . The proposed model is trained to minimize the KL divergence between the prior and posterior distributions . Then , during the inference process , the model samples knowledge merely based on the prior prior distribution and incorporates the sampled knowledge into response generation . It is proved that this model can effectively learn to generate proper and informative responses by utilizing appropriate knowledge by utilizing knowledge . The paper is well-written and easy to follow . The idea of separating the posterior and prior distributions over knowledge is interesting and novel . However , I have the following concerns . 1 .The authors claim that the proposed method is able to avoid the discrepancy between the posterior distributions over utterances and responses . But , it is not clear to me how the model can avoid this discrepancy . For example , given an utterance , different responses can be generated depending on whether appropriate knowledge is used in the response . If the model is training by selecting wrong knowledge ( e.g. , K2 in R2 ) or knowledge irrelevant to the true response ( i.e .K1 in R1 ) , it can be seen that they are completely useless since they can not provide any helpful information . This kind of discrepancy would stop the model from learning to generating proper responses by using appropriate knowledge . 2 .The proposed approach is not compared with other related works . For instance , [ Dinan et al. , 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 3 .In Table 1 , there is no comparison with [ Ghazvininejad et al .2018 ] , which also uses knowledge selection to guide the knowledge selection . 4 .It is also important to properly incorporate knowledge in response generation , such as K1 and K3 in R4 in the Wizard of Wikipedia dataset . 5 .The paper is not well-motivated . The motivation of this paper is to solve the problem of knowledge-based dialogue generation , which has not been sufficiently studied in the previous work . 6 .There are many typos in the paper . For the example in Section 3.1 , the sentence `` In this dataset , each agent is associated with a persona profile , which is served as knowledge . `` should be removed .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea of the paper is to prune the architecture of the model by optimizing the number of parameters and the pre-training loss . The experiments show that the proposed pruning method can improve the performance of BERT . The paper is well written and easy to follow . However , I have some concerns about the experiments . 1 .The experiments are only conducted on MNLI dataset . It would be more convincing if the authors can conduct experiments on other NLP tasks . 2 .The pruning based architecture search is only applied to BERT architecture . It is not clear how the pruning can be applied to other models . 3 .The authors should compare with other pruning methods , e.g. , [ 1 ] , [ 2 ] and [ 3 ] . 4 .It would be interesting to see the results of pruning BERT on other language modeling tasks , such as NLP , machine translation , etc . 5 .The paper is not well organized . For example , in the introduction , the authors claim that “ The ratio of the architecture design dimensions within a BERT encoder layer can be modified to obtain a layer with better performance . ” .However , the paper does not explain how this can be done . 6 .In the experiments , it would be better to show the results on other tasks . 7 .In Table 1 , it is better to report the accuracy of the pruned BERT models . 8 .The results on Table 2 are not convincing . For instance , the results in Table 1 are not consistent with the results reported in the paper . 9 .In Figure 2 , why the results are not better than the ones reported in [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,47,48,50,49,51,52,53,54,55,56,58,59,60,61,62,63,64,70,72,73,72 ,73,74,75,76,80,83,84,81,88,89,90,91,93,90 ,93,94,94 ,94,96,98,99,99 ,98,101,102,103,103 ,104,104 ,104 ,105,104,105,106 ,107,108 ,108 ,109 ,110 ,111 ,112 ,113 ,114 ,115 ,116 ,117 ,118 ,119 ,122 ,123 , 123 , 128 , 130 , 131 , 128,129 , 130,131 ,131 ,132 ,133 ,134 ,135 ,136 ,137 ,138 ,139 ,140 ,141 ,142 ,143 ,144 ,145 ,150 ,153 ,154 ,155 ,156 ,157 ,163 ,168 ,178 ,179 ,180 ,181 ,183 ,184 ,185 ,186 ,197 ,198 ,199 ,207 ,208 ,207 . 10 .In Section 3.1 , it seems that the authors did not compare with the existing pruning techniques in the literature . It will be better if the author can compare with them . 11 .In section 3.2 , the author claims that ‘ The fully-connected component applied to each token separately plays a much more significant role in the top layers as compared to the bottom layers ’ . This is not true . In fact , the fully connected component is not the same as the attention head . 12 .In table 1 , the result of ‘ tall and narrow ’ architecture is better than ‘ wide and shallow ’ and ‘ shallow and tall ’ architectures . 13 .In figure 2 , the difference between ‘ top and bottom ’ is not very clear . 14 .The author should provide more details about the hyper-parameters of the proposed method . 15 .The proposed method is not compared with the other pruned methods . The author should also compare with these methods .
1_2102.07983 = This paper introduces a new dataset , FEWS ( Few-shot Examples of Word Senses ) , to train and evaluate word sense disambiguation models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary , which contains examples for over 71,000 senses , which is much higher than existing datasets such as SemCor , the largest manually annotated WSD dataset , which only covers about 33,000 . The authors also conduct experiments on knowledge-based approaches and a recent neural biencoder model for WSD . The paper is well-written and easy to follow . The proposed dataset is a good addition to the literature on WSD , and the experiments are well-conducted . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset for training and evaluating WSD models in a few- and zero shot setting . The contribution of the dataset is two-fold : ( 1 ) it exposes models a broad array of senses in a low-shot setting , and ( 2 ) the large evaluation set allows for more robust evaluation of rare senses . 2 .The experiments are conducted on knowledge based approaches and the recent Neural BienCoder model , which are the strongest baselines . However it is not clear whether the proposed dataset can be used as additional training data for other WSD tasks such as transfer learning . It would be better if the authors can show the performance of the proposed datasets on transfer learning tasks .
1_1812.02425 = This paper proposes to use a teacher-student ensembling framework to learn an ensemble of multiple neural networks without incurring any additional testing costs . The idea is to use the outputs of different neural networks as supervisions to guide the target network training . To further improve the robustness of the student networks , the authors introduce an adversarial learning strategy to force the student to generate similar outputs as teachers . The experiments show that MEAL consistently improves the accuracy across a variety of popular network architectures on different datasets . The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a number of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . It is a simple extension of Swapout ( Singh , Hoiem , and Forsyth 2016 ) . 2 .The proposed method can achieve the goal of ensembleling multiple neural network with no additional testing cost . 3 .The experimental results are not convincing . For example , in Table 1 , the performance of Shake-Shake ( Gastaldi 2017 ) and MEAL ( Meals et al. , 2017 ) are not better than Shake-shake and Meals , respectively , in terms of accuracy .
1_1901.06829 = This paper proposes a novel method for grounding natural language descriptions in videos . The proposed method is based on an end-to-end reinforcement learning ( RL ) framework , where the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . At each time step , the agent makes an observation that details the currently selected video clip , and takes an action according to its policy . The environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . In addition , the authors introduce a location feature , which enables the agent to explicitly access the current grounding boundaries . The paper is well-written and easy to follow . The method is novel and well-motivated . The experimental results show that the proposed method outperforms the state-of-the-art methods . However , I have the following concerns : 1 . The novelty of the method is limited . There are several existing methods for video grounding , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) , which can be applied only when all candidates are exhaustively enumerated , as they need to rank all possible clip-sentence pairs to be successful in generating the final grounding result . Moreover , the ranking strategy is specially designed for pre-segmented videos and is not applicable to general cases . 2 .The proposed method can not be applied to general video grounding tasks . 3 .The experiments are not convincing . For example , in Table 1 , the performance of the proposed methods is not better than the baseline methods . 4 .In Table 2 , the results are not consistent with the results reported in the original paper .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a new SAT solver to solve the problem of determining the existence of a projective plane of order 10 . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates were generated with the assistance of the symbolic computation library Traces ( McKay and Piperno 2014 ) , but it is not clear how one can verify these certificates without needing to trust the output of the library . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that their work is the first SAT-based resolution of Lamâs problem with all of the exhaustive search work completed by SAT solvers . I am not sure if this statement is correct . The previous SAT-SAT solvers have become so strong that they are “ the best solution in most cases ” . Even still , they note that some problems such as some problems like Lam’s Problem have only been solved by specialpurpose means . 2 .In the proof of Theorem 3.1 , the authors assume that the code associated with projective planes of order ten must contain words that are referred to as weight 15 , weight 16 , or ‘ weight 19 ’ words . The former two cases were first ruled out by the searches of ( Lam , Thiel , and Swiercz 1989 ) and ( MacWilliams , Sloane , and Thompson ) and were recently settled via SAT-Based Existence certificates ( Bright et al. , 2020a ,b ) . It would be better if the authors can clarify the difference between their work and these previous works . 3 .The proof of theorem 3.2 is not very clear . It seems that the proof is based on the fact that there are 66 possible A1s and A2s , which is not true for the case of A2 . It is better to clarify this point . 4 .The authors should provide more details about the verification procedure . For example , how to verify the correctness of the certificates ? 5 .The paper is not well-organized . For instance , it is hard to follow the description of the proof in Section 4.1 .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and the Weisfeiler-Lehman ( WL ) graph isomorphism test . The authors show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of nodes as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNN is thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . The paper rigorously study several variants of multisett functions and theoretically characterize their discriminativity power . The results show that under certain conditions , GNN-based graph structures that can not be distinguished by popular GNN variants , such as GCN ( Kipf & Welling , 2017 ) and GraphSAGE ( Hamilton et al. , 2017a ) , and show that the power of GIN ( Santoro et al .2017 ) is equal to that of WL .The authors also develop a simple neural architecture , Graph Isomorphism Network ( GIN ) , which can capture graph structures such as graph structure . The proposed GIN can achieve state-of-the-art performance on node classification , link prediction , and graph classification tasks . However , there is little theoretical understanding of the properties and limitations of GNN , and formal analysis of the representational capacity is limited . Therefore , the design of new GNN based models is mostly based on empirical intuition , heuristics , and experimental trial-anderror .
1_1907.07355 = This paper investigates the performance of BERT on the ARCT dataset . The authors show that the model is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated by adding a copy of each data point with the claim negated and the label inverted . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that BERT ’ s performance can be entirely accounted for in terms of exploiting spurious statistical cues , but they do not provide any evidence to support this claim . 2 .In Table 1 , BERT achieves 77 % test set accuracy with its best run ( Table 1 ) , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question “ what has BERT learned about argument comprehension ? ” 3 .In the experiments , the authors only compare BERT with the SemEval dataset . It would be better if the authors can also compare with the proposed adversarial dataset , which is more challenging .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The proposed method is simple to implement and has little computational overhead . The paper is well written and easy to follow . The authors prove that for quadratic objective functions , SWA can converge to the optimal solution as well as to a smaller asymptotic noise ball than low precision SGD for strongly convex objectives . Empirically , the authors show that training with 8 bit SWA with full precision can match the full precision baseline in deep learning tasks such as ResNet-164 and CIFAR-10/100 datasets . The experimental results show that SWA outperforms the state-of-the-art in terms of generalization performance . Pros : 1 . The idea of using SWA to reduce the quantization noise during training is interesting and novel . 2 .The paper is clearly written . 3 .The experimental results are convincing . 4 .The theoretical analysis is sound . Cons : 1.1 . It is not clear to me why SWA works well with a relatively high learning rate and can tolerate additional noise . It would be better to show the performance of SWA when the learning rate is low . 2.2 .It would be more convincing if the authors can show the convergence rate of the proposed method with a fixed learning rate . 3.3 .It is better to compare the performance with other quantization methods such as quantized SGD . 4.4 .There are some typos and grammatical errors in the paper .
1_1902.04911 = This paper proposes a novel approach to knowledge-grounded dialogue generation . The authors propose to separate the posterior distribution over knowledge from the prior distribution over utterance and response . The proposed model is trained to minimize the KL divergence between the prior and posterior distributions . Then , during the inference process , the model samples knowledge merely based on the prior prior distribution and incorporates the sampled knowledge into response generation . It is proved that this model can effectively learn to generate proper and informative responses by utilizing appropriate knowledge by utilizing knowledge . The paper is well-written and easy to follow . The idea of separating the posterior and prior distributions over knowledge is interesting and novel . However , I have the following concerns . 1 .The authors claim that the proposed method is able to avoid the discrepancy between the posterior distributions over utterances and responses . But , it is not clear to me how the model can avoid this discrepancy . For example , given an utterance , different responses can be generated depending on whether appropriate knowledge is used in the response . If the model is training by selecting wrong knowledge ( e.g. , K2 in R2 ) or knowledge irrelevant to the true response ( i.e .K1 in R1 ) , it can be seen that they are completely useless since they can not provide any helpful information . This kind of discrepancy would stop the model from learning to generating proper responses by using appropriate knowledge . 2 .The proposed approach is not compared with other related works . For instance , [ Dinan et al. , 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 3 .In Table 1 , there is no comparison with [ Ghazvininejad et al .2018 ] , which also uses knowledge selection to guide the knowledge selection . 4 .It is also important to properly incorporate knowledge in response generation , such as K1 and K3 in R4 in the Wizard of Wikipedia dataset . 5 .The paper is not well-motivated . The motivation of this paper is to solve the problem of knowledge-based dialogue generation , which has not been sufficiently studied in the previous work . 6 .There are many typos in the paper . For the example in Section 3.1 , the sentence `` In this dataset , each agent is associated with a persona profile , which is served as knowledge . `` should be removed .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea of the paper is to prune the architecture of the model by optimizing the number of parameters and the pre-training loss . The experiments show that the proposed pruning method can improve the performance of BERT . The paper is well written and easy to follow . However , I have some concerns about the experiments . 1 .The experiments are only conducted on MNLI dataset . It would be more convincing if the authors can conduct experiments on other NLP tasks . 2 .The pruning based architecture search is only applied to BERT architecture . It is not clear how the pruning can be applied to other models . 3 .The authors should compare with other pruning methods , e.g. , [ 1 ] , [ 2 ] and [ 3 ] . 4 .It would be interesting to see the results of pruning BERT on other language modeling tasks , such as NLP , machine translation , etc . 5 .The paper is not well organized . For example , in the introduction , the authors claim that “ The ratio of the architecture design dimensions within a BERT encoder layer can be modified to obtain a layer with better performance . ” .However , the paper does not explain how this can be done . 6 .In the experiments , it would be better to show the results on other tasks . 7 .In Table 1 , it is better to report the accuracy of the pruned BERT models . 8 .The results on Table 2 are not convincing . For instance , the results in Table 1 are not consistent with the results reported in the paper . 9 .In Figure 2 , why the results are not better than the ones reported in [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,47,48,50,49,51,52,53,54,55,56,58,59,60,61,62,63,64,70,72,73,72 ,73,74,75,76,80,83,84,81,88,89,90,91,93,90 ,93,94,94 ,94,96,98,99,99 ,98,101,102,103,103 ,104,104 ,104 ,105,104,105,106 ,107,108 ,108 ,109 ,110 ,111 ,112 ,113 ,114 ,115 ,116 ,117 ,118 ,119 ,122 ,123 , 123 , 128 , 130 , 131 , 128,129 , 130,131 ,131 ,132 ,133 ,134 ,135 ,136 ,137 ,138 ,139 ,140 ,141 ,142 ,143 ,144 ,145 ,150 ,153 ,154 ,155 ,156 ,157 ,163 ,168 ,178 ,179 ,180 ,181 ,183 ,184 ,185 ,186 ,197 ,198 ,199 ,207 ,208 ,207 . 10 .In Section 3.1 , it seems that the authors did not compare with the existing pruning techniques in the literature . It will be better if the author can compare with them . 11 .In section 3.2 , the author claims that ‘ The fully-connected component applied to each token separately plays a much more significant role in the top layers as compared to the bottom layers ’ . This is not true . In fact , the fully connected component is not the same as the attention head . 12 .In table 1 , the result of ‘ tall and narrow ’ architecture is better than ‘ wide and shallow ’ and ‘ shallow and tall ’ architectures . 13 .In figure 2 , the difference between ‘ top and bottom ’ is not very clear . 14 .The author should provide more details about the hyper-parameters of the proposed method . 15 .The proposed method is not compared with the other pruned methods . The author should also compare with these methods .
1_2102.07983 = This paper introduces a new dataset , FEWS ( Few-shot Examples of Word Senses ) , to train and evaluate word sense disambiguation models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary , which contains examples for over 71,000 senses , which is much higher than existing datasets such as SemCor , the largest manually annotated WSD dataset , which only covers about 33,000 . The authors also conduct experiments on knowledge-based approaches and a recent neural biencoder model for WSD . The paper is well-written and easy to follow . The proposed dataset is a good addition to the literature on WSD , and the experiments are well-conducted . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset for training and evaluating WSD models in a few- and zero shot setting . The contribution of the dataset is two-fold : ( 1 ) it exposes models a broad array of senses in a low-shot setting , and ( 2 ) the large evaluation set allows for more robust evaluation of rare senses . 2 .The experiments are conducted on knowledge based approaches and the recent Neural BienCoder model , which are the strongest baselines . However it is not clear whether the proposed dataset can be used as additional training data for other WSD tasks such as transfer learning . It would be better if the authors can show the performance of the proposed datasets on transfer learning tasks .
1_1812.02425 = This paper proposes to use a teacher-student ensembling framework to learn an ensemble of multiple neural networks without incurring any additional testing costs . The idea is to use the outputs of different neural networks as supervisions to guide the target network training . To further improve the robustness of the student networks , the authors introduce an adversarial learning strategy to force the student to generate similar outputs as teachers . The experiments show that MEAL consistently improves the accuracy across a variety of popular network architectures on different datasets . The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a number of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . It is a simple extension of Swapout ( Singh , Hoiem , and Forsyth 2016 ) . 2 .The proposed method can achieve the goal of ensembleling multiple neural network with no additional testing cost . 3 .The experimental results are not convincing . For example , in Table 1 , the performance of Shake-Shake ( Gastaldi 2017 ) and MEAL ( Meals et al. , 2017 ) are not better than Shake-shake and Meals , respectively , in terms of accuracy .
1_1901.06829 = This paper proposes a novel method for grounding natural language descriptions in videos . The proposed method is based on an end-to-end reinforcement learning ( RL ) framework , where the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . At each time step , the agent makes an observation that details the currently selected video clip , and takes an action according to its policy . The environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . In addition , the authors introduce a location feature , which enables the agent to explicitly access the current grounding boundaries . The paper is well-written and easy to follow . The method is novel and well-motivated . The experimental results show that the proposed method outperforms the state-of-the-art methods . However , I have the following concerns : 1 . The novelty of the method is limited . There are several existing methods for video grounding , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) , which can be applied only when all candidates are exhaustively enumerated , as they need to rank all possible clip-sentence pairs to be successful in generating the final grounding result . Moreover , the ranking strategy is specially designed for pre-segmented videos and is not applicable to general cases . 2 .The proposed method can not be applied to general video grounding tasks . 3 .The experiments are not convincing . For example , in Table 1 , the performance of the proposed methods is not better than the baseline methods . 4 .In Table 2 , the results are not consistent with the results reported in the original paper .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
1_2012.04715 = This paper proposes a new SAT solver to solve the problem of determining the existence of a projective plane of order 10 . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates were generated with the assistance of the symbolic computation library Traces ( McKay and Piperno 2014 ) , but it is not clear how one can verify these certificates without needing to trust the output of the library . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that their work is the first SAT-based resolution of Lamâs problem with all of the exhaustive search work completed by SAT solvers . I am not sure if this statement is correct . The previous SAT-SAT solvers have become so strong that they are “ the best solution in most cases ” . Even still , they note that some problems such as some problems like Lam’s Problem have only been solved by specialpurpose means . 2 .In the proof of Theorem 3.1 , the authors assume that the code associated with projective planes of order ten must contain words that are referred to as weight 15 , weight 16 , or ‘ weight 19 ’ words . The former two cases were first ruled out by the searches of ( Lam , Thiel , and Swiercz 1989 ) and ( MacWilliams , Sloane , and Thompson ) and were recently settled via SAT-Based Existence certificates ( Bright et al. , 2020a ,b ) . It would be better if the authors can clarify the difference between their work and these previous works . 3 .The proof of theorem 3.2 is not very clear . It seems that the proof is based on the fact that there are 66 possible A1s and A2s , which is not true for the case of A2 . It is better to clarify this point . 4 .The authors should provide more details about the verification procedure . For example , how to verify the correctness of the certificates ? 5 .The paper is not well-organized . For instance , it is hard to follow the description of the proof in Section 4.1 .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and the Weisfeiler-Lehman ( WL ) graph isomorphism test . The authors show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of nodes as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNN is thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . The paper rigorously study several variants of multisett functions and theoretically characterize their discriminativity power . The results show that under certain conditions , GNN-based graph structures that can not be distinguished by popular GNN variants , such as GCN ( Kipf & Welling , 2017 ) and GraphSAGE ( Hamilton et al. , 2017a ) , and show that the power of GIN ( Santoro et al .2017 ) is equal to that of WL .The authors also develop a simple neural architecture , Graph Isomorphism Network ( GIN ) , which can capture graph structures such as graph structure . The proposed GIN can achieve state-of-the-art performance on node classification , link prediction , and graph classification tasks . However , there is little theoretical understanding of the properties and limitations of GNN , and formal analysis of the representational capacity is limited . Therefore , the design of new GNN based models is mostly based on empirical intuition , heuristics , and experimental trial-anderror .
1_1907.07355 = This paper investigates the performance of BERT on the ARCT dataset . The authors show that the model is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated by adding a copy of each data point with the claim negated and the label inverted . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that BERT ’ s performance can be entirely accounted for in terms of exploiting spurious statistical cues , but they do not provide any evidence to support this claim . 2 .In Table 1 , BERT achieves 77 % test set accuracy with its best run ( Table 1 ) , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question “ what has BERT learned about argument comprehension ? ” 3 .In the experiments , the authors only compare BERT with the SemEval dataset . It would be better if the authors can also compare with the proposed adversarial dataset , which is more challenging .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The proposed method is simple to implement and has little computational overhead . The paper is well written and easy to follow . The authors prove that for quadratic objective functions , SWA can converge to the optimal solution as well as to a smaller asymptotic noise ball than low precision SGD for strongly convex objectives . Empirically , the authors show that training with 8 bit SWA with full precision can match the full precision baseline in deep learning tasks such as ResNet-164 and CIFAR-10/100 datasets . The experimental results show that SWA outperforms the state-of-the-art in terms of generalization performance . Pros : 1 . The idea of using SWA to reduce the quantization noise during training is interesting and novel . 2 .The paper is clearly written . 3 .The experimental results are convincing . 4 .The theoretical analysis is sound . Cons : 1.1 . It is not clear to me why SWA works well with a relatively high learning rate and can tolerate additional noise . It would be better to show the performance of SWA when the learning rate is low . 2.2 .It would be more convincing if the authors can show the convergence rate of the proposed method with a fixed learning rate . 3.3 .It is better to compare the performance with other quantization methods such as quantized SGD . 4.4 .There are some typos and grammatical errors in the paper .
1_1902.04911 = This paper proposes a novel approach to knowledge-grounded dialogue generation . The authors propose to separate the posterior distribution over knowledge from the prior distribution over utterance and response . The proposed model is trained to minimize the KL divergence between the prior and posterior distributions . Then , during the inference process , the model samples knowledge merely based on the prior prior distribution and incorporates the sampled knowledge into response generation . It is proved that this model can effectively learn to generate proper and informative responses by utilizing appropriate knowledge by utilizing knowledge . The paper is well-written and easy to follow . The idea of separating the posterior and prior distributions over knowledge is interesting and novel . However , I have the following concerns . 1 .The authors claim that the proposed method is able to avoid the discrepancy between the posterior distributions over utterances and responses . But , it is not clear to me how the model can avoid this discrepancy . For example , given an utterance , different responses can be generated depending on whether appropriate knowledge is used in the response . If the model is training by selecting wrong knowledge ( e.g. , K2 in R2 ) or knowledge irrelevant to the true response ( i.e .K1 in R1 ) , it can be seen that they are completely useless since they can not provide any helpful information . This kind of discrepancy would stop the model from learning to generating proper responses by using appropriate knowledge . 2 .The proposed approach is not compared with other related works . For instance , [ Dinan et al. , 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 3 .In Table 1 , there is no comparison with [ Ghazvininejad et al .2018 ] , which also uses knowledge selection to guide the knowledge selection . 4 .It is also important to properly incorporate knowledge in response generation , such as K1 and K3 in R4 in the Wizard of Wikipedia dataset . 5 .The paper is not well-motivated . The motivation of this paper is to solve the problem of knowledge-based dialogue generation , which has not been sufficiently studied in the previous work . 6 .There are many typos in the paper . For the example in Section 3.1 , the sentence `` In this dataset , each agent is associated with a persona profile , which is served as knowledge . `` should be removed .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea of the paper is to prune the architecture of the model by optimizing the number of parameters and the pre-training loss . The experiments show that the proposed pruning method can improve the performance of BERT . The paper is well written and easy to follow . However , I have some concerns about the experiments . 1 .The experiments are only conducted on MNLI dataset . It would be more convincing if the authors can conduct experiments on other NLP tasks . 2 .The pruning based architecture search is only applied to BERT architecture . It is not clear how the pruning can be applied to other models . 3 .The authors should compare with other pruning methods , e.g. , [ 1 ] , [ 2 ] and [ 3 ] . 4 .It would be interesting to see the results of pruning BERT on other language modeling tasks , such as NLP , machine translation , etc . 5 .The paper is not well organized . For example , in the introduction , the authors claim that “ The ratio of the architecture design dimensions within a BERT encoder layer can be modified to obtain a layer with better performance . ” .However , the paper does not explain how this can be done . 6 .In the experiments , it would be better to show the results on other tasks . 7 .In Table 1 , it is better to report the accuracy of the pruned BERT models . 8 .The results on Table 2 are not convincing . For instance , the results in Table 1 are not consistent with the results reported in the paper . 9 .In Figure 2 , why the results are not better than the ones reported in [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,47,48,50,49,51,52,53,54,55,56,58,59,60,61,62,63,64,70,72,73,72 ,73,74,75,76,80,83,84,81,88,89,90,91,93,90 ,93,94,94 ,94,96,98,99,99 ,98,101,102,103,103 ,104,104 ,104 ,105,104,105,106 ,107,108 ,108 ,109 ,110 ,111 ,112 ,113 ,114 ,115 ,116 ,117 ,118 ,119 ,122 ,123 , 123 , 128 , 130 , 131 , 128,129 , 130,131 ,131 ,132 ,133 ,134 ,135 ,136 ,137 ,138 ,139 ,140 ,141 ,142 ,143 ,144 ,145 ,150 ,153 ,154 ,155 ,156 ,157 ,163 ,168 ,178 ,179 ,180 ,181 ,183 ,184 ,185 ,186 ,197 ,198 ,199 ,207 ,208 ,207 . 10 .In Section 3.1 , it seems that the authors did not compare with the existing pruning techniques in the literature . It will be better if the author can compare with them . 11 .In section 3.2 , the author claims that ‘ The fully-connected component applied to each token separately plays a much more significant role in the top layers as compared to the bottom layers ’ . This is not true . In fact , the fully connected component is not the same as the attention head . 12 .In table 1 , the result of ‘ tall and narrow ’ architecture is better than ‘ wide and shallow ’ and ‘ shallow and tall ’ architectures . 13 .In figure 2 , the difference between ‘ top and bottom ’ is not very clear . 14 .The author should provide more details about the hyper-parameters of the proposed method . 15 .The proposed method is not compared with the other pruned methods . The author should also compare with these methods .
1_2102.07983 = This paper introduces a new dataset , FEWS ( Few-shot Examples of Word Senses ) , to train and evaluate word sense disambiguation models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary , which contains examples for over 71,000 senses , which is much higher than existing datasets such as SemCor , the largest manually annotated WSD dataset , which only covers about 33,000 . The authors also conduct experiments on knowledge-based approaches and a recent neural biencoder model for WSD . The paper is well-written and easy to follow . The proposed dataset is a good addition to the literature on WSD , and the experiments are well-conducted . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset for training and evaluating WSD models in a few- and zero shot setting . The contribution of the dataset is two-fold : ( 1 ) it exposes models a broad array of senses in a low-shot setting , and ( 2 ) the large evaluation set allows for more robust evaluation of rare senses . 2 .The experiments are conducted on knowledge based approaches and the recent Neural BienCoder model , which are the strongest baselines . However it is not clear whether the proposed dataset can be used as additional training data for other WSD tasks such as transfer learning . It would be better if the authors can show the performance of the proposed datasets on transfer learning tasks .
1_1812.02425 = This paper proposes to use a teacher-student ensembling framework to learn an ensemble of multiple neural networks without incurring any additional testing costs . The idea is to use the outputs of different neural networks as supervisions to guide the target network training . To further improve the robustness of the student networks , the authors introduce an adversarial learning strategy to force the student to generate similar outputs as teachers . The experiments show that MEAL consistently improves the accuracy across a variety of popular network architectures on different datasets . The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a number of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . It is a simple extension of Swapout ( Singh , Hoiem , and Forsyth 2016 ) . 2 .The proposed method can achieve the goal of ensembleling multiple neural network with no additional testing cost . 3 .The experimental results are not convincing . For example , in Table 1 , the performance of Shake-Shake ( Gastaldi 2017 ) and MEAL ( Meals et al. , 2017 ) are not better than Shake-shake and Meals , respectively , in terms of accuracy .
1_1901.06829 = This paper proposes a novel method for grounding natural language descriptions in videos . The proposed method is based on an end-to-end reinforcement learning ( RL ) framework , where the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . At each time step , the agent makes an observation that details the currently selected video clip , and takes an action according to its policy . The environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . In addition , the authors introduce a location feature , which enables the agent to explicitly access the current grounding boundaries . The paper is well-written and easy to follow . The method is novel and well-motivated . The experimental results show that the proposed method outperforms the state-of-the-art methods . However , I have the following concerns : 1 . The novelty of the method is limited . There are several existing methods for video grounding , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) , which can be applied only when all candidates are exhaustively enumerated , as they need to rank all possible clip-sentence pairs to be successful in generating the final grounding result . Moreover , the ranking strategy is specially designed for pre-segmented videos and is not applicable to general cases . 2 .The proposed method can not be applied to general video grounding tasks . 3 .The experiments are not convincing . For example , in Table 1 , the performance of the proposed methods is not better than the baseline methods . 4 .In Table 2 , the results are not consistent with the results reported in the original paper .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . Each agent has a preference list of all objects and the agents are embedded in a social network which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The paper shows that the problem is polynomial-time solvable in general graphs and NP-hard in star-structures and complete graphs . The authors also show the NP-Hardness of Object Reachability in complete graphs , which can be modified to show the general case where the agent is an endpoint ( a leaf ) in the general path and left it unsolved . The problem is interesting and the paper is well-written . However , I have the following concerns : 1 . In the paper , there are two different preference sets for agents . One is strict , which is a full ordinal list of objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors only study the two preference sets from different aspects . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 3.1 . 3 .In Theorem 4.1 , the proof is not easy to follow . 4 .The proof of theorem 4.2 is not clear . 5 .In the proof , the inequality in Theorem 5.1 is not correct . 6 .In theorem 5.2 , there is a typo in the last line .
