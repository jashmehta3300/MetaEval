1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of this paper is to use SAT solver to solve the projective plane of order 10 problem . The authors show consistency issues in both previous searches , which highlights the difficulty of relying on special-purpose search code for nonexistence results . In addition , using well-tested SAT solvers is less error-prone than writing special purpose search code . However , it is difficult to make custom-written code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) to capture different graph structures . The main contribution of the paper is to analyze the discriminative power of popular GNN variants , such as Graph Convolutional Networks and GraphSAGE , and show that they can not learn to distinguish certain simple graph structures , and develop a simple architecture that is provably the most expressive among the class of GNN and is as powerful as the Weisfeiler-Lehman graph isomorphism test . The paper is well-written and easy to follow . However , I have the following concerns : 1 . In the abstract , the authors claim that the most powerful GNN , i.e. , Graph Isomorphism Network ( GIN ) , also empirically has high representational power as it almost perfectly fits the training data , whereas the less powerful models often severely underfit the data . But , in the experiments , the GIN only outperforms the state-of-the-art results on a number of graph classification benchmarks , which is not convincing . 2 .In the experimental results , the performance of the proposed GIN is not better than the other GNN models . 3 .The experimental results are not convincing enough . In Table 1 , the results of GIN and GCN are not comparable . 4 .In Table 2 , the result of GCN and GIN are not compared .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation with knowledge-grounded responses . The authors propose a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . The paper is well-written and easy to follow . However , I have the following concerns . 1 .The authors claim that their model is the first neural model which incorporates the posterior distribution as guidance , enabling accurate knowledge lookups and high quality response generation . But , it is not clear to me how this is achieved . For example , the prior distribution p ( k|x ) is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution is inferred only from utterances only , so that appropriate knowledge can be selected even without responses during the inference process . Compared with the previous work , our model can better incorporate appropriate knowledge in response generation , which is better than previous work . 2 .In Table 1 , the authors compare the performance of their model with Seq2Seq model with a GRU decoder where knowledge is concatenated . It would be better if the authors can show the results of different ways of incorporating knowledge incorporating knowledge . 3 .It is better to show the effect of different loss functions in Table 1 .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT model . The main idea is to prune the parameters of each layer of the model by 5 different dimensions , and then pre-train multiple variants of BERT with different values chosen for these dimensions by applying pruning-based architecture search technique that jointly optimizes the architecture of the models with the objective of minimizing ar X iv :2 00 5.0 . The paper is well written and easy to follow . However , I have the following concerns : 1 . The pruning method is based on the idea of pruning the parameters by 5 dimensions , which is not a new idea . It has been proposed in [ 1 ] , [ 2 ] and [ 3 ] . The authors should compare the results with these two papers . 2 .The pruning methods are based on 5 different parameters , but it is not clear how to choose the prunable parameters . 3 .The authors should also compare the performance of the proposed method with other pruning techniques , such as [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,44,45,46,47,48,50,49,52,54,55,56,57,58,59,60,61,62,63,64,70,72,73,74,75,76,77,78,80,83,84,89,90,90 ,91,93,94,96,98,99,100,103,104,103 ,104,105,106,107,108,113,114,115,116,117,118,119,120,122,123,123 ,123,130,131,132,133,134,134 ,135,135,136 ,136,137,137 ,139,138,139,139 ,140,140,141,142,143,144,145,150,153,155,160,165,170,175,174,176,177,178,179,181,183,184,189,190,196,197,198,199,200,202,203,204,205,207,207 ,207,208,209,211,218,223,223 ,226,228,229,226,227,228 ,229,232,233,234,237,239,234 ,234,240,242,245,240 ,245,251,250,273,255,288,273 ,288,300,300 ,330,320,330,330 ,330 ,360,400,400 ,400,450,450 ,450,500 ,600 ,600,600,700,700 ,800,800,900,900 , 1000 , 1000,1000 , 2000 , 2000,000 , 3000 , 4000 , 5000 , 4000,5000 , 5000,5000,600 , 10000 , 10000,3000 , 5000 . 4 .The proposed method can be applied towards other objectives such as FLOPs or latency . It would be better if the authors can compare with other methods such as DistilBERT ( Sanh et al .2019 ) and RoBERTa ( Liu et al. , 2019 ) . 5 .In the experiments , the authors need to compare with the results of [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 46 , 47 , 48 , 50 , 51 , 52 , 53 , 54 , 56 , 57 , 58 , 59 , 60 , 62 , 63 , 64 , 68 , 69 , 70 , 72 , 73 , 74 , 78 , 79 , 80 , 81 , 82 , 83 , 84 , 88 , 89 , 90 , 92 , 93 , 94 , 89, 92 , 92, 93 , 93, 94 , 94, 94, 95 , 96 , 98 , 99 , 100 , 102 , 103 , 104 , 108 , 113 , 113,114 , 115 , 116 , 118 , 119 , 123 , 120 , 118, 123 , 123,124 , 125 , 128 , 133 , 134 , 135 , 136 , 137 , 138 , 139 , 140 , 145 , 144 , 149 , 149,150 , 144,151 , 146 , 147 , 148 , 155 , 156 , 175 , 177 , 178 , 179 , 179,178 , 179-179 , 174 , 181 , 184 ,
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a reinforcement learning based framework for the task of video grounding , which aims to temporally localize a natural language description in a video . To alleviate this problem , the authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset and Charades-STA dataset while observing only 10 or less clips per video . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The main contribution of this paper is the multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . 2 .It is not clear why the observation network takes visual and textual information as well as current temporal boundaries into consideration . 3 .The experiments are not convincing . It is better to conduct experiments on more datasets .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to solve the problem of finding a projective plane of order 10 from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solvers instead of special-purpose search code to perform the search . However , the paper does not provide a formal proof of the correctness of the SAT solver . It would be better if the authors can provide the formal proof in the supplementary material .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) to capture different graph structures . The main contribution of the paper is to analyze the discriminative power of popular GNN variants , such as Graph Convolutional Networks and GraphSAGE , and show that they can not learn to distinguish certain simple graph structures , and develop a simple architecture that is provably the most expressive among the class of GNN and is as powerful as the Weisfeiler-Lehman graph isomorphism test . The paper is well-written and easy to follow . However , I have the following concerns : 1 . In the abstract , the authors claim that the most powerful GNN , i.e. , Graph Isomorphism Network ( GIN ) , also empirically has high representational power as it almost perfectly fits the training data , whereas the less powerful models often severely underfit the data . But , in the experiments , the GIN only outperforms the state-of-the-art results on a number of graph classification benchmarks , which is not convincing . 2 .In the experimental results , the performance of the proposed GIN is not better than the other GNN models . 3 .The experimental results are not convincing enough . In Table 1 , the results of GIN and GCN are not comparable . 4 .In Table 2 , the result of GCN and GIN are not compared .
1_1907.07355 = This paper presents an analysis of BERT 's performance on the argument comprehension task . The authors show that BERT is able to reach a peak performance of 77 % on the task by exploiting spurious statistical cues in the dataset . They also show that a range of models all exploit these cues and propose to use an adversarial dataset on which all models achieve random accuracy . The paper is well written and easy to follow . The analysis is interesting and provides a new perspective on the BERT model . I think this paper should be accepted to ICLR .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . Theoretical results show that the proposed method converges arbitrarily close to the optimal solution for quadratic objectives , and to a noise ball asymptotically smaller than low Precision SGD in strongly convex settings . The paper is well-written and easy to follow . However , I have the following concerns : 1 . In the experiments , it is not clear to me how to choose the number of iterations for the experiments . It would be better if the authors can provide more details about the choice of number of epochs . 2 .In the experiment section , the authors should provide more information about the training procedure . For example , how many epochs are used in the experiments ? 3 .In Section 4.2 , it would be good to provide more explanation about the algorithm . 4 .In Theorem 1 , it seems that the variance of these samples always satisfies E [ f ( w ) - \sigma_2 ] = σ_2 for some constant σ > 0 . It is better to provide some explanation about why this is true .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that employs a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed model is novel and well-motivated . 3 .The experiments are comprehensive and show the effectiveness of the model . 4 .The paper is technically sound . Cons : 1 ) The proposed model seems to be a straightforward combination of two existing models . It would be better if the authors can provide a more detailed analysis of the differences between the two models . 2 ) It is not clear why the authors propose to use the posterior distribution over knowledge instead of the actual response distribution .
1_2005.06628 = This paper proposes a method to reduce the number of parameters in the BERT model . The main idea is to use a smaller number of layers in the encoder of BERT . The authors show that this can be achieved by reducing the architecture design dimensions rather than reducing the Transformer encoder layers . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that the proposed method can be applied towards other objectives such as FLOPs or latency . But the authors do not provide any experimental results to support this claim . 2 .The experimental results are not convincing . The proposed method is only compared with BERT with three layers . It is not clear whether this method can also be applied to other models such as Transformer-based models . 3 .In Table 1 , the authors only show the results for BERT without three layers , which is not enough to show the effectiveness of this method .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a reinforcement learning based framework for the task of video grounding , which aims to localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed framework is novel in that it successfully combines supervised learning with reinforcement learning in a multi-task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The experimental results show that the proposed method outperforms the baselines . Cons : 1.1 . The novelty of this paper is limited . 2.2 .The novelty of the proposed model is limited because it is a combination of supervised learning and reinforcement learning . 3.3 .The experiment results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . This paper gives an algorithm for the 2-Sat problem . Then the authors show that when the preferences are weak ( ties among objects are allowed ) , the problem becomes NP-hard . The paper is well-written and easy to follow . The main idea of this paper is to study the housing market problem under social networks , where two agents may swap their items under two conditions : ( 1 ) neighbors in social network and ( 2 ) they find it mutually profitable ( or no one will become worse under weak preferences ) . The idea of the paper is interesting . However , I have the following concerns about this paper . 1 .The paper is not well-motivated . The motivation of the problem is not clear to me . In this paper , the authors assume that the agents have the same set of preferences . But , in practice , the agents may have different preferences . For example , the agent may have two sets of preferences : ( i ) a set of objects , and ( ii ) no objects are reachable . In other words , it is not easy to understand the problem . 2 .In the paper , there are some typos in the proof of Theorem 1 and Theorem 2 . 3 .In Theorem 3.1 , there is a typo in the definition of Pareto Efficiency .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) to capture different graph structures . The main contribution of the paper is to analyze the discriminative power of popular GNN variants , such as Graph Convolutional Networks and GraphSAGE , and show that they can not learn to distinguish certain simple graph structures , and develop a simple architecture that is provably the most expressive among the class of GNN and is as powerful as the Weisfeiler-Lehman graph isomorphism test . The paper is well-written and easy to follow . However , I have the following concerns : 1 . In the abstract , the authors claim that the most powerful GNN , i.e. , Graph Isomorphism Network ( GIN ) , also empirically has high representational power as it almost perfectly fits the training data , whereas the less powerful models often severely underfit the data . But , in the experiments , the GIN only outperforms the state-of-the-art results on a number of graph classification benchmarks , which is not convincing . 2 .In the experimental results , the performance of the proposed GIN is not better than the other GNN models . 3 .The experimental results are not convincing enough . In Table 1 , the results of GIN and GCN are not comparable . 4 .In Table 2 , the result of GCN and GIN are not compared .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to show that the performance of BERT is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result , but I am not sure if it is significant enough to be published in ICLR . 2 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , but it is not clear to me why this is the case . It would be better if the authors can provide more explanations . 3 .In Table 1 , it would be interesting to see the results of different models on the same dataset . 4 .I do not see how this paper can be considered a generalization of ACL .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation with knowledge-grounded responses . The authors propose a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . The paper is well-written and easy to follow . However , I have the following concerns . 1 .The authors claim that their model is the first neural model which incorporates the posterior distribution as guidance , enabling accurate knowledge lookups and high quality response generation . But , it is not clear to me how this is achieved . For example , the prior distribution p ( k|x ) is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution is inferred only from utterances only , so that appropriate knowledge can be selected even without responses during the inference process . Compared with the previous work , our model can better incorporate appropriate knowledge in response generation , which is better than previous work . 2 .In Table 1 , the authors compare the performance of their model with Seq2Seq model with a GRU decoder where knowledge is concatenated . It would be better if the authors can show the results of different ways of incorporating knowledge incorporating knowledge . 3 .It is better to show the effect of different loss functions in Table 1 .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT model . The main idea is to prune the design dimensions of the model , i.e. , number of encoder layers , hidden size h , and number of attention heads a . The pruning is done by finding the optimal design dimensions for each of these dimensions , and then pre-training multiple variants of BERT with different values chosen for these dimensions by applying pruning-based architecture search technique . Experiments on GLUE and SQuAD datasets show that the pruned model achieves 6.6 % higher average accuracy compared to the model with three encoder layer . Pros : - The paper is well-written and easy to follow . - The idea of pruning the model parameters is interesting . - Experiments are well-conducted . Cons : - It is not clear to me why the proposed pruning method is better than other pruning methods such as DistilBERT ( Sanh et al. , 2019 ) and RoBERTa ( Liu et al .2019 ) .- It is unclear to me how the proposed method can be applied to other objectives such as FLOPs or latency . - It would be better if the authors can provide more details about the hyperparameters used in the pruning process . For example , how many parameters are used in each pruning step ?
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensembling from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim , Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experimental results are not convincing . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a reinforcement learning based framework for the task of video grounding , which aims to temporally localize a natural language description in a video . To alleviate this problem , the authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . The paper is well written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The main contribution of this paper is to formulate the video grounding problem as a sequential decision-making problem , which is similar to the problem of video retrieval , but it is not clear to me how this problem can be solved . 2 .The experimental results are not convincing . The performance of this method is not better than the state of the art methods . For example , the results in Table 1 and Table 2 are not comparable . 3 .The proposed method does not outperform the baselines . 4 .The experiments are not well designed . For instance , in Table 2 , the performance of the baseline methods are not compared .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors tackle the problem of finding a projective plane of order 10 from projective geometry . The authors translate the problem into Boolean logic and use satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of the paper is to use SAT solvers instead of special-purpose search code to solve the problem . The reason is that using well-tested SAT solver is less error-prone than writing custom-written code . But the paper does not provide a formal proof of the correctness of the proposed SAT encoding . 2 .The authors claim that the proposed method can be used to solve large satisfiability instances when the cubing solver can generate many cubes encoding subproblems of approximately equal difficulty . But it is not clear to me how the authors can use the SAT encoding to solve such large instances . 3 .The paper is not well organized . For example , it is hard to follow the description of the algorithm . 4 .It is unclear to me what is the purpose of using the SAT encoder in the paper . It seems that the authors only use it to solve a small satisfiability instance .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) to capture different graph structures . The main contribution of the paper is to analyze the discriminative power of popular GNN variants , such as Graph Convolutional Networks and GraphSAGE , and show that they can not learn to distinguish certain simple graph structures , and develop a simple architecture that is provably the most expressive among the class of GNN and is as powerful as the Weisfeiler-Lehman graph isomorphism test . The paper is well-written and easy to follow . However , I have the following concerns : 1 . In the abstract , the authors claim that the most powerful GNN , i.e. , Graph Isomorphism Network ( GIN ) , also empirically has high representational power as it almost perfectly fits the training data , whereas the less powerful models often severely underfit the data . But , in the experiments , the GIN only outperforms the state-of-the-art results on a number of graph classification benchmarks , which is not convincing . 2 .In the experimental results , the performance of the proposed GIN is not better than the other GNN models . 3 .The experimental results are not convincing enough . In Table 1 , the results of GIN and GCN are not comparable . 4 .In Table 2 , the result of GCN and GIN are not compared .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low precision SGD iterates with a modified learning rate schedule . The authors show that it can match the performance of full-precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The paper is well-written and easy to follow . Theoretical analysis shows that it converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than SGD in strongly convex settings . Empirical results show that training with 8-bit SWALp can match full precision training with SGD baseline in deep learning tasks such as training Preactivation ResNet-164 ( He et al. , 2016 ) on Cifar-10 and CIFAR-100 datasets . This paper proposes a principled approach using stochastic weight averaging while quantizing all numbers including gradient accumulator and the velocity vectors during training . By averaging we find centred ar X iv :1 90 4.4.1 . Theorem 1 is already guaranteed to apply to all iterates and this iterates for all subsequent iterates . In particular , it is guaranteed for this iterate to apply for all iterations . In the proof of Lemma 1 , the authors use a method similar to the method in HALP ( De Sa et al .2018 ) to analyze the dynamics of the SWA update . It would be better if the authors can explain the difference between HALP and SWA .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that uses both prior and posterior distributions over knowledge to facilitate knowledge selection . The paper is well-written and easy to follow . The idea of using the posterior distribution over knowledge is interesting and novel . However , I have the following concerns : 1 . In the prior knowledge module , the prior distribution p ( k|x ) is inferred from both the utterance and the response , while the posterior p ( y|x , y ) is only inferred from the responses . It is not clear to me how the prior p ( x ) can be used to select appropriate knowledge . 2 .In the inference process , the authors use the Kullback-Leibler divergence loss ( KLDivLoss ) to measure the distance between the prior and the posterior distributions . It seems that the authors do not use the KL divergence between the posterior and the prior . 3 .The authors should compare the performance of the proposed model with the baselines on the Persona dataset and Wizard-of-Wikipedia dataset . 4 .The proposed model is not compared with the baseline methods in the literature .
1_2005.06628 = This paper proposes a pruning-based approach to reduce the number of parameters in the BERT language model . The main idea is to prune the architecture of the encoder and the attention head of the Transformer . The pruning is done by optimizing the architecture design dimensions of each layer of the model , and the pruned parameters are then used to train the model in a pre-trained fashion . The authors show that the proposed method can achieve 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the model is interesting . 3 .The proposed pruning method is simple and effective . Cons : 1.1 . It is not clear why the authors chose to optimize the design dimensions in the pruning framework rather than launching a pretraining job for each of these choices . 2.2 .The authors should compare the performance of the proposed pruned model with other pruning methods such as RoBERTa and DistilBERT . 3.3 .The experiments are only conducted on one language modeling task ( GLUE ) . It would be better if the authors can conduct more experiments on other language modeling tasks such as NLP tasks .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experimental results are not convincing . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a method for video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a sequential decision making problem and propose a reinforcement learning based framework to regulate the temporal grounding boundaries progressively based on its policy . The proposed method achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset and Charades-STA dataset while observing only 10 or less clips per video . Pros : 1 . The paper is well written and easy to follow . 2 .The proposed method is novel in that it successfully combines supervised learning with reinforcement learning in a multi-task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The experimental results show that the proposed method outperforms the baselines . Cons : 1.1 . The novelty of this paper is limited . The main contribution of the paper is to formulate the task of video grounding as a problem of sequential decision-making problem . However , it is not clear to me how this problem can be solved by the proposed approach . 2.2 .The novelty of the proposed model is limited to the observation network that takes visual and textual information as well as current temporal boundaries into consideration . 3.3 .The experiments are not convincing enough .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) to capture different graph structures . The main contribution of the paper is to analyze the discriminative power of popular GNN variants , such as Graph Convolutional Networks and GraphSAGE , and show that they can not learn to distinguish certain simple graph structures , and develop a simple architecture that is provably the most expressive among the class of GNN and is as powerful as the Weisfeiler-Lehman graph isomorphism test . The paper is well-written and easy to follow . However , I have the following concerns : 1 . In the abstract , the authors claim that the most powerful GNN , i.e. , Graph Isomorphism Network ( GIN ) , also empirically has high representational power as it almost perfectly fits the training data , whereas the less powerful models often severely underfit the data . But , in the experiments , the GIN only outperforms the state-of-the-art results on a number of graph classification benchmarks , which is not convincing . 2 .In the experimental results , the performance of the proposed GIN is not better than the other GNN models . 3 .The experimental results are not convincing enough . In Table 1 , the results of GIN and GCN are not comparable . 4 .In Table 2 , the result of GCN and GIN are not compared .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The authors show that the proposed method can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The paper is well written and easy to follow . The proposed method is simple to implement and the theoretical analysis is sound . However , I have the following concerns : 1 . In the experiments , it is not clear how to choose the hyperparameters for the experiments . For example , how to tune the learning rate ? 2 .In the experiment , it would be better if the authors can provide more details about the implementation details . For instance , how many iterations are used in the experiments ? 3 .The authors should compare with HALP , which is a variant of SGD that uses low precision numbers . 4 .In Section 4.2 , the authors claim that SWA converges arbitrarily close to the optimal solution for quadratic objectives , and to a noise ball asymptotically smaller than SGD in strongly convex settings . It would be more convincing if the author can provide some theoretical analysis on the convergence of SWA .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation with knowledge-grounded responses . The authors propose a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . The paper is well-written and easy to follow . However , I have the following concerns . 1 .The authors claim that their model is the first neural model which incorporates the posterior distribution as guidance , enabling accurate knowledge lookups and high quality response generation . But , it is not clear to me how this is achieved . For example , the prior distribution p ( k|x ) is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution is inferred only from utterances only , so that appropriate knowledge can be selected even without responses during the inference process . Compared with the previous work , our model can better incorporate appropriate knowledge in response generation , which is better than previous work . 2 .In Table 1 , the authors compare the performance of their model with Seq2Seq model with a GRU decoder where knowledge is concatenated . It would be better if the authors can show the results of different ways of incorporating knowledge incorporating knowledge . 3 .It is better to show the effect of different loss functions in Table 1 .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT model . The main idea is to prune the parameters of each layer of the model by 5 different dimensions , and then pre-train multiple variants of BERT with different values chosen for these dimensions by applying pruning-based architecture search technique . Experiments on GLUE and SQuAD datasets show that the proposed method achieves 6.6 % higher average accuracy compared to BERT without three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the model parameters is interesting . 3 .The proposed method is simple and effective . 4 .The experimental results are convincing . Cons : 1 ) It is not clear to me why the proposed pruning method is better than other pruning methods such as RoBERTa ( Liu et al. , 2019 ) and DistilBERT ( Sanh et al .2019 ) . 2 ) The proposed method can be applied to other objectives such as FLOPs or latency . 3 ) The experiments are limited to GLUE dataset .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with FewS better capture rare senses in existing WSD datasets . Finally , the authors find humans outperform the best baseline models on the dataset . The paper is well-written and easy to follow . The contribution of the paper is clear and the dataset is novel . However , I have the following concerns . 1 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , the current benchmark for WSD evaluation , SemCor , only contains 7.96 unique zero-shot and 761 unique few-shot senses ( where the sense is seen three or fewer times in the SemCor training set across development and test sets ) . It would be better if the authors can provide more details about the training and evaluation set . 2 .In Table 1 , the performance of the biencoder is not compared with human annotators . It is better to compare the performance with the state-of-the-art baselines . 3 .It is not clear why the authors did not compare with other datasets such as WSD Evaluation Framework ( Raganato et al. , 2017 ) and SemCor . 4 .It would be more convincing if the author can provide the number of senses in the training set and the evaluation set in Table 1 .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper is technically sound . Cons : 1.1 . The novelty of the proposed method is limited . 2.2 .The experimental results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .