1_2012.04715 = This paper proposes a SAT solver to solve the projective plane of order 10 problem , which is the most challenging subcase of Lamâs problem . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates were generated with the assistance of the symbolic computation library Traces ( McKay and Piperno 2014 ) , but the authors describe how one can verify the certificate without needing to trust the output of the library . The paper is well written and easy to follow . However , I have the following concerns : 1 . It is not clear to me how to verify the correctness of the certificates generated by the Traces library . For example , how to check whether the certificates are valid or not ? 2 .In the proof of Theorem 3.1 , the authors claim that the certificate verifier is more verifiable because it can check the certificates for themselves and ( once they believe in the encoding ) be convinced in the nonexistence of a projective planes of order ten without having to trust a search procedure . But I am not sure if this is the case . 3 .The proof of theorem 3.2 is not correct . The proof is based on the fact that there is no proof of the existence of the plane of the order of 10 . 4 .In Theorem 4.2 , it is claimed that there are 66 possible A1s and A2s , but there are only 66 possibilities for the A1 case . I am wondering if the authors can provide more details on how to choose the number of possibilities . 5 .I am not convinced about Theorem 5.1 . It seems that the proof is only valid for the case of A2 and A1 , but not for the other two cases . 6 .In theorem 5.3 , it seems that it is not possible to prove that the certificates of the certificate are valid .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and the Weisfeiler-Lehman graph isomorphism test ( WL ) test . The authors show that GNN can be as powerful as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to study the power of different GNN variants in learning to represent and distinguish between different graph structures . The key insight is that a GNN has as large discriminative power as WL if the aggregation scheme can be highly expressive . The analysis is based on the assumption that the set of feature vectors of a given node 's neighbors is represented as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNN is thought of as an aggregation function over the multisets . Hence , to have strong representational power , the aggregation function must be able to aggregate different multi-sets into different representations . 2 .Theorem 3.1 shows that GIN is as powerful in distinguishing different graphs as the graph isomorphic test . But , it is not clear why GIN can not distinguish the graph structures such as GCNSAGE and GraphSAGE . 3 .The experimental results are not convincing . In Table 1 , the performance of GNN-GIN is not better than WL-WL test . It is hard to see the advantage of GIN . 4 .In Table 2 , the results of GCN-SAGE are not comparable to the results in Table 1 . 5 .In the experiments , the number of parameters in GIN seems to be too small . It would be better if the authors can provide more details .
1_1907.07355 = This paper investigates the surprising performance of BERT on the argument reasoning mining task . The authors show that BERT exploits the presence of cue words in the warrant , especially “ not ” . They show that the major problem can be eliminated in ARCT by adding a copy of each data point with the claim negated and the label inverted . This means that the distribution of statistical cues in the warrants will be mirrored over both labels , eliminating the signal . On this adversarial dataset all models perform randomly , with BERT achieving a maximum test set accuracy of 53 % . This paper provides a more robust evaluation of argument comprehension and should be adopted as the standard in future work on this dataset . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that “ BERT ’ s surprising performance can be entirely accounted for in terms of exploiting spurious statistical cues . ” However , it is not clear to me why this is the case . In Table 1 , BERT achieves 77 % accuracy with its best run , only three points below the average ( untrained ) human baseline . It is therefore surprising to me . 2 .In Table 2 , why BERT performs better than the unsupervised baseline , which is only 3 points below human baseline ?
1_1904.11943 = This paper proposes a new method for low-precision training of deep neural networks . The proposed method is based on stochastic weight averaging ( SWALP ) , where all the numbers except the gradient accumulator are quantized to 8 bits without changing the network structure . The authors prove that SWA converges to the optimal solution for quadratic objectives with no loss of accuracy from the quantization noise . For strongly convex objectives , the proposed method converges a smaller asymptotic noise ball than that of low precision SGD . The paper is well written and easy to follow . The method is simple to implement and has little computational overhead . The experimental results show that 8-bit SWA can match the full precision training of SGD on CIFAR-10/100 and VGG-16 with both VGG and PreResNet-164 datasets .
1_1902.04911 = This paper proposes a novel approach for dialogue generation based on knowledge-grounded generative models . The authors propose to separate the prior distribution over knowledge and the posterior distribution over responses . The posterior distribution is inferred from both the utterance and the response , while the prior is inferred only from the response . The model is trained to minimize the KL divergence between the prior and posterior distributions so that the model can approximate the posterior . Then , during the inference process , the model samples knowledge merely based on the prior distributions and incorporates the sampled knowledge into response generation . The paper is well written and easy to follow . The proposed approach is novel and the experiments are convincing . However , I have some concerns about the novelty of the proposed approach . 1 .The proposed approach seems to be a straightforward combination of existing approaches . It is not clear to me why the authors chose to use the knowledge graph as the prior instead of the ground-truth knowledge . 2 .In the experiments , the authors only compare with the baselines on the Wizard-of-Wikipedia dataset . It would be interesting to see the performance of the approach on the Persona-chat dataset as well . 3 .It would be good to see how the approach compares with other approaches such as the commonsense model proposed by Dinan et al . [ ? ] .4 .The authors claim that the approach is able to generate more informative responses by incorporating knowledge into the response generation , but it is not shown in the experiments .
1_2005.06628 = This paper proposes a pruning-based architecture search method to improve the performance of the BERT model by pruning the number of parameters and reducing the size of the architecture design dimensions . The pruning method is based on the idea of pruning each layer of BERT by five different dimensions , instead of two . The authors show that the ratio of the pruning dimensions within a BERT encoder layer can be modified to obtain a layer with better perfor- Design dimensions . This paper is well-written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me why the authors chose to prune the design dimensions instead of parameter sharing across layers and factorizing the embedding layers . 2 .The pruning based architecture search technique is only applied to the pre-training loss and not to the model parameters . It would be better if the authors can show the results on the training loss and model parameters in the experiments . 3 .The authors should compare with the results of ALBERT ( https : //arxiv.org/abs/1711.06570 ) and RoBERTa ( http : //proceedings.mlr.press/v48/roberta.pdf ) . 4 .In the experiments , the authors only compare the performance on MNLI and MRPC SST-2 , which are not state-of-the-art . The results on SQuAD v1.1 and v2.0 should be reported .
1_2102.07983 = This paper introduces a new dataset , FEWS ( Few-shot Examples of Word Senses ) , which is used to train and evaluate Word Sense Disambiguation ( WSD ) models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary definitions of 71,000 senses , which are used as training data for WSD models . The paper is well-written and easy to follow . The idea of using Wikipedia definitions of senses for training and evaluation of WSD is interesting , and the dataset is a good addition to the literature . However , I have the following concerns : 1 . It is not clear to me why the authors did not compare with existing datasets such as SemCor ( SemCor is the largest manually annotated WSD dataset ) . 2 .The authors claim that the proposed dataset is better than SemCor because it has more senses , but it is unclear to me how it compares with SemCor in terms of the number of senses . 3 .It is also not clear how the authors compare the performance of the proposed datasets with the state-of-the-art baselines such as the biencoder and knowledge-based approaches . 4 .The transfer learning experiments are not convincing . The transfer learning results are not very convincing . For example , in Table 2 , the transfer learning performance of BienCoder is not better than human annotators , while in Table 3 , it is worse than human . It would be better if the authors can provide more explanations .
1_1812.02425 = This paper proposes a method for ensembling multiple neural networks with no additional testing cost . The method is based on the teacher-student learning paradigm , where the teacher is a pre-trained network and the student is a student network trained with adversarial training . The teacher network is trained to predict the output of the student network . The student network is also trained to generate similar outputs as the teachers . The proposed method is evaluated on CIFAR-10/100 , SVHN , and ImageNet . The results show that the proposed method consistently improves the accuracy across a variety of popular network architectures on different datasets . The paper is well-written and easy to follow . However , I have some concerns about the novelty of the method . 1 .The proposed method uses soft labels instead of one-hot vector labels . It would be better if the authors can provide more explanation about why soft labels are better than the traditional hot labels . 2 .The authors claim that the soft labels provide more coverage for co-occurring and visually related objects and scenes . It is better to provide more analysis about the effect of soft labels on the performance . 3 .In the experiments , the authors only compare the performance of the teacher and student networks . It will be better to show the results of the other networks as well . 4 .In Table 1 , the results are not consistent with the results in Table 1 .
1_1901.06829 = This paper proposes a novel method for grounding natural language descriptions in videos . The proposed method is based on an observation network that takes visual and textual information as well as current temporal boundaries into consideration . Given the observation , the decision making policy is modeled by a policy network that outputs the probability distribution over all possible actions , and then adjusts the grounding boundary according to the policy . The policy network is implemented using a recurrent neural network in order to leverage the feature ground truth for temporal boundary regression as well at each step . Experimental results show that the proposed method achieves state-of-the-art performance on two well-known datasets and validate the superiority of the proposed model . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of this paper is limited . The main contribution of this work is the observation network and the policy network , which are not novel . 2 .The proposed method can be trained end-to-end for natural language grounding within several steps of glimpses ( observations ) and temporal boundaries ( actions ) . 3 .The experimental results are not convincing . In Table 1 , the performance of Charades and ActivityNet DenseCaption is not better than the baseline method . 4 .In Table 2 , the results of the multi-task learning is not clear . It is better to show the results for the multi task learning . 5 .In Figure 3 , it is hard to see how the performance is improved by the use of the location feature . It would be better to include the results with the results from the original Charades dataset .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . There are two different preference sets for agents . One is strict , which is a full ordinal list of all objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors mainly consider Object Reachability and Pareto Efficiency under different preferences . They proved that it is NP-hard to find a reachable minimum of the individual utilities in complete graphs and star-structures , and polynomial-time solvable under strict preferences under the weak preferences . Furthermore , they prove that Weak Object Reachable ( Object reachability ) under the strong preferences is polynomially solvable in a path under the strict preferences . The paper is well-written and easy to follow . However , I have the following concerns : 1 . In the paper , it is assumed that the agents are embedded in a social network , which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network and they find it mutually profitable ( or no one will become worse under weak preferences ) , and they can make some exchanges . In some models , some agents have a tie with others such that they know each other , and can exchange , but they can not exchange , even if they can mutually get benefits . 2 .It is not clear to me how to solve the problem of finding the optimal assignment within all the reachable assignments . 3 .The paper is not well-organized . For example , in the abstract , the first sentence of the first paragraph of the abstract says that the paper studies housing market in social network with simple trades between pairs of neighbours in the network . But in the introduction , it says that this model is a hot topic to study resource allocation problems over social networks and analyze the influence of networks . 4 .In the second paragraph of Section 3.1 , the paper says that `` In this model , there are the same number of agents and objects and each agents is initially endowed with a single object . Each agent has a preference list of objects . `` However , in real life , there can be more than two agents and there are more than three objects . 5 .In section 3.2 , the sentence `` In some cases , it implicitly assumed that all agents have ties with each other such that each agent could only be assigned a connected component . `` should be removed . 6 .In Theorem 1 , the statement of Theorem 2.1 is not correct . The statement of Proposition 1.1 should be replaced by Proposition 2.2 . 7 .In Algorithm 1 , there is a typo in the last line .
1_2012.04715 = This paper proposes a new SAT solver to solve the problem of determining the existence of a projective plane of order 10 . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates were generated with the assistance of the symbolic computation library Traces ( McKay and Piperno 2014 ) , but the authors describe how one can verify the certificate without needing to trust the output of the library . The paper is well written and easy to follow . However , I have the following concerns : 1 . It is not clear to me how to verify the correctness of the certificates generated by the Traces library . For example , how to check whether the certificates are valid ? 2 .The authors claim that the certificate is more verifiable because it is more reliable than writing special-purpose search code , but I do not see how the certificate verifier can verify it . 3 .In the proof of Theorem 3.1 , the authors state that the proof is based on the fact that the code associated with a hypothetical projective planes of order ten must contain that are weight 16 , weight 19 , or `` primitive `` weight 19 words . I am not sure if this is correct . 4 .The proof of theorem 3.2 is not correct . The authors should check the proof in the supplementary material . 5 .In Theorem 4.2 , the proof should be more precise . 6 .In theorem 4.3 , the definition of the number of points in the plane is not consistent with the definition in Theorem 2.1 .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and shows that they are as powerful as the Weisfeiler-Lehman ( WL ) graph isomorphism test . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of the paper is to show that GNN can be as expressive as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . The key insight is that a GNN has as large discriminative power as WL if its aggregation scheme can model an injective function . However the paper does not provide any theoretical analysis on this . 2 .The experimental results are not convincing . The authors only compare the performance of GNN with the power of WL and GCNSAGE . It would be better if the authors can compare with other GNN-based models such as Graph Isomorphism Network ( GIN ) . 3 .The proposed GIN is not compared with the state-of-the-art GNN models . It is better to compare GIN with other graph neural network models . 4 .In the experiments , the authors should compare with the results of other graph classification methods such as GAN .
1_1907.07355 = This paper investigates the performance of BERT on the argument reasoning mining task . The authors show that BERT is able to exploit the presence of cue words in the warrant , especially “ not ” . They show that this can be eliminated in ARCT by adding a copy of each data point with the claim negated and the label inverted . This means that the distribution of statistical cues in the warrants will be mirrored over both labels , eliminating the signal . On this adversarial dataset all models perform randomly , with BERT achieving a maximum test set accuracy of 53 % . The paper is well-written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me how to interpret the results in Table 1 . In Table 1 , BERT achieves 77 % accuracy with its best run , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question : what has BERT learned about argument comprehension ? To investigate BERT ’ s decision making , the authors looked at data points it finds easy to classify over multiple runs . 2 .In Table 2 , the results are not consistent with the results of Habernal et al . ( 2018b ) , which shows that the same analysis with the SemEval submissions , and consistent with their results . 3 .It is also not clear why BERT exploits “ cue words ” in the ARCT task . 4 .The authors claim that “ In ARCT , we show that the major problem can be entirely accounted for in terms of exploiting spurious statistical cues . ” However , it is unclear to me what is the “ major problem ” of ARCT .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The paper is well written and easy to follow . The proposed method is simple to implement and has little computational overhead . For quadratic objective , the proposed method converges to the optimal solution with no loss of accuracy from the quantization . For strongly convex objective , it can converge to a smaller asymptotic noise ball than SGD for low precision training . Empirical results show that training with 8-bit SWA can match the full precision SGD baseline in deep learning tasks such as training ResNet and CIFAR-10 and CKrizhevsky & HIF-100 datasets . I think this paper makes the following contributions : 1 . It shows that SWA is able to reduce the performance gap between low precision and full precision training and shows that it can significantly reduce the training time between the two . 2 .It shows that the proposed SWA method can achieve better generalization performance compared to the state-of-the-art methods . 3 .The proposed method can also reduce the computational cost compared to SGD . 4 .The experimental results are convincing .
1_1902.04911 = This paper proposes a method for knowledge-grounded dialogue generation . The authors propose to use knowledge selection mechanism to separate the prior distribution over knowledge from the posterior distribution over the response distribution . The model is trained to minimize the distance between the prior and posterior distributions . Then , the model samples knowledge based on the prior prior distribution and incorporates the sampled knowledge into response generation . Experiments are conducted on Persona-chat and Wizard of Wikipedia dataset . The paper is well written and easy to follow . The idea of knowledge selection is interesting and novel . However , I have some concerns about the experiments . 1 .The authors claim that the proposed method is better than the baselines . But , there is no comparison with baselines in Table 1 . It would be better if the authors can show the performance of baselines on the same dataset . 2 .In Table 1 , the authors should show the results of the model without knowledge selection . It is better to compare with the model with knowledge selection and model without prior distribution . 3 .It is better if authors can provide more details about the training procedure . For example , how many iterations are used in the training process ? 4 .In the experiments , it would be more convincing if the author can show results on more datasets . 5 .The author should provide more explanations about the results in Table 2 .
1_2005.06628 = This paper proposes a pruning-based architecture search method to improve the performance of the BERT model by pruning the number of parameters and reducing the size of the architecture design dimensions . The pruning method is based on the idea of pruning each layer of BERT by five different dimensions , instead of two . The authors show that the ratio of the pruning dimensions within a BERT encoder layer can be modified to obtain a layer with better perfor- Design dimensions . This paper is well-written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me why the authors chose to prune the design dimensions instead of parameter sharing across layers and factorizing the embedding layers . 2 .The pruning based architecture search technique is only applied to the pre-training loss and not to the model parameters . It would be better if the authors can show the results on the training loss and model parameters in the experiments . 3 .The authors should compare with the results of ALBERT ( https : //arxiv.org/abs/1711.06570 ) and RoBERTa ( http : //proceedings.mlr.press/v48/roberta.pdf ) . 4 .In the experiments , the authors only compare the performance on MNLI and MRPC SST-2 , which are not state-of-the-art . The results on SQuAD v1.1 and v2.0 should be reported .
1_2102.07983 = This paper introduces a new dataset called FEWS ( Few-shot Examples of Word Senses ) , which is a dataset for evaluating Word Sense Disambiguation ( WSD ) models in few-shot and zero-shot settings . The dataset is built on top of the Wiktionary dataset , which contains 71,000 senses . The authors evaluate the performance of different knowledge-based approaches and a recent neural biencoder model for WSD on the new dataset . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset to evaluate WSD models in the low-shot setting , but it is not clear to me how significant this contribution is . 2 .The evaluation of the proposed dataset is only conducted on a few senses . It would be interesting to see the performance on other senses . 3 .It would be good to see how the proposed datasets can be used for transfer learning . 4 .The paper is not well-organized . There are many typos and grammatical errors in the paper .
1_1812.02425 = Summary : This paper proposes a method to learn an ensemble of multiple neural networks without incurring any additional testing costs . The proposed method is based on the teacher-student learning paradigm for ensembling , where the teacher is a pre-trained network and the student is a student network trained with adversarial training . The paper is well written and easy to follow . Experiments show that the proposed method consistently improves the accuracy across a variety of popular network architectures on different datasets . Strengths : 1 . The method is simple and effective . 2 .The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , and ImageNet . 3 .The paper is easy to read . Weaknesses : 1 ) The novelty of this paper is limited . The idea of using adversarial learning to improve the robustness of the student network is not new . 2 ) The paper does not compare with other methods . 3 ) The experiments are not convincing . 4 ) It is not clear how to choose the hyper-parameters of the adversarial loss . 5 ) The experimental results are not sufficient .
1_1901.06829 = This paper proposes an end-to-end reinforcement learning ( RL ) based framework for grounding natural language descriptions in videos . The paper is well-written and easy to follow . The proposed method is novel and well-motivated . However , I have the following concerns : 1 . The novelty of the method is limited . The method is a simple combination of existing methods . For example , the proposed method can not be compared with the state-of-the-art methods for action localization and video grounding . 2 .The method is not compared with other methods for video grounding , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) .3 .The proposed method does not outperform the baselines . 4 .The experimental results are not convincing . In Table 1 , the performance of the proposed methods is not better than the baseline methods . The authors should compare the performance with the baseline methods . 5 .In Table 2 , the authors should show the results of the baseline method .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . There are two different preference sets for agents . One is strict , which is a full ordinal list of all objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors studied the Object Reachability ( OR ) and Pareto Efficiency ( PE ) problems in this setting . The authors proved that the problem is polynomial-time solvable under star-structures and NP-complete under tree structures . They also proved that Object Reachable in a path under strict preferences , and that Object reachability in weak preferences is NP-hard in general graphs . The paper is well-written and easy to follow . However , I have the following concerns : 1 . In the paper , it is assumed that the agents are embedded in a social network to denote the ability to exchange objects between pairs of neighbors in the network . This assumption is too strong , and it is not clear to me how this assumption can be relaxed in real-world applications . For example , in the real world , there are many real-life applications such as allocation of housings , organ exchange , and so on . 2 .The paper is not well-motivated . It is hard to understand the motivation of the paper . It seems that the authors are trying to show that under the assumption of the social network , the problem can be solved in polynomially time in trees and complete graphs , but it is difficult to understand why this assumption is necessary . 3 .It is also not clear why the authors assume that all agents have a tie with each other such that they know each other , and can make some exchanges . In some models , some agents often do not know each others , and cannot exchange , even if they can mutually get benefits . 4 .The authors should compare their results with existing results in the literature .
1_2012.04715 = This paper proposes a SAT solver to solve the projective plane of order 10 problem , which is the most challenging subcase of Lam 's problem . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates are generated with the assistance of the symbolic computation library Traces . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that the certificates are more verifiable , but they do not provide a formal proof of the nonexistence of projective planes of order ten because they rely on results that currently have no formal computerverifiable proofs . 2 .It is not clear to me how the certificates can be verified by a third party . 3 .In the proof of Theorem 3.1 , the authors use a special-purpose solver , but I do not know how to verify the correctness of the certificates . 4 .The proof of theorem 3.2 is not correct . The proof is based on the fact that the error correcting code must contain words that are called `` primitive `` or `` weight `` 19 words , which are the first two cases that are first ruled out by the search of ( MacWilliams , Slane , and Thompson 1973 ) and were recently settled via SAT-based nonexistence certificates ( Bright et al. , 2020a ,b ) . 5 .In Theorem 4.2 , the proof is not complete . It is not possible to prove the existence of the plane .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and shows that they are at most as powerful as the Weisfeiler-Lehman ( WL ) graph isomorphism test in distinguishing graph structures . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of feature vectors of a given node ‘ s neighbors as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNNS can be thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . 2 .The paper also shows that the most powerful GNN by our theory , ie., Graph Isomorphism Network ( GIN ) , also empirically has high representation power as it fits the data perfectly . 3 .The experimental results show that the proposed GIN can not distinguish graph structures that can not be distinguished by popular GNN variants , GCN ( Kipf & Welling , 2017 ) and GraphSA ( Hamilton et al. , 2017a ) and GIN ( Velickovic et .al. , 2018 ) . 4 .In the experiments , the authors compare the performance of GIN with various aggregation functions with various graph aggregation functions . It would be better if the authors can compare the results with the results of GNN with different aggregation functions and graph-level pooling schemes .
1_1907.07355 = This paper investigates the performance of BERT on the argument reasoning mining task . The authors show that BERT is able to exploit the presence of cue words in the warrant , especially “ not ” . They show that this can be eliminated in ARCT by adding a copy of each data point with the claim negated and the label inverted . This means that the distribution of statistical cues in the warrants will be mirrored over both labels , eliminating the signal . The adversarial dataset provides a more robust evaluation of argument comprehension and should be adopted as the standard in future work on this dataset . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that “ BERT achieves 77 % test set accuracy with its best run ( Table 1 ) , only three points below the average ( untrained ) human baseline . ” However , it is not clear to me why this is the case . In Table 1 , the human baseline is only 3 points below BERT ’ s performance . 2 .In Table 2 , the authors claim “ The ARCT SemEval shared task ( Habernal et al. , 2018b ) verified the challenging nature of this task ” , but it is unclear to me how this is verified . 3 .In Figure 3 , it seems that the authors did not compare BERT with the state-of-the-art methods on the ARCT dataset . 4 .In Section 4.2 , it would be better to show the results of the BERT baseline on the same dataset . 5 .It would be more convincing if the authors can provide more details about the dataset .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The authors prove that for quadratic and strongly convex objectives , SWA converges to a noise ball that is asymptotically smaller than that of low precision SGD . They also show that their method can significantly reduce the performance gap between low precision and full precision training . This paper is well-written and easy to follow . The idea of using SWA to reduce the quantization noise during training is interesting and novel . However , I have the following concerns : 1 . The paper is not well-motivated . The motivation of SWA is not clear . In the introduction , the authors claim that SWA takes an average of SGD iterates with a modified learning rate schedule and has been shown to lead to wider optima ( Izmailov et al. , 2018a ) . But in the experiments , it seems that the authors only compare SWA with SGD with a low learning rate . 2 .The experiments are not convincing . In Table 1 , the performance of 8-bit SWA does not match the full precision baseline . 3 .In Table 2 , the accuracy of the proposed method is not better than the baseline . 4 .The authors should provide more details about the implementation details . For example , how to choose the number of bits in the quantized grid ? 5 .In the experiment , it is better to compare the performance with the state-of-the-art methods .
1_1902.04911 = This paper proposes a method for knowledge-grounded dialogue generation . The authors propose to use knowledge selection as a prior distribution over the posterior distribution over knowledge , which is inferred from both the utterance and the response . The model is trained to minimize the KL divergence between the prior and posterior distributions . Then , the model samples knowledge from the prior distribution and incorporates the sampled knowledge into response generation . Experiments are conducted on the Persona-chat dataset and Wizardof-Wikipedia dataset to demonstrate the effectiveness of the proposed method . The paper is well-written and easy to follow . The proposed method is novel and interesting . However , I have the following concerns . 1 .The proposed method seems to be very similar to [ Dinan et al. , 2018 ] , which also uses knowledge selection to guide the knowledge selection . The only difference is that the authors use the knowledge distribution instead of the semantic similarity between the utterances and the responses . 2 .It is not clear to me why the authors chose to use the proposed knowledge selection method instead of using the ground-truth knowledge . 3 .The authors should compare the performance of their method with the baselines in Table 1 and Table 2 . It would be better if the authors can provide more details about the baseline methods . 4 .In Table 2 , the authors should provide more information about the number of parameters of the model . For example , how many parameters are used in the model ? 5 .In the experiments , it would be more convincing if the author can show the results on more datasets .
1_2005.06628 = This paper proposes a pruning-based architecture search method for reducing the number of parameters in the Transformer-based BERT model . The paper is well-written and easy to follow . The experiments are well-conducted and the results are convincing . However , I have the following concerns : 1 . The authors claim that the top layers of BERT are more important than the bottom layers , but they do not provide any evidence to support this claim . For example , in Table 1 , it is shown that the fully connected component applied to each token separately plays a much more significant role in top layers as compared to bottom layers . 2 .In Table 2 , the performance of the pruned model is not significantly better than the original model . It would be better if the authors can provide some explanation why this is the case . 3 .It is not clear to me how the pruning based architecture search can be applied to BERT . For instance , the authors should provide more details on how to prune the size of the attention heads .
1_2102.07983 = This paper introduces a new dataset called FEWS ( Few-shot Examples of Word Senses ) , which is a dataset for evaluating Word Sense Disambiguation ( WSD ) models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary definitions of 71,000 senses , which are used as training data for WSD models . The authors also conduct experiments to compare the performance of knowledge-based approaches and a recent neural biencoder model on the new dataset . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset to evaluate the WSD model in the few-and-zero-shot setting , but it is not clear to me how significant this contribution is . 2 .The experimental results are not convincing . For example , in Table 1 , the results of the biencoders are not compared to human annotators , which makes it hard to judge the significance of the results . 3 .The transfer learning experiments are not well-conducted . It would be better to conduct the transfer learning experiment to see if the proposed dataset can be used as additional training data to improve the performance on other WSD tasks .
1_1812.02425 = This paper proposes a method for ensembling multiple neural networks with no additional testing cost . The idea is to use the outputs of different neural networks as supervisions to guide the target network training . To further improve the robustness of student networks , the authors introduce an adversarial learning strategy to force the student to generate similar outputs as teachers . The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a variety of existing network architectures . The paper is well-written and easy to follow . The experiments show that MEAL consistently improves the accuracy across several popular network architectures on different datasets . However , I have the following concerns . 1 .The proposed method is based on the teacher-student learning paradigm . The teacher and student networks are the reference networks and the target networks are called teachers and the *Equal contribution . It is not clear to me why the soft labels are used instead of the traditional one-hot vector labels . 2 .The soft labels should be informative for the specific image . In other words , the labels should not be identical for all the given images with the same class . It can also be observed that soft labels can provide the additional intra- and inter-category relations of datasets . 3 .It is better to compare the performance of the proposed method with other ensembles , such as dropout , dropconnect , stochastic depth , and swapout .
1_1901.06829 = This paper proposes an end-to-end reinforcement learning framework for grounding natural language descriptions in videos . Specifically , the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . The agent makes an observation that details the currently selected video clip , and takes an action according to its policy , and the environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . The paper is well-written and easy to follow . The proposed method is novel and interesting . However , I have the following concerns : 1 . The novelty of this paper is limited . It is not clear how the proposed method can be applied to other video grounding tasks such as video retrieval and text-oriented video highlight detection . 2 .The experiments are not convincing . The performance of the proposed model is not better than the state-of-the-art baselines . 3 .The proposed method does not outperform the baselines in Table 1 and Table 2 . 4 .The paper is not well-organized . For example , there are many typos and grammatical errors .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . There are two different preference sets for agents . One is strict , which is a full ordinal list of all objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors study the problem of Object Reachability and Pareto Efficiency under different preferences . The authors prove that under strict preferences , the problem is solvable in polynomial time , and under weak preferences , they prove that the problem can be solved in a path-based algorithm . The paper is well-written and easy to follow . The problem studied in this paper is interesting and the results are interesting . However , I have the following concerns : 1 . In the paper , it is assumed that the agents are embedded in a social network , which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network and they find it mutually profitable ( or no one will become worse under weak preference ) , and they can mutually get benefits . Under this model , many problems have been studied , i.e .object reachability , assignment reachability and pareto efficiency . 2 .It is not clear to me why the authors assume that all agents have a tie with each other such that they know each other , and can make some exchanges . In some models , some agents often do not know each each other and can not exchange , and even if they can get benefits , they can still not exchange . 3 .The paper is not well-organized . For example , in the abstract , it says that the paper studies housing market where the agent is embedded in social network to denote the ability of exchange between pairs of objects between them . But in the introduction , the paper says that this model is a hot topic to study resource allocation problems over social networks and analyze the influence of networks . In fact , recently it is also considered network-based Fair Division in allocating indivisible resources .
1_2012.04715 = This paper proposes a SAT solver to solve the projective plane of order 10 problem , which is the most challenging subcase of Lam 's problem . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates are generated with the assistance of the symbolic computation library Traces . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that the certificates are more verifiable , but they do not provide a formal proof of the nonexistence of projective planes of order ten because they rely on results that currently have no formal computerverifiable proofs . 2 .It is not clear to me how the certificates can be verified by a third party . 3 .In the proof of Theorem 3.1 , the authors use a special-purpose solver , but I do not know how to verify the correctness of the certificates . 4 .The proof of theorem 3.2 is not correct . The proof is based on the fact that the error correcting code must contain words that are called `` primitive `` or `` weight `` 19 words , which are the first two cases that are first ruled out by the search of ( MacWilliams , Slane , and Thompson 1973 ) and were recently settled via SAT-based nonexistence certificates ( Bright et al. , 2020a ,b ) . 5 .In Theorem 4.2 , the proof is not complete . It is not possible to prove the existence of the plane .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and the Weisfeiler-Lehman graph isomorphism test ( WL ) test . The authors show that GNN can be as powerful as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to study the power of different GNN variants in learning to represent and distinguish between different graph structures . The key insight is that a GNN has as large discriminative power as WL if the aggregation scheme can be highly expressive . The analysis is based on the assumption that the set of feature vectors of a given node 's neighbors is represented as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNN is thought of as an aggregation function over the multisets . Hence , to have strong representational power , the aggregation function must be able to aggregate different multi-sets into different representations . 2 .Theorem 3.1 shows that GIN is as powerful in distinguishing different graphs as the graph isomorphic test . But , it is not clear why GIN can not distinguish the graph structures such as GCNSAGE and GraphSAGE . 3 .The experimental results are not convincing . In Table 1 , the performance of GNN-GIN is not better than WL-WL test . It is hard to see the advantage of GIN . 4 .In Table 2 , the results of GCN-SAGE are not comparable to the results in Table 1 . 5 .In the experiments , the number of parameters in GIN seems to be too small . It would be better if the authors can provide more details .
1_1907.07355 = This paper presents an analysis of the performance of BERT on the argument reasoning mining task ( ARCT ) . The authors show that BERT is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated in ARCT by adding a copy of each data point with the claim negated and the label inverted . This means that the distribution of statistical cues in the warrants will be mirrored over both labels , eliminating the signal . On this adversarial dataset all models perform randomly , with BERT achieving a maximum test set accuracy of 53 % . This paper provides a more robust evaluation of argument comprehension and should be adopted as the standard in future work on this dataset . The paper is well written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me how to interpret the results in Table 1 . In Table 1 , BERT achieves 77 % accuracy with its best run , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question : what has BERT learned about argument comprehension ? To investigate BERT ’ s decision making , the authors looked at data points it finds easy to classify over multiple runs . 2 .In Table 2 , the results are not consistent with the results of Habernal et al . ( 2018b ) , which shows BERT exploits “ cue words ” in the ARCT SemEval submissions , and consistent with their results . It would be better if the authors can provide more explanation about this result .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) to reduce the performance gap between low-precision and full precision training in deep learning by quantizing all numbers except the gradient accumulator to 8 bits without changing the network structure . The proposed method is simple to implement and has little computational overhead . The experimental results show that the proposed method can match the full precision SGD baseline on Cifar-10 and CIFAR-100 with both VGG16 and VGG-1616 . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The motivation of the paper is not very clear . The authors claim that SWA has been shown to lead to wider optima and better generalization performance in the context of low precision training , but it is not clear to me why SWA can boost the performance of low precision training and that performance improvement is more significant than in the case of SWA applied to full precision . 2 .In the experiments , the authors only compare with the state-of-the-art results of Wang et al . ( 2018 ) . It would be better if the authors can compare with more recent results , e.g. , [ 1 ] , [ 2 ] and [ 3 ] . 3 .The authors should compare with other quantization methods such as [ 4 ] . 4 .The experiments are not convincing enough . For example , in Table 1 , it is better to compare with [ 5 ] . 5 .In Table 2 , the comparison with [ 6 ] is missing . 6 .In Section 4.2 , it would be more convincing if the author can show the results of [ 7 ] . 7 .In Figure 3 , it seems that the authors did not compare the results with [ 8 ] . It will be better to show the result of [ 9 ] .
1_1902.04911 = This paper proposes a method for dialogue generation based on knowledge-based models . The key idea is to separate the prior distribution over knowledge and the posterior distribution over responses . The prior distribution is inferred from both utterance and response , while the posterior is inferred based on the actual knowledge used in the response . The authors propose to minimize the KL divergence between the prior and posterior distributions during the training process . Then , during the inference process , the model samples knowledge without any posterior information ( i.e. , without any prior information ) and incorporates the sampled knowledge into response generation . Experiments are conducted on the Persona-chat dataset and Wizard of Wikipedia dataset . The paper is well-written and easy to follow . The proposed method is novel and interesting . However , I have the following concerns . 1 .The authors claim that the proposed method can generate more informative responses by utilizing appropriate knowledge . But , it is not clear to me how the model can generate informative responses . For example , if the model is trained by selecting wrong knowledge ( e.g. , K2 in R2 ) or knowledge irrelevant to the true response , it can be seen that they are completely useless since they can not provide any helpful information . 2 .The proposed model is not compared with other related works . For instance , the authors should compare with the commonsense model proposed in [ Zhou et al. , 2018 ] , which also uses knowledge as a prior distribution to guide the knowledge selection . 3 .In Table 1 , the performance of the proposed approach is not better than the baselines . It would be better if the authors can compare with other baselines such as [ Dinan et al .2018 ] and [ Ghazvininejad et al 2018 ] . 4 .The paper is not well-organized . It is hard to understand the motivation of the paper . The motivation of this paper is to learn a model that is able to generate informative and appropriate responses by incorporating knowledge in response generation , but the paper does not explain the motivation behind the proposed model . 5 .The experiments are not convincing . In Table 2 , the results are not very convincing . It seems that the model does not work well when the response is generated by using knowledge based on K1 and K3 . In addition , there are many typos in the paper , such as “ R1 utilizes no knowledge and thus ends up in a less informative response ” .
1_2005.06628 = This paper proposes a pruning-based architecture search method to improve the performance of the BERT model . The paper is well written and easy to follow . However , I have the following concerns : 1 . It is not clear to me what is the advantage of the proposed pruning method compared to other pruning methods . For example , the pruning of the attention heads in ALBERT ( Lan et al. , 2019 ) . 2 .It is unclear to me why the proposed method is better than the other two pruning techniques . 3 .The experiments are not convincing . In Table 1 and Table 2 , the results of the pruned BERT are not better than other pruned models . The authors should compare the results with other prune methods . 4 .In Table 3 , the authors should show the results for different pruning strategies . 5 .The authors should provide more details about the hyper-parameters used for pruning . 6 .The paper is not well organized . For instance , in Section 3.1 , it is hard to follow the algorithm . 7 .In Section 4.2 , the author should provide the details of the architecture search algorithm .
1_2102.07983 = This paper introduces a new dataset called FEWS ( Few-shot Examples of Word Senses ) , which is a dataset for evaluating Word Sense Disambiguation ( WSD ) models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary definitions of 71,000 senses , which are used as training data for WSD models . The authors also conduct experiments to compare the performance of knowledge-based approaches and a recent neural biencoder model on the new dataset . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset to evaluate the WSD model in the few-and-zero-shot setting , but it is not clear to me how significant this contribution is . 2 .The experimental results are not convincing . For example , in Table 1 , the results of the biencoders are not compared to human annotators , which makes it hard to judge the significance of the results . 3 .The transfer learning experiments are not well-conducted . It would be better to conduct the transfer learning experiment to see if the proposed dataset can be used as additional training data to improve the performance on other WSD tasks .
1_1812.02425 = This paper proposes a method for ensembling multiple neural networks with no additional testing cost . The proposed method is based on the teacher-student learning paradigm , where the teacher network is pre-trained and the student network is trained using adversarial learning . The authors show that the proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a variety of existing network architectures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of this paper is limited . It is a simple extension of the existing ensembles . 2 .The proposed method can not be used for applications with limited memory , storage space , or memory-constrained devices . 3 .The experimental results are not convincing . In Table 1 , the performance of Shake-shake and MEAL are not better than Shake-Shake . The improvement of MEAL is only 0.54 % on Cifar-10 , which is not significant .
1_1901.06829 = This paper proposes an end-to-end reinforcement learning framework for grounding natural language descriptions in videos . Specifically , the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . The agent makes an observation that details the currently selected video clip , and takes an action according to its policy , and the environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . The paper is well-written and easy to follow . The proposed method is novel and interesting . However , I have the following concerns : 1 . The novelty of this paper is limited . It is not clear how the proposed method can be applied to other video grounding tasks such as video retrieval and text-oriented video highlight detection . 2 .The experiments are not convincing . The performance of the proposed model is not better than the state-of-the-art baselines . 3 .The proposed method does not outperform the baselines in Table 1 and Table 2 . 4 .The paper is not well-organized . For example , there are many typos and grammatical errors .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . There are two different preference sets for agents . One is strict , which is a full ordinal list of all objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors study the problem of Object Reachability and Pareto Efficiency under different preferences . The authors prove that under strict preferences , the problem is solvable in polynomial time , and under weak preferences , they prove that the problem can be solved in a path-based algorithm . The paper is well-written and easy to follow . The problem studied in this paper is interesting and the results are interesting . However , I have the following concerns : 1 . In the paper , it is assumed that the agents are embedded in a social network , which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network and they find it mutually profitable ( or no one will become worse under weak preference ) , and they can mutually get benefits . Under this model , many problems have been studied , i.e .object reachability , assignment reachability and pareto efficiency . 2 .It is not clear to me why the authors assume that all agents have a tie with each other such that they know each other , and can make some exchanges . In some models , some agents often do not know each each other and can not exchange , and even if they can get benefits , they can still not exchange . 3 .The paper is not well-organized . For example , in the abstract , it says that the paper studies housing market where the agent is embedded in social network to denote the ability of exchange between pairs of objects between them . But in the introduction , the paper says that this model is a hot topic to study resource allocation problems over social networks and analyze the influence of networks . In fact , recently it is also considered network-based Fair Division in allocating indivisible resources .
1_2012.04715 = This paper proposes a SAT solver to solve the projective plane of order 10 problem , which is the most challenging subcase of Lam 's problem . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates are generated with the assistance of the symbolic computation library Traces . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that the certificates are more verifiable , but they do not provide a formal proof of the nonexistence of projective planes of order ten because they rely on results that currently have no formal computerverifiable proofs . 2 .It is not clear to me how the certificates can be verified by a third party . 3 .In the proof of Theorem 3.1 , the authors use a special-purpose solver , but I do not know how to verify the correctness of the certificates . 4 .The proof of theorem 3.2 is not correct . The proof is based on the fact that the error correcting code must contain words that are called `` primitive `` or `` weight `` 19 words , which are the first two cases that are first ruled out by the search of ( MacWilliams , Slane , and Thompson 1973 ) and were recently settled via SAT-based nonexistence certificates ( Bright et al. , 2020a ,b ) . 5 .In Theorem 4.2 , the proof is not complete . It is not possible to prove the existence of the plane .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and shows that they are at most as powerful as the Weisfeiler-Lehman ( WL ) graph isomorphism test . The main insight is that a GNN can have as large discriminative power as the WL test if the GNN ’s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of feature vectors of a given node ’ s neighbors as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNNS can be thought of as an aggregation function over the multisets . Hence , to have strong representational power , GNN must be able to aggregate different multi-sets into different representations . The paper also shows that the most powerful GNN by our theory , Graph Isomorphism Network ( GIN ) , has high expressive power as it perfectly fits the training data perfectly . This paper is well-written and easy to follow . However , I have the following concerns . 1 .Theorem 3.1 is not clear to me . What is the definition of the function f ( x , y ) ? 2 .In the proof of Lemma 3.2 , what is the meaning of the $ \mathcal { X } _ { i , j } $ ? 3 .What is $ \theta $ in Lemma 2.1 ? 4 .The experimental results are not convincing . In Table 1 , the performance of GCN and GIN is not better than WL . The reason is that GCN has better performance than the proposed GIN , but WL is better than GCN .
1_1907.07355 = This paper investigates the performance of BERT on the argument reasoning mining task . The authors show that BERT is able to exploit the presence of cue words in the warrant , especially “ not ” . They show that this can be eliminated in ARCT by adding a copy of each data point with the claim negated and the label inverted . This means that the distribution of statistical cues in the warrants will be mirrored over both labels , eliminating the signal . On this adversarial dataset all models perform randomly , with BERT achieving a maximum test set accuracy of 53 % . The paper is well-written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me how to interpret the results in Table 1 . In Table 1 , BERT achieves 77 % accuracy with its best run , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question : what has BERT learned about argument comprehension ? To investigate BERT ’ s decision making , the authors looked at data points it finds easy to classify over multiple runs . 2 .In Table 2 , the results are not consistent with the results of Habernal et al . ( 2018b ) , which shows that the same analysis with the SemEval submissions , and consistent with their results . 3 .It is also not clear why BERT exploits “ cue words ” in the ARCT task . 4 .The authors claim that “ In ARCT , we show that the major problem can be entirely accounted for in terms of exploiting spurious statistical cues . ” However , it is unclear to me what is the “ major problem ” of ARCT .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) to reduce the performance gap between low-precision training and full precision training by quantizing all numbers except the gradient accumulator to 8 bits without changing the network structure . The authors prove that for quadratic objective functions , SWA converges to a noise ball that is asymptotically smaller than the noise ball of SGD for strongly convex problems . The paper is well written and easy to follow . The proposed method is simple to implement and has little computational overhead . Empirical results show that the proposed method can match the full precision SGD baseline in deep learning tasks such as training ResNet-164 on CIFAR-10 and Cifar-100 datasets . However , I have some concerns about this paper . 1 .The proposed method seems to be a straightforward extension of SWA ( Izmailov et al. , 2018a ) . It is not clear to me why SWA can improve the performance of low precision training . 2 .The authors claim that SWA works well with a relatively high learning rate and can tolerate additional noise during training . But the experiments in this paper only show the performance improvement when the learning rate is low . 3 .In the experiments , the authors only compare with the state-of-the-art results of Wang et al . ( 2018 ) . 4 .In Table 1 , it is better to show the results of the proposed SWA method with the same number of parameters as the baseline . 5 .In Figure 2 , it seems that the performance gain is not significant .
1_1902.04911 = This paper proposes to use knowledge selection to guide dialogue generation . The key idea is to separate the prior distribution over knowledge from the posterior distribution over the response . The model is trained to minimize the KL divergence between the prior and posterior distributions . Then , the model samples the knowledge distribution based on the posterior information and incorporates the sampled knowledge into the response generation . Experiments are conducted on the Persona-chat dataset and Wizard of Wikipedia dataset . The paper is well-written and easy to follow . The idea of using knowledge selection for dialogue generation is interesting . However , I have the following concerns : 1 . The proposed model is not compared with any other knowledge-grounded dialogue generation models . For example , [ Dinan et al . [ 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 2 .The proposed model does not outperform the existing models by incorporating more knowledge . It is better to compare the proposed model with the baselines . 3 .The experiments are only conducted on one dataset . It would be better to conduct experiments on more datasets . 4 .The paper is not well-motivated . The motivation of this work is not clear . The authors claim that knowledge selection mechanism is effective for appropriate response generation , but they do not provide any empirical evidence to support this claim . 5 .The authors should provide more details about the model . For instance , what is the architecture of the model ? How many parameters are used for the model and how many parameters for the knowledge selection ? 6 .In the experiments , the authors only compare with the baseline models . It will be better if the authors can show the performance of the proposed method on other models as well .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea is to prune the architecture of the model by minimizing both the pre-training loss and the number of model parameters . Experiments are conducted on GLUE , SQuAD v1.1 and SQuad v2.0 . The paper is well written and easy to follow . However , I have the following concerns : 1 . It is not clear to me how the pruning is done in this paper . The pruning method is based on the idea of pruning the attention heads , which is not a new idea . 2 .The experiments are limited to GLUE . It would be better if the authors can conduct experiments on other NLP tasks . 3 .The authors should compare with other pruning methods , such as pruning based pruning of attention heads in ALBERT ( Lan et al. , 2019 ) . 4 .The results of the experiments are not convincing . For example , in Table 1 , the performance of the pruned BERT is not better than the original BERT .
1_2102.07983 = This paper introduces a new dataset called FEWS ( Few-shot Examples of Word Senses ) , which is a dataset for evaluating Word Sense Disambiguation ( WSD ) models in few-shot and zero-shot settings . The dataset is built on top of the Wiktionary dataset , which contains 71,000 senses . The authors evaluate the performance of different knowledge-based approaches and a recent neural biencoder model for WSD on the new dataset . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset to evaluate WSD models in the low-shot setting , but it is not clear to me how significant this contribution is . 2 .The evaluation of the proposed dataset is only conducted on a few senses . It would be interesting to see the performance on other senses . 3 .It would be good to see how the proposed datasets can be used for transfer learning . 4 .The paper is not well-organized . There are many typos and grammatical errors in the paper .
1_1812.02425 = This paper proposes a method for ensembling multiple neural networks with no additional testing cost . The proposed method is based on the teacher-student learning paradigm , where the teacher is a pre-trained network and the student is a student network trained with adversarial training . The paper is well written and easy to follow . However , I have the following concerns : 1 . The novelty of this paper is limited . The idea of using adversarial learning to improve the performance of the student network is not new . It has been proposed in [ 1 ] , [ 2 ] and [ 3 ] . The authors should compare their method with these methods . 2 .The experimental results are not convincing . For example , the proposed method does not outperform the shake-shake baseline on CIFAR-10/100 and SVHN . 3 .In Table 1 , the performance gain is not significant . It is better to compare the performance with the state-of-the-art methods . 4 .The proposed method can not be applied to mobile and embedded devices . It will be better if the authors can conduct experiments on mobile devices . 5 .The authors should provide more details about the training procedure of the teacher and student networks . For instance , how many parameters are used in the teacher network and student network ? 6 .It is not clear why the authors use the soft labels instead of the traditional one-hot vector labels . It would be better to provide more information about the soft label . 7 .The paper is not well organized . The description of the method is not easy to understand . It should be organized in the main body of the paper .
1_1901.06829 = This paper proposes an end-to-end reinforcement learning framework for grounding natural language descriptions in videos . The paper is well-written and easy to follow . The proposed method is novel and well-motivated . However , I have some concerns about the novelty of the proposed method . 1 .The proposed method seems to be a direct application of IoU to video grounding . It is not clear to me why the authors chose to use IoU as the grounding method . It seems to me that IoU can be applied to other tasks such as video retrieval or video highlight detection . 2 .In the experiments , the authors only show the results on one video . It would be better to show results on more videos . 3 .The authors should compare their method with the state-of-the-art methods such as [ 1 ] and [ 2 ] . 4 .It would be more convincing if the authors can show the performance of their method on more than one video and compare it with other methods . 5 .The paper is not well-organized . For example , there are many typos and grammatical errors .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . There are two different preference sets for agents . One is strict , which is a full ordinal list of all objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors study the problem of Object Reachability and Pareto Efficiency under different preferences . The authors prove that under strict preferences , the problem is solvable in polynomial time , and under weak preferences , they prove that the problem can be solved in a path-based algorithm . The paper is well-written and easy to follow . The problem studied in this paper is interesting and the results are interesting . However , I have the following concerns : 1 . In the paper , it is assumed that the agents are embedded in a social network , which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network and they find it mutually profitable ( or no one will become worse under weak preference ) , and they can mutually get benefits . Under this model , many problems have been studied , i.e .object reachability , assignment reachability and pareto efficiency . 2 .It is not clear to me why the authors assume that all agents have a tie with each other such that they know each other , and can make some exchanges . In some models , some agents often do not know each each other and can not exchange , and even if they can get benefits , they can still not exchange . 3 .The paper is not well-organized . For example , in the abstract , it says that the paper studies housing market where the agent is embedded in social network to denote the ability of exchange between pairs of objects between them . But in the introduction , the paper says that this model is a hot topic to study resource allocation problems over social networks and analyze the influence of networks . In fact , recently it is also considered network-based Fair Division in allocating indivisible resources .
1_2012.04715 = This paper proposes a SAT solver to solve the projective plane of order 10 problem , which is the most challenging subcase of Lam 's problem . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates are generated with the assistance of the symbolic computation library Traces . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that the certificates are more verifiable , but they do not provide a formal proof of the nonexistence of projective planes of order ten because they rely on results that currently have no formal computerverifiable proofs . 2 .It is not clear to me how the certificates can be verified by a third party . 3 .In the proof of Theorem 3.1 , the authors use a special-purpose solver , but I do not know how to verify the correctness of the certificates . 4 .The proof of theorem 3.2 is not correct . The proof is based on the fact that the error correcting code must contain words that are called `` primitive `` or `` weight `` 19 words , which are the first two cases that are first ruled out by the search of ( MacWilliams , Slane , and Thompson 1973 ) and were recently settled via SAT-based nonexistence certificates ( Bright et al. , 2020a ,b ) . 5 .In Theorem 4.2 , the proof is not complete . It is not possible to prove the existence of the plane .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and shows that they are at most as powerful as the Weisfeiler-Lehman ( WL ) graph isomorphism test in distinguishing graph structures . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of feature vectors of a given node ‘ s neighbors as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNNS can be thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . 2 .The paper also shows that the most powerful GNN by our theory , ie., Graph Isomorphism Network ( GIN ) , also empirically has high representation power as it fits the data perfectly . 3 .The experimental results show that the proposed GIN can not distinguish graph structures that can not be distinguished by popular GNN variants , GCN ( Kipf & Welling , 2017 ) and GraphSA ( Hamilton et al. , 2017a ) and GIN ( Velickovic et .al. , 2018 ) . 4 .In the experiments , the authors compare the performance of GIN with various aggregation functions with various graph aggregation functions . It would be better if the authors can compare the results with the results of GNN with different aggregation functions and graph-level pooling schemes .
1_1907.07355 = This paper presents an analysis of the performance of BERT on the argument reasoning mining task ( ARCT ) . The authors show that BERT is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated in ARCT by adding a copy of each data point with the claim negated and the label inverted . This means that the distribution of statistical cues in the warrants will be mirrored over both labels , eliminating the signal . On this adversarial dataset all models perform randomly , with BERT achieving a maximum test set accuracy of 53 % . This paper provides a more robust evaluation of argument comprehension and should be adopted as the standard in future work on this dataset . The paper is well written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me how to interpret the results in Table 1 . In Table 1 , BERT achieves 77 % accuracy with its best run , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question : what has BERT learned about argument comprehension ? To investigate BERT ’ s decision making , the authors looked at data points it finds easy to classify over multiple runs . 2 .In Table 2 , the results are not consistent with the results of Habernal et al . ( 2018b ) , which shows BERT exploits “ cue words ” in the ARCT SemEval submissions , and consistent with their results . It would be better if the authors can provide more explanation about this result .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The authors prove that for quadratic and strongly convex objectives , SWA converges to a noise ball that is asymptotically smaller than that of low precision SGD . They also show that their method can significantly reduce the performance gap between low precision and full precision training . This paper is well-written and easy to follow . The idea of using SWA to reduce the quantization noise during training is interesting and novel . However , I have the following concerns : 1 . The paper is not well-motivated . The motivation of SWA is not clear . In the introduction , the authors claim that SWA takes an average of SGD iterates with a modified learning rate schedule and has been shown to lead to wider optima ( Izmailov et al. , 2018a ) . But in the experiments , it seems that the authors only compare SWA with SGD with a low learning rate . 2 .The experiments are not convincing . In Table 1 , the performance of 8-bit SWA does not match the full precision baseline . 3 .In Table 2 , the accuracy of the proposed method is not better than the baseline . 4 .The authors should provide more details about the implementation details . For example , how to choose the number of bits in the quantized grid ? 5 .In the experiment , it is better to compare the performance with the state-of-the-art methods .
1_1902.04911 = This paper proposes to use knowledge selection to guide dialogue generation . The key idea is to separate the prior distribution over knowledge from the posterior distribution over the response . The model is trained to minimize the KL divergence between the prior and posterior distributions . Then , the model samples the knowledge distribution based on the posterior information and incorporates the sampled knowledge into the response generation . Experiments are conducted on the Persona-chat dataset and Wizard of Wikipedia dataset . The paper is well-written and easy to follow . The idea of using knowledge selection for dialogue generation is interesting . However , I have the following concerns : 1 . The proposed model is not compared with any other knowledge-grounded dialogue generation models . For example , [ Dinan et al . [ 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 2 .The proposed model does not outperform the existing models by incorporating more knowledge . It is better to compare the proposed model with the baselines . 3 .The experiments are only conducted on one dataset . It would be better to conduct experiments on more datasets . 4 .The paper is not well-motivated . The motivation of this work is not clear . The authors claim that knowledge selection mechanism is effective for appropriate response generation , but they do not provide any empirical evidence to support this claim . 5 .The authors should provide more details about the model . For instance , what is the architecture of the model ? How many parameters are used for the model and how many parameters for the knowledge selection ? 6 .In the experiments , the authors only compare with the baseline models . It will be better if the authors can show the performance of the proposed method on other models as well .
1_2005.06628 = This paper proposes a pruning-based architecture search method to improve the performance of the BERT model by pruning the number of parameters and reducing the size of the architecture design dimensions . The pruning method is based on the idea of pruning each layer of BERT by five different dimensions , instead of two . The authors show that the ratio of the pruning dimensions within a BERT encoder layer can be modified to obtain a layer with better perfor- Design dimensions . This paper is well-written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me why the authors chose to prune the design dimensions instead of parameter sharing across layers and factorizing the embedding layers . 2 .The pruning based architecture search technique is only applied to the pre-training loss and not to the model parameters . It would be better if the authors can show the results on the training loss and model parameters in the experiments . 3 .The authors should compare with the results of ALBERT ( https : //arxiv.org/abs/1711.06570 ) and RoBERTa ( http : //proceedings.mlr.press/v48/roberta.pdf ) . 4 .In the experiments , the authors only compare the performance on MNLI and MRPC SST-2 , which are not state-of-the-art . The results on SQuAD v1.1 and v2.0 should be reported .
1_2102.07983 = This paper introduces a new dataset called FEWS ( Few-shot Examples of Word Senses ) , which is a dataset for evaluating Word Sense Disambiguation ( WSD ) models in few-shot and zero-shot settings . The dataset is built on top of the Wiktionary dataset , which contains 71,000 senses . The authors evaluate the performance of different knowledge-based approaches and a recent neural biencoder model for WSD on the new dataset . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset to evaluate WSD models in the low-shot setting , but it is not clear to me how significant this contribution is . 2 .The evaluation of the proposed dataset is only conducted on a few senses . It would be interesting to see the performance on other senses . 3 .It would be good to see how the proposed datasets can be used for transfer learning . 4 .The paper is not well-organized . There are many typos and grammatical errors in the paper .
1_1812.02425 = This paper proposes a method for ensembling multiple neural networks with no additional testing cost . The proposed method is based on the teacher-student learning paradigm , where the teacher network is pre-trained and the student network is trained using adversarial learning . The authors propose to use soft labels instead of the traditional one-hot vector labels to improve the classification performance . The experiments show that the proposed method consistently improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a variety of existing network architectures . The paper is well-written and easy to follow . However , the novelty of this paper is limited . The idea of using soft labels is not new , and the experiments are not convincing enough to show the effectiveness of this method . The experimental results are not strong enough to support the effectiveness .
1_1901.06829 = This paper proposes an end-to-end reinforcement learning framework for grounding natural language descriptions in videos . Specifically , the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . The agent makes an observation that details the currently selected video clip , and takes an action according to its policy , and the environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . The paper is well-written and easy to follow . The proposed method is novel and interesting . However , I have the following concerns : 1 . The novelty of this paper is limited . There are many existing methods for action localization and video grounding tasks , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) .The proposed method can be applied only when all candidates are exhaustively enumerated , as they need to rank all possible clip-sentence pairs to be successful in generating the final grounding result . Moreover , the ranking strategy is specially designed for pre-segmented videos and is not applicable to general cases . To effectively detect the temporal boundaries of a given description from a given video , especially from a long , untrimmed video , it is important to make judgments only by several glimpses . 2 .The paper is not well-motivated . In the introduction , the authors claim that it requires to map natural language description to a close set of labels before applying these methods , which inevitably causes information loss . Although there is a growing interest in grounded natural language in videos , i.e. , if the sentence “The parent watches the boy play drums ” is mapped to an action label “play drums , ” , action detectors can hardly tell the start point of the presence of the parent . 3 .The experiments are not convincing . In Table 1 , the performance of the proposed method seems to be worse than the state-of-the-art methods . The authors should compare the performance with other methods .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . There are two different preference sets for agents . One is strict , which is a full ordinal list of all objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors study the problem of Object Reachability and Pareto Efficiency under different preferences . The authors prove that under strict preferences , the problem is solvable in polynomial time , and under weak preferences , they prove that the problem can be solved in a path-based algorithm . The paper is well-written and easy to follow . The problem studied in this paper is interesting and the results are interesting . However , I have the following concerns : 1 . In the paper , it is assumed that the agents are embedded in a social network , which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network and they find it mutually profitable ( or no one will become worse under weak preference ) , and they can mutually get benefits . Under this model , many problems have been studied , i.e .object reachability , assignment reachability and pareto efficiency . 2 .It is not clear to me why the authors assume that all agents have a tie with each other such that they know each other , and can make some exchanges . In some models , some agents often do not know each each other and can not exchange , and even if they can get benefits , they can still not exchange . 3 .The paper is not well-organized . For example , in the abstract , it says that the paper studies housing market where the agent is embedded in social network to denote the ability of exchange between pairs of objects between them . But in the introduction , the paper says that this model is a hot topic to study resource allocation problems over social networks and analyze the influence of networks . In fact , recently it is also considered network-based Fair Division in allocating indivisible resources .
1_2012.04715 = This paper proposes a SAT solver to solve the projective plane of order 10 problem , which is the most challenging subcase of Lam 's problem . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates are generated with the assistance of the symbolic computation library Traces . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that the certificates are more verifiable , but they do not provide a formal proof of the nonexistence of projective planes of order ten because they rely on results that currently have no formal computerverifiable proofs . 2 .It is not clear to me how the certificates can be verified by a third party . 3 .In the proof of Theorem 3.1 , the authors use a special-purpose solver , but I do not know how to verify the correctness of the certificates . 4 .The proof of theorem 3.2 is not correct . The proof is based on the fact that the error correcting code must contain words that are called `` primitive `` or `` weight `` 19 words , which are the first two cases that are first ruled out by the search of ( MacWilliams , Slane , and Thompson 1973 ) and were recently settled via SAT-based nonexistence certificates ( Bright et al. , 2020a ,b ) . 5 .In Theorem 4.2 , the proof is not complete . It is not possible to prove the existence of the plane .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and shows that they are at most as powerful as the Weisfeiler-Lehman ( WL ) graph isomorphism test in distinguishing graph structures . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of feature vectors of a given node ‘ s neighbors as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNNS can be thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . 2 .The paper also shows that the most powerful GNN by our theory , ie., Graph Isomorphism Network ( GIN ) , also empirically has high representation power as it fits the data perfectly . 3 .The experimental results show that the proposed GIN can not distinguish graph structures that can not be distinguished by popular GNN variants , GCN ( Kipf & Welling , 2017 ) and GraphSA ( Hamilton et al. , 2017a ) and GIN ( Velickovic et .al. , 2018 ) . 4 .In the experiments , the authors compare the performance of GIN with various aggregation functions with various graph aggregation functions . It would be better if the authors can compare the results with the results of GNN with different aggregation functions and graph-level pooling schemes .
1_1907.07355 = This paper presents an analysis of the performance of BERT on the argument reasoning mining task ( ARCT ) . The authors show that BERT is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated in ARCT by adding a copy of each data point with the claim negated and the label inverted . This means that the distribution of statistical cues in the warrants will be mirrored over both labels , eliminating the signal . On this adversarial dataset all models perform randomly , with BERT achieving a maximum test set accuracy of 53 % . This paper provides a more robust evaluation of argument comprehension and should be adopted as the standard in future work on this dataset . The paper is well written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me how to interpret the results in Table 1 . In Table 1 , BERT achieves 77 % accuracy with its best run , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question : what has BERT learned about argument comprehension ? To investigate BERT ’ s decision making , the authors looked at data points it finds easy to classify over multiple runs . 2 .In Table 2 , the results are not consistent with the results of Habernal et al . ( 2018b ) , which shows BERT exploits “ cue words ” in the ARCT SemEval submissions , and consistent with their results . It would be better if the authors can provide more explanation about this result .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The authors prove that for quadratic and strongly convex objectives , SWA converges to a noise ball that is asymptotically smaller than that of low precision SGD . They also show that their method can significantly reduce the performance gap between low precision and full precision training . This paper is well-written and easy to follow . The idea of using SWA to reduce the quantization noise during training is interesting and novel . However , I have the following concerns : 1 . The paper is not well-motivated . The motivation of SWA is not clear . In the introduction , the authors claim that SWA takes an average of SGD iterates with a modified learning rate schedule and has been shown to lead to wider optima ( Izmailov et al. , 2018a ) . But in the experiments , it seems that the authors only compare SWA with SGD with a low learning rate . 2 .The experiments are not convincing . In Table 1 , the performance of 8-bit SWA does not match the full precision baseline . 3 .In Table 2 , the accuracy of the proposed method is not better than the baseline . 4 .The authors should provide more details about the implementation details . For example , how to choose the number of bits in the quantized grid ? 5 .In the experiment , it is better to compare the performance with the state-of-the-art methods .
1_1902.04911 = This paper proposes a method for knowledge-grounded dialogue generation . The authors propose to use a knowledge selection mechanism to separate the prior distribution over knowledge and the posterior distribution over the response . The model is trained to minimize the KL divergence between the prior and posterior distributions . The posterior distribution is inferred from both the utterance and the response , and the actual knowledge used in the response is considered . The paper is well-written and easy to follow . The proposed method is novel and the experiments are convincing . However , I have the following concerns : 1 . The novelty of the method is limited . The method is a straightforward combination of existing methods . 2 .The experiments are only conducted on one dialogue dataset . It would be more convincing if the method can be applied to other dialogue datasets . 3 .The authors should compare the method with other knowledge selection methods . For example , the knowledge selection method in [ Dinan et al. , 2018 ] is based on the semantic similarity between the utterances and the knowledge . It is not clear whether the proposed method can work on other datasets . 4 .The paper is not well-motivated . In the introduction , the authors claim that knowledge selection is difficult to obtain in reality . But the authors do not provide any evidence to support this claim . 5 .In the experiments , the results are not convincing . In Table 1 , there is no comparison with other methods . 6 .In Table 2 , there are no comparisons with the baselines .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea is to prune the architecture of the model by minimizing both the pre-training loss and the number of model parameters . The paper is well written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me what is the advantage of the proposed pruning method compared to other pruning methods . For example , the pruning of the attention heads in ALBERT ( Lan et al. , 2019 ) . 2 .The authors claim that the proposed method can reduce the model parameters while increasing the latency , but the experiments do not show the effect of pruning on the model performance . 3 .The experiments are only conducted on the MNLI task . It would be better if the authors can conduct experiments on other NLP tasks , e.g. , WMT , NLI , etc . .
1_2102.07983 = This paper introduces a new dataset called FEWS ( Few-shot Examples of Word Senses ) , which is a dataset for evaluating Word Sense Disambiguation ( WSD ) models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary definitions of 71,000 senses , which are used as training data for WSD models . The authors also conduct experiments to compare the performance of knowledge-based approaches and a recent neural biencoder model on the new dataset . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset to evaluate the WSD model in the few-and-zero-shot setting , but it is not clear to me how significant this contribution is . 2 .The experimental results are not convincing . For example , in Table 1 , the results of the biencoders are not compared to human annotators , which makes it hard to judge the significance of the results . 3 .The transfer learning experiments are not well-conducted . It would be better to conduct the transfer learning experiment to see if the proposed dataset can be used as additional training data to improve the performance on other WSD tasks .
1_1812.02425 = This paper proposes a method for ensembling multiple neural networks with no additional testing cost . The proposed method is based on the teacher-student learning paradigm , where the teacher is a pre-trained network and the student is a student network trained with adversarial training . The paper is well written and easy to follow . However , I have the following concerns : 1 . The novelty of this paper is limited . The idea of using adversarial learning to improve the performance of the student network is not new . It has been proposed in [ 1 ] , [ 2 ] and [ 3 ] . The authors should compare their method with these methods . 2 .The experimental results are not convincing . The improvement of the proposed method on CIFAR-10/100/100 and SVHN/ImageNet is not significant . 3 .The proposed method does not outperform the state-of-the-art methods on ImageNet . 4 .It is not clear how to choose the hyperparameters . For example , the hyper-parameters of the teacher and student networks should be compared . 5 .The authors should provide more details about the experiments . For instance , how to select the hyper parameters for the teacher network and student network ? 6 .The experiments are not complete . The author should conduct more experiments on more datasets . 7 .The author should compare the performance with other ensembles such as [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,31,32,33,34,36,37,38,39,40,41,42,43,44,47,48,50,49,51,52,54,55,56,57,58,59,60,61,62,63,64,70,72,73,72 ,73,74,75,76,77,78,80,81,82,83,84,88,89,90,91,93,90 ,91,92,94,98,99,99 ,100,100,101,102,103,103 ,104,104 ,104 ,103,104,105,105 ,106,107,108 ,108,108,111,113,114,115,116,117,118,119,122,123,130,131,132,133,134,136,137,139,131 ,136,136 ,137,138,137 ,139,139 ,140,140,141,142,143,144,145,150,152,153,154,160,163,168,174,175,176,177,178,178 ,179,179,181,183,198,199,200,202,202 ,202,203,203 ,204,204,205 ,205,205,206,207,208 ,207,207 ,208,208,207-207-208-209-210-201-203-205-202-203 ,205-205 ,207-211-211 . 8 .In Table 1 , it is better to compare the results with [ 1 , 2 , 3 , 4 , 5 , 6 ] . 9 .The paper is not well organized . For the experiments , the authors should include the results of [ 2,3 , 4,5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54 , 56 , 57 , 58 , 59 , 60 , 62 , 63 , 64 , 68 , 67 , 68, 68 , 69 , 72 , 69, 69 , 70 , 72, 72 , 73 , 74 , 73, 78 , 68 : The authors need to add more details . 10 .The results of Table 1 are not consistent with the results in Table 2 . 11 .In table 2 , the results for [ 3,4 , 5,6 , 7,8 ] are not comparable . 12 .For Table 3 , the author should add the result of [ 4,7 , 8 ] to Table 1 .
1_1901.06829 = This paper proposes an end-to-end reinforcement learning framework for grounding natural language descriptions in videos . The paper is well-written and easy to follow . The proposed method is novel and well-motivated . However , I have some concerns about the novelty of the proposed method . 1 .The proposed method seems to be a direct application of existing methods for video grounding . The novelty of this paper is limited . The method is a combination of several existing methods . For example , the method of Gao et al . ( 2017 ) , which is used in this paper , is not compared with . 2 .The method of Hendricks et al. , 2017 is not applicable to general cases . 3 .The results are not convincing . The performance of the method is not better than the state-of-the-art methods . 4 .The paper is not well-organized . There are many typos and grammatical errors in the paper . I suggest the authors to proofread the paper carefully .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . There are two different preference sets for agents . One is strict , which is a full ordinal list of all objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors study the problem of Object Reachability and Pareto Efficiency under different preferences . The authors prove that under strict preferences , the problem is solvable in polynomial time , and under weak preferences , they prove that the problem can be solved in a path-based algorithm . The paper is well-written and easy to follow . The problem studied in this paper is interesting and the results are interesting . However , I have the following concerns : 1 . In the paper , it is assumed that the agents are embedded in a social network , which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network and they find it mutually profitable ( or no one will become worse under weak preference ) , and they can mutually get benefits . Under this model , many problems have been studied , i.e .object reachability , assignment reachability and pareto efficiency . 2 .It is not clear to me why the authors assume that all agents have a tie with each other such that they know each other , and can make some exchanges . In some models , some agents often do not know each each other and can not exchange , and even if they can get benefits , they can still not exchange . 3 .The paper is not well-organized . For example , in the abstract , it says that the paper studies housing market where the agent is embedded in social network to denote the ability of exchange between pairs of objects between them . But in the introduction , the paper says that this model is a hot topic to study resource allocation problems over social networks and analyze the influence of networks . In fact , recently it is also considered network-based Fair Division in allocating indivisible resources .
1_2012.04715 = This paper proposes a SAT solver to solve the projective plane of order 10 problem , which is the most challenging subcase of Lam 's problem . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates are generated with the assistance of the symbolic computation library Traces . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that the certificates are more verifiable , but they do not provide a formal proof of the nonexistence of projective planes of order ten because they rely on results that currently have no formal computerverifiable proofs . 2 .It is not clear to me how the certificates can be verified by a third party . 3 .In the proof of Theorem 3.1 , the authors use a special-purpose solver , but I do not know how to verify the correctness of the certificates . 4 .The proof of theorem 3.2 is not correct . The proof is based on the fact that the error correcting code must contain words that are called `` primitive `` or `` weight `` 19 words , which are the first two cases that are first ruled out by the search of ( MacWilliams , Slane , and Thompson 1973 ) and were recently settled via SAT-based nonexistence certificates ( Bright et al. , 2020a ,b ) . 5 .In Theorem 4.2 , the proof is not complete . It is not possible to prove the existence of the plane .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and shows that they are at most as powerful as the Weisfeiler-Lehman ( WL ) graph isomorphism test in distinguishing graph structures . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of feature vectors of a given node ‘ s neighbors as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNNS can be thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . 2 .The paper also shows that the most powerful GNN by our theory , ie., Graph Isomorphism Network ( GIN ) , also empirically has high representation power as it fits the data perfectly . 3 .The experimental results show that the proposed GIN can not distinguish graph structures that can not be distinguished by popular GNN variants , GCN ( Kipf & Welling , 2017 ) and GraphSA ( Hamilton et al. , 2017a ) and GIN ( Velickovic et .al. , 2018 ) . 4 .In the experiments , the authors compare the performance of GIN with various aggregation functions with various graph aggregation functions . It would be better if the authors can compare the results with the results of GNN with different aggregation functions and graph-level pooling schemes .
1_1907.07355 = This paper investigates the performance of BERT on the argument reasoning mining task . The authors show that BERT is able to exploit the presence of cue words in the warrant , especially “ not ” . They show that this can be eliminated in ARCT by adding a copy of each data point with the claim negated and the label inverted . This means that the distribution of statistical cues in the warrants will be mirrored over both labels , eliminating the signal . The adversarial dataset provides a more robust evaluation of argument comprehension and should be adopted as the standard in future work on this dataset . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that “ BERT achieves 77 % test set accuracy with its best run ( Table 1 ) , only three points below the average ( untrained ) human baseline . ” However , it is not clear to me why this is the case . In Table 1 , the human baseline is the same as BERT . 2 .In Table 2 , the authors claim “ In ARCT , we show that the major problem can be entirely accounted for in terms of exploiting spurious statistical cues ” , but it is unclear to me what is the “ major problem ” here . 3 .In Section 3.2 , the paper claims that ” BERT exploits the presence ” of cue word in the ” Not ” in the ARCT task , but in Table 2 the authors only report the results for “ ” not . 4 .In Figure 3 , it seems that the authors did not compare with the results of Habernal et al . ( 2018b ) . 5 .In the experiment part , it would be better to show the results with the same number of runs . 6 .In section 3.3 , the author claims “ The authors found that the BERT model exploits the existence of “ cue words ” and “ non-trivial ” words , which is consistent with their results ” but the authors do not provide any evidence . 7 .In table 4 , the results are not consistent with the ones in Table 1 .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The authors prove that for quadratic and strongly convex objectives , SWA converges to a noise ball that is asymptotically smaller than that of low precision SGD . They also show that their method can significantly reduce the performance gap between low precision and full precision training . This paper is well-written and easy to follow . The idea of using SWA to reduce the quantization noise during training is interesting and novel . However , I have the following concerns : 1 . The paper is not well-motivated . The motivation of SWA is not clear . In the introduction , the authors claim that SWA takes an average of SGD iterates with a modified learning rate schedule and has been shown to lead to wider optima ( Izmailov et al. , 2018a ) . But in the experiments , it seems that the authors only compare SWA with SGD with a low learning rate . 2 .The experiments are not convincing . In Table 1 , the performance of 8-bit SWA does not match the full precision baseline . 3 .In Table 2 , the accuracy of the proposed method is not better than the baseline . 4 .The authors should provide more details about the implementation details . For example , how to choose the number of bits in the quantized grid ? 5 .In the experiment , it is better to compare the performance with the state-of-the-art methods .
1_1902.04911 = This paper proposes to use knowledge selection to guide dialogue generation . The key idea is to separate the prior distribution over knowledge from the posterior distribution over the response . The model is trained to minimize the KL divergence between the prior and posterior distributions . Then , the model samples the knowledge distribution based on the posterior information and incorporates the sampled knowledge into the response generation . Experiments are conducted on the Persona-chat dataset and Wizard of Wikipedia dataset . The paper is well-written and easy to follow . The idea of using knowledge selection for dialogue generation is interesting . However , I have the following concerns : 1 . The proposed model is not compared with any other knowledge-grounded dialogue generation models . For example , [ Dinan et al . [ 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 2 .The proposed model does not outperform the existing models by incorporating more knowledge . It is better to compare the proposed model with the baselines . 3 .The experiments are only conducted on one dataset . It would be better to conduct experiments on more datasets . 4 .The paper is not well-motivated . The motivation of this work is not clear . The authors claim that knowledge selection mechanism is effective for appropriate response generation , but they do not provide any empirical evidence to support this claim . 5 .The authors should provide more details about the model . For instance , what is the architecture of the model ? How many parameters are used for the model and how many parameters for the knowledge selection ? 6 .In the experiments , the authors only compare with the baseline models . It will be better if the authors can show the performance of the proposed method on other models as well .
1_2005.06628 = This paper proposes a pruning-based architecture search method to improve the performance of the BERT model . The paper is well written and easy to follow . However , I have the following concerns : 1 . It is not clear to me what is the main contribution of this paper . The main contribution is the pruning of the architecture design . The pruning is based on the idea of pruning the number of parameters of the model , which is not new . 2 .The experiments are not convincing . The authors only compare the performance on one task ( MNLI ) . It would be more convincing if the authors can show the results on more tasks . 3 .The authors should compare with other pruning methods , such as pruning attention heads . 4 .The paper is not well organized . There are many typos and grammatical errors in the paper . For example , in the first paragraph of Section 3.1 , the authors use the word “ pruning ” several times .
1_2102.07983 = This paper introduces a new dataset called FEWS ( Few-shot Examples of Word Senses ) , which is a dataset for evaluating Word Sense Disambiguation ( WSD ) models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary definitions of 71,000 senses , which are used as training data for WSD models . The authors also conduct experiments to compare the performance of knowledge-based approaches and a recent neural biencoder model on the new dataset . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset to evaluate the WSD model in the few-and-zero-shot setting , but it is not clear to me how significant this contribution is . 2 .The experimental results are not convincing . For example , in Table 1 , the results of the biencoders are not compared to human annotators , which makes it hard to judge the significance of the results . 3 .The transfer learning experiments are not well-conducted . It would be better to conduct the transfer learning experiment to see if the proposed dataset can be used as additional training data to improve the performance on other WSD tasks .
1_1812.02425 = This paper proposes a method for ensembling multiple neural networks with no additional testing cost . The idea is to use the outputs of different neural networks as supervisions to guide the target network training . To further improve the robustness of student networks , the authors introduce an adversarial learning strategy to force the student to generate similar outputs as teachers . The proposed method improves the state-of-the-art accuracy on CIFAR-10/100 , SVHN , ImageNet for a variety of existing network architectures . The paper is well-written and easy to follow . The experiments show that MEAL consistently improves the accuracy across several popular network architectures on different datasets . However , I have the following concerns . 1 .The proposed method is based on the teacher-student learning paradigm . The teacher and student networks are the reference networks and the target networks are called teachers and the *Equal contribution . It is not clear to me why the soft labels are used instead of the traditional one-hot vector labels . 2 .The soft labels should be informative for the specific image . In other words , the labels should not be identical for all the given images with the same class . It can also be observed that soft labels can provide the additional intra- and inter-category relations of datasets . 3 .It is better to compare the performance of the proposed method with other ensembles , such as dropout , dropconnect , stochastic depth , and swapout .
1_1901.06829 = This paper proposes an end-to-end reinforcement learning framework for grounding natural language descriptions in videos . Specifically , the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . The agent makes an observation that details the currently selected video clip , and takes an action according to its policy , and the environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . The paper is well-written and easy to follow . The proposed method is novel and interesting . However , I have the following concerns : 1 . The novelty of this paper is limited . It is not clear how the proposed method can be applied to other video grounding tasks such as video retrieval and text-oriented video highlight detection . 2 .The experiments are not convincing . The performance of the proposed model is not better than the state-of-the-art baselines . 3 .The proposed method does not outperform the baselines in Table 1 and Table 2 . 4 .The paper is not well-organized . For example , there are many typos and grammatical errors .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . There are two different preference sets for agents . One is strict , which is a full ordinal list of all objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors study the problem of Object Reachability and Pareto Efficiency under different preferences . The authors prove that under strict preferences , the problem is solvable in polynomial time , and under weak preferences , they prove that the problem can be solved in a path-based algorithm . The paper is well-written and easy to follow . The problem studied in this paper is interesting and the results are interesting . However , I have the following concerns : 1 . In the paper , it is assumed that the agents are embedded in a social network , which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network and they find it mutually profitable ( or no one will become worse under weak preference ) , and they can mutually get benefits . Under this model , many problems have been studied , i.e .object reachability , assignment reachability and pareto efficiency . 2 .It is not clear to me why the authors assume that all agents have a tie with each other such that they know each other , and can make some exchanges . In some models , some agents often do not know each each other and can not exchange , and even if they can get benefits , they can still not exchange . 3 .The paper is not well-organized . For example , in the abstract , it says that the paper studies housing market where the agent is embedded in social network to denote the ability of exchange between pairs of objects between them . But in the introduction , the paper says that this model is a hot topic to study resource allocation problems over social networks and analyze the influence of networks . In fact , recently it is also considered network-based Fair Division in allocating indivisible resources .
1_2012.04715 = This paper proposes a SAT solver to solve the projective plane of order 10 problem , which is the most challenging subcase of Lam 's problem . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete , and verify the certificates with a proof verifier . The certificates are generated with the assistance of the symbolic computation library Traces . The paper is well written and easy to follow . However , I have the following concerns : 1 . The authors claim that the certificates are more verifiable , but they do not provide a complete formal proof of the nonexistence of a projective planes of order ten because they rely on results that currently have no formal computerverifiable proofs . 2 .It is not clear to me how the certificates can be verified with a third party . 3 .In the proof of Theorem 3.1 , the authors use the proof that the error correcting code associated with a hypothetical projective geometry must contain words that are weight 15 , weight 16 , or “ primitive weight 19 ” , which are referred to as weight 15 and “ primitive ” in the paper . It would be better if the authors can provide more details on how to verify the correctness of the certificates . 4 .The proof of theorem 3.2 is not very clear . It seems that the proof is based on the fact that the problem is solved by a special purpose solver . It is better to provide a more detailed proof .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and shows that they are at most as powerful as the Weisfeiler-Lehman ( WL ) graph isomorphism test in distinguishing graph structures . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of feature vectors of a given node ‘ s neighbors as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNNS can be thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . 2 .The paper also shows that the most powerful GNN by our theory , ie., Graph Isomorphism Network ( GIN ) , also empirically has high representation power as it fits the data perfectly . 3 .The experimental results show that the proposed GIN can not distinguish graph structures that can not be distinguished by popular GNN variants , GCN ( Kipf & Welling , 2017 ) and GraphSA ( Hamilton et al. , 2017a ) and GIN ( Velickovic et .al. , 2018 ) . 4 .In the experiments , the authors compare the performance of GIN with various aggregation functions with various graph aggregation functions . It would be better if the authors can compare the results with the results of GNN with different aggregation functions and graph-level pooling schemes .
1_1907.07355 = This paper investigates the performance of BERT on the ARCT dataset . The authors find that BERT is able to exploit the presence of cue words in the warrant , especially “ not ” . They show that this can be eliminated in ARCT by adding a copy of each data point with the claim negated and the label inverted . This means that the distribution of statistical cues in the warrants will be mirrored over both labels , eliminating the signal . On this adversarial dataset all models perform randomly , with BERT achieving a maximum test set accuracy of 53 % . This paper provides a more robust evaluation of argument comprehension and should be adopted as the standard in future work on this dataset . I think this paper is well-written and easy to follow . However , I have the following concerns about this paper . 1 .The authors claim that “ BERT ’ s surprising performance can be entirely accounted for in terms of exploiting spurious statistical cues . ” However , they show that the major problem can ’ t be eliminated by ARCT since R_ { A } ^ { C } = R^ { C+1 } , which means that R * * can be written such that R* * can not be ignored . 2 .In Table 1 , BERT achieves 77 % accuracy with its best run , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question “ what has BERT learned about argument comprehension ? ” 3 .I think the authors should provide more details about the dataset used in the experiments .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The authors prove that for quadratic and strongly convex objectives , SWA converges to a noise ball that is asymptotically smaller than that of low precision SGD . They also show that their method can significantly reduce the performance gap between low precision and full precision training . This paper is well-written and easy to follow . The idea of using SWA to reduce the quantization noise during training is interesting and novel . However , I have the following concerns : 1 . The paper is not well-motivated . The motivation of SWA is not clear . In the introduction , the authors claim that SWA takes an average of SGD iterates with a modified learning rate schedule and has been shown to lead to wider optima ( Izmailov et al. , 2018a ) . But in the experiments , it seems that the authors only compare SWA with SGD with a low learning rate . 2 .The experiments are not convincing . In Table 1 , the performance of 8-bit SWA does not match the full precision baseline . 3 .In Table 2 , the accuracy of the proposed method is not better than the baseline . 4 .The authors should provide more details about the implementation details . For example , how to choose the number of bits in the quantized grid ? 5 .In the experiment , it is better to compare the performance with the state-of-the-art methods .
1_1902.04911 = This paper proposes to use knowledge selection to guide dialogue generation . The key idea is to separate the prior distribution over knowledge from the posterior distribution over the response . The model is trained to minimize the KL divergence between the prior and posterior distributions . Then , the model samples the knowledge distribution based on the posterior information and incorporates the sampled knowledge into the response generation . Experiments are conducted on the Persona-chat dataset and Wizard of Wikipedia dataset . The paper is well-written and easy to follow . The idea of using knowledge selection for dialogue generation is interesting . However , I have the following concerns : 1 . The proposed model is not compared with any other knowledge-grounded dialogue generation models . For example , [ Dinan et al . [ 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 2 .The proposed model does not outperform the existing models by incorporating more knowledge . It is better to compare the proposed model with the baselines . 3 .The experiments are only conducted on one dataset . It would be better to conduct experiments on more datasets . 4 .The paper is not well-motivated . The motivation of this work is not clear . The authors claim that knowledge selection mechanism is effective for appropriate response generation , but they do not provide any empirical evidence to support this claim . 5 .The authors should provide more details about the model . For instance , what is the architecture of the model ? How many parameters are used for the model and how many parameters for the knowledge selection ? 6 .In the experiments , the authors only compare with the baseline models . It will be better if the authors can show the performance of the proposed method on other models as well .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea is to prune the architecture of the model by minimizing both the pre-training loss and the number of model parameters . The paper is well written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me what is the advantage of the proposed pruning method compared to other pruning methods . For example , the pruning of the attention heads in ALBERT ( Lan et al. , 2019 ) . 2 .The authors claim that the proposed method can reduce the model parameters while increasing the latency , but the experiments do not show the effect of pruning on the model performance . 3 .The experiments are only conducted on the MNLI task . It would be better if the authors can conduct experiments on other NLP tasks , e.g. , WMT , NLI , etc . .
1_2102.07983 = This paper introduces a new dataset called FEWS ( Few-shot Examples of Word Senses ) , which is a dataset for evaluating Word Sense Disambiguation ( WSD ) models in few-shot and zero-shot settings . The dataset is built on top of Wiktionary definitions of 71,000 senses , which are used as training data for WSD models . The authors also conduct experiments to compare the performance of knowledge-based approaches and a recent neural biencoder model on the new dataset . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset to evaluate the WSD model in the few-and-zero-shot setting , but it is not clear to me how significant this contribution is . 2 .The experimental results are not convincing . For example , in Table 1 , the results of the biencoders are not compared to human annotators , which makes it hard to judge the significance of the results . 3 .The transfer learning experiments are not well-conducted . It would be better to conduct the transfer learning experiment to see if the proposed dataset can be used as additional training data to improve the performance on other WSD tasks .
1_1812.02425 = This paper proposes a method for ensembling multiple neural networks with no additional testing cost . The proposed method is based on the teacher-student learning paradigm , where the teacher is a pre-trained network and the student is a student network . The teacher is trained to predict the outputs of different network architectures with different or identical augmented input . The student network is trained with adversarial learning to force the student to generate similar outputs as teachers . Experiments show that the proposed method consistently improves the accuracy across a variety of popular network architectures on different datasets . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of this paper is limited . The idea of using adversarial training to improve the performance of the student networks is not new . The authors should compare the performance with other adversarial methods , e.g. , [ 1 ] , [ 2 ] . 2 .The experimental results are not convincing . For example , in Table 1 , the results of Shake-Shake-MeAL ( Gastaldi 2017 ) and MEAL ( Meals 2018 ) are not better than Shake-shake-Meals . 3 .In Table 2 , the improvement of MEAL is not significant . It is better to compare with other methods such as [ 3 ] and [ 4 ] . 4 .The authors should also compare with [ 5 ] . 5 .The experiments are not complete . For instance , the authors should conduct more experiments on ImageNet dataset .
1_1901.06829 = This paper proposes an end-to-end reinforcement learning framework for grounding natural language descriptions in videos . The paper is well-written and easy to follow . The proposed method is novel and well-motivated . However , I have some concerns about the novelty of the proposed method . 1 .The proposed method seems to be a direct application of IoU to the video grounding task . It is not clear to me why the authors chose to use IoU as the grounding method . It seems to me that IoU can be applied to other tasks such as video retrieval or highlight detection . 2 .The authors claim that the method is able to handle video grounding without having to search over the entire video . But the results in Table 1 and Table 2 do not support this claim . 3 .In Table 1 , the results for IoU and IoU+RNN are not comparable . 4 .The results of the multi-task learning are not convincing . The results of multi-tasking is not convincing either . 5 .In the experiments , the authors only compare with the baseline methods . It would be more convincing if the authors can show the performance of their method with other baselines . 6 .The paper is not well-organized . There are many typos and grammatical errors in the paper .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . There are two different preference sets for agents . One is strict , which is a full ordinal list of all objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors study the problem of Object Reachability and Pareto Efficiency under different preferences . The authors prove that under strict preferences , the problem is solvable in polynomial time , and under weak preferences , they prove that the problem can be solved in a path-based algorithm . The paper is well-written and easy to follow . The problem studied in this paper is interesting and the results are interesting . However , I have the following concerns : 1 . In the paper , it is assumed that the agents are embedded in a social network , which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network and they find it mutually profitable ( or no one will become worse under weak preference ) , and they can mutually get benefits . Under this model , many problems have been studied , i.e .object reachability , assignment reachability and pareto efficiency . 2 .It is not clear to me why the authors assume that all agents have a tie with each other such that they know each other , and can make some exchanges . In some models , some agents often do not know each each other and can not exchange , and even if they can get benefits , they can still not exchange . 3 .The paper is not well-organized . For example , in the abstract , it says that the paper studies housing market where the agent is embedded in social network to denote the ability of exchange between pairs of objects between them . But in the introduction , the paper says that this model is a hot topic to study resource allocation problems over social networks and analyze the influence of networks . In fact , recently it is also considered network-based Fair Division in allocating indivisible resources .
1_2012.04715 = This paper proposes a SAT solver to solve the projective plane of order 10 problem , which is the most challenging subcase of Lam 's problem . The main contribution of the paper is to generate certificates that demonstrate the A2 search is complete and verify the certificates with a proof verifier . The certificates are generated with the assistance of the symbolic computation library Traces . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The authors claim that the certificates are more verifiable , but they do not provide a formal proof of the nonexistence of projective planes of order ten because they rely on results that currently have no formal computerverifiable proofs . 2 .It is not clear to me how the certificates can be verified by a third party . 3 .In the proof of Theorem 3.1 , the authors use a special-purpose solver , but I do not know how to verify the correctness of the certificates . 4 .The proof of theorem 3.2 is not correct . The proof is based on the fact that the error correcting code must contain words that are called `` primitive `` or `` weight `` 19 words , which are the first two cases that are first ruled out by the search of ( MacWilliams , Slane , and Thompson 1973 ) and were recently settled via SAT-based nonexistence certificates ( Bright et al. , 2020a ,b ) . 5 .In Theorem 4.2 , the proof is not complete . It is not possible to prove the existence of the plane .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) and shows that they are at most as powerful as the Weisfeiler-Lehman ( WL ) graph isomorphism test in distinguishing graph structures . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to show that GNN can have as large discriminative power as the WL test if the GNN ’ s aggregation scheme is highly expressive and can model injective functions . To mathematically formalize the above insight , the framework first represents the set of feature vectors of a given node ‘ s neighbors as a multiset , i.e. , a set with possibly repeating elements . Then , the neighbor aggregation in GNNS can be thought of as an aggregation function over the multisets . Hence , to have strong representational power , a GNN must be able to aggregate different multi-sets into different representations . 2 .The paper also shows that the most powerful GNN by our theory , ie., Graph Isomorphism Network ( GIN ) , also empirically has high representation power as it fits the data perfectly . 3 .The experimental results show that the proposed GIN can not distinguish graph structures that can not be distinguished by popular GNN variants , GCN ( Kipf & Welling , 2017 ) and GraphSA ( Hamilton et al. , 2017a ) and GIN ( Velickovic et .al. , 2018 ) . 4 .In the experiments , the authors compare the performance of GIN with various aggregation functions with various graph aggregation functions . It would be better if the authors can compare the results with the results of GNN with different aggregation functions and graph-level pooling schemes .
1_1907.07355 = This paper presents an analysis of the performance of BERT on the argument reasoning mining task ( ARCT ) . The authors show that BERT is able to exploit the presence of cue words in the warrant , especially “ not ” . They also show that this effect can be eliminated in ARCT by adding a copy of each data point with the claim negated and the label inverted . This means that the distribution of statistical cues in the warrants will be mirrored over both labels , eliminating the signal . On this adversarial dataset all models perform randomly , with BERT achieving a maximum test set accuracy of 53 % . This paper provides a more robust evaluation of argument comprehension and should be adopted as the standard in future work on this dataset . The paper is well written and easy to follow . However , I have the following concerns about this paper : 1 . It is not clear to me how to interpret the results in Table 1 . In Table 1 , BERT achieves 77 % accuracy with its best run , only three points below the average ( untrained ) human baseline . Without supplying the required world knowledge for this task it does not seem reasonable to expect it to perform so well . This motivates the question : what has BERT learned about argument comprehension ? To investigate BERT ’ s decision making , the authors looked at data points it finds easy to classify over multiple runs . 2 .In Table 2 , the results are not consistent with the results of Habernal et al . ( 2018b ) , which shows BERT exploits “ cue words ” in the ARCT SemEval submissions , and consistent with their results . It would be better if the authors can provide more explanation about this result .
1_1904.11943 = This paper proposes to use stochastic weight averaging ( SWALP ) in low-precision training where all numbers including the gradient accumulator and velocity vectors are represented in low precision during training . The authors prove that for quadratic and strongly convex objectives , SWA converges to a noise ball that is asymptotically smaller than that of low precision SGD . They also show that their method can significantly reduce the performance gap between low precision and full precision training . This paper is well-written and easy to follow . The idea of using SWA to reduce the quantization noise during training is interesting and novel . However , I have the following concerns : 1 . The paper is not well-motivated . The motivation of SWA is not clear . In the introduction , the authors claim that SWA takes an average of SGD iterates with a modified learning rate schedule and has been shown to lead to wider optima ( Izmailov et al. , 2018a ) . But in the experiments , it seems that the authors only compare SWA with SGD with a low learning rate . 2 .The experiments are not convincing . In Table 1 , the performance of 8-bit SWA does not match the full precision baseline . 3 .In Table 2 , the accuracy of the proposed method is not better than the baseline . 4 .The authors should provide more details about the implementation details . For example , how to choose the number of bits in the quantized grid ? 5 .In the experiment , it is better to compare the performance with the state-of-the-art methods .
1_1902.04911 = This paper proposes to use knowledge selection to guide dialogue generation . The key idea is to separate the prior distribution over knowledge from the posterior distribution over the response . The model is trained to minimize the KL divergence between the prior and posterior distributions . Then , the model samples the knowledge distribution based on the posterior information and incorporates the sampled knowledge into the response generation . Experiments are conducted on the Persona-chat dataset and Wizard of Wikipedia dataset . The paper is well-written and easy to follow . The idea of using knowledge selection for dialogue generation is interesting . However , I have the following concerns : 1 . The proposed model is not compared with any other knowledge-grounded dialogue generation models . For example , [ Dinan et al . [ 2018 ] used ground-truth knowledge to guide knowledge selection , which demonstrates improvements over those not using such information . 2 .The proposed model does not outperform the existing models by incorporating more knowledge . It is better to compare the proposed model with the baselines . 3 .The experiments are only conducted on one dataset . It would be better to conduct experiments on more datasets . 4 .The paper is not well-motivated . The motivation of this work is not clear . The authors claim that knowledge selection mechanism is effective for appropriate response generation , but they do not provide any empirical evidence to support this claim . 5 .The authors should provide more details about the model . For instance , what is the architecture of the model ? How many parameters are used for the model and how many parameters for the knowledge selection ? 6 .In the experiments , the authors only compare with the baseline models . It will be better if the authors can show the performance of the proposed method on other models as well .
1_2005.06628 = This paper proposes a pruning-based architecture search method for the BERT model . The main idea is to prune the architecture of the model by minimizing both the pre-training loss and the number of model parameters . Experiments are conducted on GLUE , SQuAD v1.1 and SQuad v2.0 . The paper is well written and easy to follow . However , I have the following concerns : 1 . It is not clear to me how the pruning is done in this paper . The pruning method is based on the idea of pruning the attention heads , which is not a new idea . 2 .The experiments are limited to GLUE . It would be better if the authors can conduct experiments on other NLP tasks . 3 .The authors should compare with other pruning methods , such as pruning based pruning of attention heads in ALBERT ( Lan et al. , 2019 ) . 4 .The results of the experiments are not convincing . For example , in Table 1 , the performance of the pruned BERT is not better than the original BERT .
1_2102.07983 = This paper introduces a new dataset called FEWS ( Few-shot Examples of Word Senses ) , which is a dataset for evaluating word sense disambiguation models in few-shot and zero-shot settings . The dataset is built on top of the Wiktionary dataset , which contains 71,000 senses . The authors compare the performance of knowledge-based approaches and a recent neural biencoder model for WSD on the new dataset . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the paper is limited . The main contribution of this paper is the introduction of a dataset to evaluate WSD models in the low-shot setting , but it is not clear to me why this dataset is better than existing datasets such as SemCor and SemNet . 2 .The authors claim that the dataset is more comprehensive than SemCor , but I am not sure why this is the case . SemCor is the largest manually annotated WSD dataset , while the dataset in this paper only contains 71 ,000 senses , which makes it hard to compare with SemCor . 3 .In Table 1 , the authors mention that the BienCoder outperforms human annotators , but there is no comparison with the state-of-the-art on the dataset . 4 .The transfer learning experiment is interesting , but the authors do not provide any results on transfer learning . 5 .The paper is not well-organized . For example , in the introduction , it is hard to understand what is the purpose of Table 1 and Table 2 . 6 .In Section 3.2 , the paper claims that the proposed dataset will be used as additional training data for other WSD tasks , but no results are provided .
1_1812.02425 = This paper proposes a method for ensembling multiple neural networks with no additional testing cost . The proposed method is based on the teacher-student learning paradigm , where the teacher is a pre-trained network and the student is a student network . The teacher is trained to predict the outputs of different network architectures with different or identical augmented input . The student network is trained with adversarial learning to force the student to generate similar outputs as teachers . Experiments show that the proposed method consistently improves the accuracy across a variety of popular network architectures on different datasets . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of this paper is limited . The idea of using adversarial training to improve the performance of the student networks is not new . The authors should compare with the following works : `` Adversarial Learning for Deep Ensembling of Multiple Neural Networks `` , https : //arxiv.org/abs/1902.08832 , https: //arXiv.pdf 2 . The experimental results are not convincing . In Table 1 , the results of Shake-Shake-MeAL and Shake-Meal are not comparable . The difference between the results in Table 1 and Table 2 is not significant . 3 .In Table 2 , the difference between Shake-shake and MEAL is not clear . It is better to show the difference in Table 2 and Table 3 .
1_1901.06829 = This paper proposes an end-to-end reinforcement learning framework for grounding natural language descriptions in videos . Specifically , the agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its boundaries , and then it determines where to move the temporal grounding boundaries based on the policy . The agent makes an observation that details the currently selected video clip , and takes an action according to its policy , and the environment is updated accordingly and offers the agent a reward that represents how well the chosen action performs . The paper is well-written and easy to follow . The proposed method is novel and interesting . However , I have the following concerns : 1 . The novelty of this paper is limited . There are many existing methods for action localization and video grounding tasks , e.g. , ( Hendricks et al. , 2017 ; Liu et al .2018 ) .The proposed method can be applied only when all candidates are exhaustively enumerated , as they need to rank all possible clip-sentence pairs to be successful in generating the final grounding result . Moreover , the ranking strategy is specially designed for pre-segmented videos and is not applicable to general cases . To effectively detect the temporal boundaries of a given description from a given video , especially from a long , untrimmed video , it is important to make judgments only by several glimpses . 2 .The paper is not well-motivated . In the introduction , the authors claim that it requires to map natural language description to a close set of labels before applying these methods , which inevitably causes information loss . Although there is a growing interest in grounded natural language in videos , i.e. , if the sentence “The parent watches the boy play drums ” is mapped to an action label “play drums , ” , action detectors can hardly tell the start point of the presence of the parent . 3 .The experiments are not convincing . In Table 1 , the performance of the proposed method seems to be worse than the state-of-the-art methods . The authors should compare the performance with other methods .
1_1909.07557 = This paper studies the assignment problem in the Housing Market setting , where each agent can only receive one single object and each agent has preferences over objects . There are two different preference sets for agents . One is strict , which is a full ordinal list of all objects , and the other one is weak , where agents are allowed to be indifferent between objects . Both preference sets have been widely studied . In this paper , the authors study the problem of Object Reachability and Pareto Efficiency under different preferences . The authors prove that under strict preferences , the problem is solvable in polynomial time , and under weak preferences , they prove that the problem can be solved in a path-based algorithm . The paper is well-written and easy to follow . The problem studied in this paper is interesting and the results are interesting . However , I have the following concerns : 1 . In the paper , it is assumed that the agents are embedded in a social network , which determines their ability to exchange their objects . Two agents may swap their items under two conditions : they are neighbors in the social network and they find it mutually profitable ( or no one will become worse under weak preference ) , and they can mutually get benefits . Under this model , many problems have been studied , i.e .object reachability , assignment reachability and pareto efficiency . 2 .It is not clear to me why the authors assume that all agents have a tie with each other such that they know each other , and can make some exchanges . In some models , some agents often do not know each each other and can not exchange , and even if they can get benefits , they can still not exchange . 3 .The paper is not well-organized . For example , in the abstract , it says that the paper studies housing market where the agent is embedded in social network to denote the ability of exchange between pairs of objects between them . But in the introduction , the paper says that this model is a hot topic to study resource allocation problems over social networks and analyze the influence of networks . In fact , recently it is also considered network-based Fair Division in allocating indivisible resources .
