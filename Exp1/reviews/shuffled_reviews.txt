1_2012.04715 = In this paper , the authors tackle the problem of finding a projective plane of order 10 from projective geometry . The authors translate the problem into Boolean logic and use satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solvers instead of special-purpose search code to solve the problem . However , it is not clear to me why the authors chose to use the SAT solver instead of the cube-and-conquer method . It would be better if the authors can provide more details about the choice of the solver . In particular , it would be interesting to see the performance of the proposed solver compared to the existing solver in terms of speed and accuracy .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors show that the proposed method converges arbitrarily close to the optimal solution for quadratic objectives , and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and well-organized . However , I have the following concerns : 1 . In the experiments , it is not clear how to choose the hyperparameters for the experiments . For example , how to set the learning rate ? 2 .In Table 1 , the comparison between SWA and HALP is not fair . In HALP , HALP uses a fixed learning rate , while SWA uses a cyclic learning rate . It is better to compare the performance between the two methods in Table 1 . 3 .In the experiment , the authors should compare the accuracy of SWA with HALP with the same number of iterations . 4 .In Section 4.2 , the author should provide more details about the algorithm .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that employs a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed model is novel and well-motivated . 3 .The experiments are comprehensive and show the effectiveness of the model . 4 .The paper is technically sound . Cons : 1 ) The proposed model seems to be a straightforward combination of two existing models . It would be better if the authors can provide a more detailed analysis of the differences between the two models . 2 ) It is not clear why the authors use the KLDivLoss to measure the proximity between the prior distribution and the posterior distribution . 3 ) The authors should provide more details about the model architecture . 4 ) The experiments are only conducted on the Wizard-of-Wikipedia dataset . It is better to conduct experiments on other datasets .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT language model . The main idea is to prune the parameters of the model in order to reduce its size . The paper is well written and easy to follow . However , I have the following concerns : 1 . The proposed method is based on pruning the architecture design dimensions of BERT , which is not a new idea . The authors should compare with other pruning methods such as DistilBERT ( Sanh et al. , 2019 ) and RoBERTa ( Liu et al .2019 ) .2 .The experiments are limited to GLUE and SQuAD datasets . It would be better if the authors can conduct more experiments on other NLP tasks . 3 .The authors should provide more details about the pruning method . For example , how many prunable parameters are pruned in each pruning step ? 4 .The proposed method can be applied to other objectives such as FLOPs or latency . It is better to show the performance of the proposed method on other tasks .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a reinforcement learning based framework for the task of video grounding , which aims to temporally localize a natural language description in a video . To alleviate this problem , the authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset and Charades-STA dataset , while observing only 10 or less clips per video . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . It is not clear to me how the proposed model is different from the existing methods . 2 .The proposed method does not seem to be able to solve the problem of grounding natural language in image/video . 3 .The experimental results are not convincing . The authors only report the results on two datasets . It would be more convincing if the authors can report results on more datasets .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem is solvable when the network is a path or a star . This paper answers this open problem positively by giving a polynomially-time algorithm . Then the authors show that when the preferences are weak ( ties among objects are allowed ) , the problem becomes NP-hard when the networks is path or star . The paper is well-written and easy to follow . The main ideas of the two algorithms are similar : first characterizing some structural properties of the problem and then reducing the problem to the 2-Sat problem . In order to study the problems systematically and further understand the computational complexity of the problems , the authors also consider Pareto Efficiency and give several hardness results .
1_2012.04715 = In this paper , the authors tackle the problem of finding a projective plane of order 10 from projective geometry . The authors translate the problem into Boolean logic and use satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solvers instead of special-purpose search code to solve the problem . However , it is not clear to me why the authors chose to use the SAT solver instead of the cube-and-conquer method . It would be better if the authors can provide more details about the choice of the solver . In particular , it would be interesting to see the performance of the proposed solver compared to the existing solver in terms of speed and accuracy .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) to capture different graph structures . The main contribution of the paper is to analyze the discriminative power of popular GNN variants , such as Graph Convolutional Networks and GraphSAGE , and show that they can not learn to distinguish certain simple graph structures , and develop a simple architecture that is provably the most expressive among the class of GNN . The paper is well-written and easy to follow . However , I have the following concerns : 1 . In the abstract , the authors claim that the most powerful GNN by their theory , i.e. , Graph Isomorphism Network ( GIN ) , also empirically has high representational power as it almost perfectly fits the training data , whereas the less powerful Gnn variants often severely underfit the data . But , in the experiments , the GIN only achieves state-of-the-art results on a number of graph classification benchmarks , which is not convincing . 2 .In the experimental results , the performance of the proposed GIN model is not better than the state of the art models . 3 .The authors should compare the proposed model with other GNN models , e.g. , GCN ( Kipff & Welling 2017 ) and GIN ( Hamilton et al. , 2017 ) . 4 .In Section 5.2 , it is claimed that GIN is as powerful as the Weisfeiler-Lehman graph isomorphism test , but it is not clear why .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full-precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . Theoretical analysis shows that the proposed method converges arbitrarily close to the optimal solution for quadratic objectives , and to a noise ball asymptotically smaller than low Precision SGD in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear how to choose the number of epochs for the experiments . It is better to use the same number for all the epochs . 2 .In the proof of Lemma 1 , the authors claim that the convergence rate is guaranteed to be smaller than that of SGD . But in the experiment , the performance gap between SWA update and full precision training is not shown . It would be better if the authors can show the performance difference between the two methods . 3 .The authors should provide more details about the implementation details . For example , how many epochs are used in the experiments ? How many iterations are used for each epoch ?
1_1902.04911 = This paper proposes an end-to-end neural model which employs a novel knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Specifically , a posterior distribution over knowledge is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution inferred from utterances only is used to approximate the posterior distribution so that appropriate knowledge can be selected even without responses during the inference process . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The proposed model is not novel . The prior distribution p ( y|x , y ) is inferred only from utterance only , while the posterior p ( k|x ) is assumed to be unknown during inference . 2 .The proposed model can not be compared with other existing knowledge-grounded dialogue models . For example , the commonsense model proposed in [ Zhou et al. , 2018 ] took commonsense knowledge into account , which is served as knowledge background to facilitate conversation understanding . 3 .It is not clear how the proposed method can be applied to other dialogue systems .
1_2005.06628 = This paper proposes a method to reduce the number of parameters in the BERT model . The main idea is to use a smaller number of layers in the encoder of BERT . The authors show that this can be achieved by reducing the architecture design dimensions rather than reducing the Transformer encoder layers . The paper is well written and easy to follow . However , I have the following concerns . 1 .The authors claim that the proposed method can be applied towards other objectives such as FLOPs or latency . But the authors do not provide any experimental results to support this claim . 2 .The experimental results are not convincing . The proposed method is only compared with BERT with three layers . It is not clear whether this method can also be applied to other models such as Transformer-based models . 3 .In Table 1 , the authors only show the results for BERT without three layers , which is not enough to show the effectiveness of this method .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensembling from multi-original models ; ( 3 ) the teacher network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are only conducted on ImageNet dataset . It would be better to conduct experiments on other datasets .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper is technically sound . Cons : 1.1 . The novelty of the proposed method is limited . 2.2 .The experimental results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors study the problem of finding a projective plane of order 10 from projective geometry . The authors translate the problem into Boolean logic and use satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to use SAT solvers instead of special-purpose search code to solve the problem . The reason is that using well-tested SAT solver is less error-prone than writing custom-written code . But it is not clear to me how to verify the correctness of the certificates produced by the solver . 2 .The authors claim that the proposed method is more efficient than writing special purpose search code , but I do n't see any evidence to support this claim . For example , in the proof of Theorem 2.1 , it is shown that the certificates obtained by the proposed solver are consistent with the results of the original search of Lam , Crossfield , and Thiel ( 1989 ) and the independent verification in 2011 . It would be better if the authors can provide more evidence to show the consistency of the results in the paper . 3 .The paper is not self-contained . It is hard to follow the details of the algorithm . For instance , how to choose the number of blocks in each instance ? How to select the blocks in the first six rows of A2 ? What is the definition of A5 ? 4 .The proof of theorem 1.1 is not very clear . It seems to me that the proof is based on the fact that A1 and A2 are not isomorphic . Is it possible to prove that A2 and A3 are isomorphic ?
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of graph isomorphism and shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The authors then develop a simple architecture that is provably the most expressive among the class of GNN and is as powerful as the Weisfeiler-Lehman graph isomorphicism test . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification benchmarks . The paper is well-written and easy to follow . However , I have the following concerns : 1 . In the abstract , the authors claim that GNN ’ s aggregation scheme is a class of functions over multisets that their neural networks can represent , and analyze whether they are able to represent injective multiset functions . This ability to map any two different graphs to different embeddings , however , implies solving the challenging graph-isomorphism problem . 2 .In the proof of Lemma 3.1 , it is not clear to me how to prove that h ( c , X ) = ( 1 + ) · f ( c ) + f ( x ) is unique for each pair ( c,X ) . 3 .Theorem 3.2 shows that for infinitely many choices of , including all irrational numbers , there exists a function f : X → Rn so that f ( C ) = f ( X ) + \sum_ { x_ { i , j } _ { ij } ( x_j } ) , where c and j are the nodes of the same graph . But in Eq .4.4 , h_G = CONCAT ( READOUT ( h ( k ) v |v ) ) is not true . 4 .In Eq.4.5 , the number of iterations of GIN is not defined . 5 .In Section 5.2 , it seems that the authors did not compare the performance of the proposed GIN with other GNN models , such as GCN and GraphSAGE . It would be better if the authors can provide the experimental results of these two models .
1_1907.07355 = This paper presents an analysis of BERT 's performance on the argument comprehension task . The authors show that BERT is able to reach a peak performance of 77 % on the task by exploiting spurious statistical cues in the dataset . They also show that a range of models all exploit these cues and propose to use an adversarial dataset on which all models achieve random accuracy . The paper is well written and easy to follow . The analysis is interesting and provides a new perspective on the BERT model . I think this paper should be accepted to ICLR .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also provide theoretical analysis on the convergence properties of the proposed method and show that it converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than SGD in strongly convex settings . The paper is well-written and well-organized . However , I have the following concerns : 1 . The theoretical analysis is based on the assumption that the variance of these samples always satisfies E [ ‖∇f ( w ) - \tilde f ( w ’ ) ‖22 ] , which is a standard assumption used in the analysis of SGD . It is not clear to me why this assumption is necessary in practice . 2 .In the experiments , the authors only compare the performance with 8-bit quantized SGD and full precision training . It would be better if the authors can compare with other quantization methods such as HALP , QSGD , and ZipML . 3 .In this paper , the author proposed to use BFP instead of fixedpoint quantization for quantization .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that employs a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed model is novel and well-motivated . 3 .The experiments are comprehensive and show the effectiveness of the model . 4 .The paper is technically sound . Cons : 1 ) The proposed model seems to be a straightforward combination of two existing models . It would be better if the authors can provide a more detailed analysis of the differences between the two models . 2 ) It is not clear why the authors use the KLDivLoss to measure the proximity between the prior distribution and the posterior distribution . 3 ) The authors should provide more details about the model architecture . 4 ) The experiments are only conducted on the Wizard-of-Wikipedia dataset . It is better to conduct experiments on other datasets .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT model . The main idea is to prune the parameters of each layer of the model by 5 different dimensions . The pruning method is based on pruning-based architecture search . The authors show that the pruned model achieves 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The proposed method is simple and effective . 3 .The experimental results are convincing . 4 .The paper is easy to read and follow . Cons : The main concern is the novelty of the proposed method . The idea of pruning the parameters is not new . It has been proposed in [ 1 ] , [ 2 ] and [ 3 ] . However , the authors did not compare their method with these methods . The proposed method can be applied to other objectives such as FLOPs or latency . In addition , it is not clear whether the proposed pruning methods can be used to improve the performance of BERT . The experiments are only conducted on two datasets . It will be better if the authors can conduct more experiments on more datasets .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where they define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim , Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experimental results are not convincing . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The experimental results show that the proposed approach outperforms the baselines . Cons : 1.1 . The novelty of this paper is limited . 2.2 .The novelty of the proposed method is limited in the sense that it is a combination of supervised learning and reinforcement learning . 3.3 .The experiments are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to resolve the projective plane of order 10 problem by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well written and easy to follow . However , I have the following concerns : 1 . The main contribution of this paper is to use SAT solvers instead of special-purpose search code to solve the problem , which is not a new idea . In fact , results that were recently proven using SAT solver such as the resolution of the Boolean Pythagorean triples problem ( Heule , Kullmann , and Marek 2016 ) and a case of the Erdo ’ s discrepancy conjecture ( Konev and Lisitsa 2014 ) have since had formal proofs generated based on the SAT encoding ( Cruz-Filipe , Marques-Silva , and Schneider-Kamp 2018 ; Keller 2019 ) . 2 .The paper does not provide a possible avenue for constructing a formal proof : by deriving our SAT encoding and the mathematical results that we rely on inside the formal proof system .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) to capture different graph structures . The main contribution of the paper is to analyze the discriminative power of popular GNN variants , such as Graph Convolutional Networks and GraphSAGE , and show that they can not learn to distinguish certain simple graph structures , and develop a simple architecture that is provably the most expressive among the class of GNN and is as powerful as the Weisfeiler-Lehman graph isomorphism test . The paper is well-written and easy to follow . However , I have the following concerns : 1 . In the abstract , the authors claim that the most powerful GNN , i.e. , Graph Isomorphism Network ( GIN ) , also empirically has high representational power as it almost perfectly fits the training data , whereas the less powerful models often severely underfit the data . But , in the experiments , the GIN only outperforms the state-of-the-art results on a number of graph classification benchmarks , which is not convincing . 2 .In the experimental results , the performance of the proposed GIN is not better than the other GNN models . 3 .The experimental results are not convincing enough . In Table 1 , the results of GIN and GCN are not comparable . 4 .In Table 2 , the result of GCN and GIN are not compared .
1_1907.07355 = This paper presents an analysis of BERT 's performance on the argument comprehension task . The authors show that BERT is able to reach a peak performance of 77 % on the task by exploiting spurious statistical cues in the dataset . They also show that a range of models all exploit these cues and propose to use an adversarial dataset on which all models achieve random accuracy . The paper is well written and easy to follow . The analysis is interesting and provides a new perspective on the BERT model . I think this paper should be accepted to ICLR .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full-precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . Theoretical analysis shows that the proposed method converges arbitrarily close to the optimal solution for quadratic objectives , and to a noise ball asymptotically smaller than low Precision SGD in strongly convex settings . Empirical results show that training with 8-bit SWA can match full precision SGd baseline in deep learning tasks such as training Preactivation ResNet-164 ( He et al. , 2016 ) on CIFAR-10 and Cifar-100 datasets ( Krizhevsky & Hinton , 2009 ) . Overall , this paper is well written and well organized . However , I have the following concerns about this paper : 1 . The theoretical analysis is based on the Lipschitz constant M of the second derivative of f , which defined such that f ( x , y ) is a 2-norm matrix . It is not clear to me how to use this term in practice . 2 .In the experiments , it is better to compare with other methods such as HALP and QSGD . 3 .In Table 1 , the comparison between SWA and HALP is not fair since HALP uses a fixed-point quantization and SWA uses a shared exponent . It would be better to show the performance difference between the two methods .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that uses both prior and posterior distributions over knowledge to facilitate knowledge selection . The paper is well-written and easy to follow . The idea of using the posterior distribution over knowledge is interesting and novel . However , I have the following concerns : 1 . In the prior knowledge module , the prior distribution p ( k|x ) is inferred from both the utterance and the response , while the posterior p ( y|x , y ) is only inferred from the responses . It is not clear to me how the prior p ( x ) can be used to select appropriate knowledge . 2 .In the inference process , the authors use the Kullback-Leibler divergence loss ( KLDivLoss ) to measure the distance between the prior and the posterior distributions . It seems that the authors do not use the KL divergence between the posterior and the prior . 3 .The authors should compare the performance of the proposed model with the baselines on the Persona dataset and Wizard-of-Wikipedia dataset . 4 .The proposed model is not compared with the baseline methods in the literature .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT model . The main idea is to prune the parameters of each layer of the model by 5 different dimensions . The pruning method is based on pruning the architecture design dimensions and the pruning-based architecture search technique . Experiments on GLUE and SQuAD datasets show that the pruned model achieves 6.6 % higher average accuracy compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning different layers of BERT is interesting and novel . 3 .The experimental results are convincing . 4 .The pruning based architecture search method can be applied to other objectives such as FLOPs or latency . Cons : 1 ) It is not clear to me why the prunable parameters in Eq . ( 1 ) and ( 2 ) are pruned . 2 ) The pruned parameters are not different from the original BERT parameters . 3 ) The proposed method is not compared with other pruning methods such as DistilBERT and RoBERTa .
1_2102.07983 = This paper introduces a new low-shot disambiguation dataset called FEWS ( Few-shot Examples of Word Senses ) , which is a dataset of few-shot and zero-shot examples of a wide variety of senses drawn from Wiktionary . The paper is well-written and easy to follow . The dataset is a good contribution to the field of word sense disambigraphy . However , I have some concerns about the novelty of the dataset and the experimental results . 1 .The authors claim that the dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot instances of a broad variety of words senses . I am not sure if this statement is true . For example , in Table 1 , it is not clear to me how many senses are covered by the dataset . 2 .In Table 2 , it seems that the authors did not compare with the SemCor dataset ( Miller et al. , 1993 ) as training data and consolidates a number of evaluation sets ( Pradhan et al .2007a , 2007b ) into a standardized evaluation suite . It would be interesting to see the performance of the models trained on the new dataset compared with SemCor . 3 .I am also not sure why the authors do not compare the results of the proposed dataset with other existing datasets , e.g. , SemCor and the WSD Evaluation Framework ( Raganato et al , 2017 ) . 4 .It would be nice to see how the dataset compares with other datasets in terms of the number of senses covered .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensembling from multi-original models ; ( 3 ) the teacher network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are only conducted on ImageNet dataset . It would be better to conduct experiments on other datasets .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper is technically sound . Cons : 1.1 . The novelty of the proposed method is limited . 2.2 .The experimental results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is star-path or path-path . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of the paper is to study the problem of finding the optimal assignment for each agent in the housing market problem . But , it is not clear to me how to solve the problem when the agents have different preferences . For example , in the social network setting , each agent has two preferences and each agent receives one object . 2 .In the star-star setting , the number of feasible swaps in an instance may increase . This may increase the searching space of the problem dramatically and make the problem harder to search for a solution . 3 .The paper is not well-motivated . In general , the idea of this paper is similar to the work of Gourve s et al . [ 15 ] , which also studied the social networks and compared them to the classic fairness notions and designed new protocols to find envy-free allocations in cake cutting . The author should compare the results of this work with the results in [ 15 , 17 , 18 , 19 ] .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that uses both prior and posterior distributions over knowledge to facilitate knowledge selection . The paper is well-written and easy to follow . The idea of using the posterior distribution over knowledge is interesting and novel . The authors propose to use the prior distribution to ensure the appropriate selection of knowledge during the training process . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . However , I have the following concerns : 1 . In Table 2 , it is not clear how the performance of the model is compared with the baselines in terms of human evaluation . It would be better if the authors can show the results of the human evaluation as well . 2 .It would be more convincing if the author can show that the model can generate more informative responses by incorporating knowledge more properly and generating appropriate and informative responses .
1_2005.06628 = This paper proposes a pruning-based approach to reduce the number of parameters in the BERT language model . The main idea is to prune the architecture of the encoder and the attention head of the Transformer . The pruning is done by optimizing the architecture design dimensions of each layer of the model , and the pruned parameters are then used to train the model in a pre-trained fashion . The authors show that the proposed method can achieve 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the model is interesting . 3 .The proposed pruning method is simple and effective . Cons : 1.1 . It is not clear why the authors chose to optimize the design dimensions in the pruning framework rather than launching a pretraining job for each of these choices . 2.2 .The authors should compare the performance of the proposed pruned model with other pruning methods such as RoBERTa and DistilBERT . 3.3 .The experiments are only conducted on one language modeling task ( GLUE ) . It would be better if the authors can conduct more experiments on other language modeling tasks such as NLP tasks .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a reinforcement learning based framework for the task of video grounding , which aims to temporally localize a natural language description in a video . To alleviate this problem , the authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset and Charades-STA dataset while observing only 10 or less clips per video . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The main contribution of this paper is the multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . 2 .It is not clear why the observation network takes visual and textual information as well as current temporal boundaries into consideration . 3 .The experiments are not convincing enough . The authors should compare the performance of their proposed method with other methods . For example , it is better to compare the results with the following methods : [ 1 ] https : //arxiv.org/abs/1805.05832 , [ 2 ] http : //proceedings.mlr.press/v48/jmlr2018/papers/v49/papers_v48.pdf , and [ 3 ] [ 4 ] [ 5 ] http://www.ncbi.nlm.nih.gov/pmc/articles/PMC-1802.065761/ [ 6 ] http: //www.cs.cmu.edu.edu/~douglas/papers.html
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is star-path or path-path . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of the paper is to study the problem of finding the optimal assignment for each agent in the housing market problem . But , it is not clear to me how to solve the problem when the agents have different preferences . For example , in the social network setting , each agent has two preferences and each agent receives one object . 2 .In the star-star setting , the number of feasible swaps in an instance may increase . This may increase the searching space of the problem dramatically and make the problem harder to search for a solution . 3 .The paper is not well-motivated . In general , the idea of this paper is similar to the work of Gourve s et al . [ 15 ] , which also studied the social networks and compared them to the classic fairness notions and designed new protocols to find envy-free allocations in cake cutting . The author should compare the results of this work with the results in [ 15 , 17 , 18 , 19 ] .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of this paper is to use SAT solver to solve the projective plane of order 10 problem . The authors show consistency issues in both previous searches , which highlights the difficulty of relying on special-purpose search code for nonexistence results . In addition , using well-tested SAT solvers is less error-prone than writing special purpose search code . However , it is difficult to make custom-written code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than SGD in strongly convex settings . The paper is well-written and well-organized . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in Table 2 . 2 .In Table 1 , it would be more convincing if SWA is compared with HALP in terms of accuracy . 3 .In the experiment , the authors should compare with other quantization methods such as QSGD and ZipML . 4 .In Section 4.2 , it will be better to show the convergence rate of SWA with respect to the number of iterations of SGD .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation with knowledge-grounded responses . The authors propose a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . The paper is well-written and easy to follow . However , I have the following concerns . 1 .The authors claim that their model is the first neural model which incorporates the posterior distribution as guidance , enabling accurate knowledge lookups and high quality response generation . But , it is not clear to me how this is achieved . For example , the prior distribution p ( k|x ) is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution is inferred only from utterances only , so that appropriate knowledge can be selected even without responses during the inference process . Compared with the previous work , our model can better incorporate appropriate knowledge in response generation , which is better than previous work . 2 .In Table 1 , the authors compare the performance of their model with Seq2Seq model with a GRU decoder where knowledge is concatenated . It would be better if the authors can show the results of different ways of incorporating knowledge incorporating knowledge . 3 .It is better to show the effect of different loss functions in Table 1 .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT model . The main idea is to prune the architecture of the model by using pruning-based architecture search to find the correct architecture design dimensions . The authors show that the pruned model achieves 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The proposed pruning method is simple and effective . 3 .The experimental results show the effectiveness of the proposed method compared to other pruning methods such as DistilBERT ( Sanh et al. , 2019 ) and RoBERTa ( Liu et al .2019 ) . 4 .The paper is easy to read . Cons : 1.1 . It is not clear to me how the pruning is done in this paper . The pruning algorithm is not explained in the paper . 2.2 .The authors should provide more details about the architecture search algorithm . 3.3 .In the experiments , the authors only compare with BERT base and BERTLARGE . It would be better if the authors can show the results of BERT and schuBERT with different number of pruned parameters .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a reinforcement learning based framework for video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed method is novel in that it successfully combines supervised learning with reinforcement learning in a multi-task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The experimental results show steady performance gains by considering additional supervised boundary information during training . Cons : 1 ) The novelty of this paper is limited . The main contribution of this work is to formulate the task of temporal grounding descriptions in videos as a sequential decision-making problem . However , it is not clear to me how the proposed method can be applied to other video grounding tasks . 2 ) The experimental results are not convincing . The performance of the proposed model is not better than the baselines .
1_1909.07557 = This paper studies the Housing Market problem , where the agents are embedded in a social network to denote the ability to exchange objects between them . Two agents may swap their items under two conditions : they are neighbors in the social network , and they find it mutually profitable ( or no one will become worse under weak preferences ) . The authors show that when the preferences of the agents is weak ( ties among objects are allowed ) , the problem becomes NP-hard when the network is a path and can be solved in polynomial time when the networks is a star . They also show the computational complexity of finding different optimal assignments for the problem in the special case where the networks are path or star . The paper is well-written and easy to follow . The main idea of the paper is to study the problem of finding the optimal assignment for a given agent in the housing market problem . However , I have the following concerns : 1 . In the paper , the authors assume that the agents can only receive a single object and have preferences over all objects . In practice , the agents may have multiple preferences and it is not clear to me how to solve the problem . 2 .The paper is not well-organized . For example , it is hard to follow the proof of Theorem 1 and Theorem 2 . 3 .The proof of Lemma 1 and Lemma 2 are not clear . 4 .In Theorem 3 , the algorithm is not clearly explained . It is better to explain the algorithm in more details .
1_2012.04715 = In this paper , the authors attempt to solve the projective plane of order 10 problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solvers instead of special-purpose search code to perform the search . However , I have the following concerns about the paper : 1 . The authors claim that using SAT solver is less error-prone than writing special purpose search code , but I do n't think this is true . In fact , using well-tested solvers is more error prone than writing custom-written code . 2 .The paper claims that the consistency issues in both previous searches are due to the use of special purpose code . But the authors did n't provide any evidence to support this claim . 3 .The authors claimed that the original search and an independent verification in 2011 discovered no such projective planes , but these searches were each performed using highly specialized custom-coded code and did not produce existence certificates . I think the authors need to provide more evidence to justify the claim . 4 .I think the paper needs to provide a more detailed analysis of the results of the previous search and the independent verification .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) to capture different graph structures . The authors provide a theoretical framework for analyzing GNN ’ s expressive power and show that popular GNN variants , such as Graph Convolutional Networks ( GCN ) and GraphSAGE , can not learn to distinguish simple graph structures such as subtree structures . They then develop a simple architecture that is provably the most expressive among the class of GNN and is as powerful as the Weisfeiler-Lehman graph isomorphism test . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The paper is well-written and easy to follow . However , I have the following concerns about this paper . 1 .Theoretical results are not very surprising . Theorem 3.1 shows that GNN can not represent injective multiset functions . It would be better if the authors can provide some intuition on why this is the case . 2 .Theorem 4.2 shows that max-pooling-based GNN does not learn the same representation as GNN . It is not clear to me how to interpret this result . 3 .In the experiments , the authors only compare the proposed method with GCN with mean aggregators . It will be better to compare with other mean-pooled GNN methods . 4 .The authors should compare with more recent GNN models such as GCN and GraphSage . 5 .The proposed method is not compared with other GNN architectures such as Jumping Knowledge Networks .
1_1907.07355 = This paper presents an interesting analysis of BERT 's performance on the argument reasoning comprehension task . The authors show that the model is exploiting spurious statistical cues in the dataset . They analyze the nature of these cues and demonstrate that a range of models all exploit them . This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The results are interesting . However , I have the following concerns . 1 .The authors claim that BERT ’ s peak performance of 77 % on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline . This result is entirely accounted for by exploitation of spurious statistics in the data . It would be better if the authors can provide more explanation about this phenomenon . 2 .The paper is not well-motivated . It is not clear to me why BERT is able to perform so well on this task . It seems to me that it is because of the fact that the authors do not provide any explanation about why the model performs so well . 3 .In the experiments , the authors only compare BERT with the best model of Botschen et al . ( 2018 ) and the SemEval winner GIST ( Choi and Lee , 2018 ) . I would like to see the performance of other models , such as ESIM and InferSentConneau ( 2017 ) , as well as the state-of-the-art models .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full-precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . Theoretical analysis shows that the proposed method converges arbitrarily close to the optimal solution for quadratic objectives , and to a noise ball asymptotically smaller than low Precision SGD in strongly convex settings . Empirical results show that training with 8-bit SWA can match full precision SGd baseline in deep learning tasks such as training Preactivation ResNet-164 ( He et al. , 2016 ) on Cifar-10 and CIFAR-100 datasets . Overall , this paper is well written and well organized . However , I have the following concerns about this paper : 1 . The theoretical analysis is based on the Lipschitz constant M of the second derivative of f , which defined such that f ( x , y ) is a 2-norm matrix . It is not clear to me why the authors defined such a term M as an extra term . 2 .In the experiments , the authors only compare with the full precision SWA and HALP . It would be better to compare with other methods such as QSGD , HALP , and Q-SGD . 3 .It would be more convincing if the authors can show the convergence rate of SWA with different number of iterations . 4 .The authors should provide more details about the implementation details . For example , how to choose the number of epochs ?
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that employs a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed model is novel and well-motivated . 3 .The experiments are comprehensive and show that the proposed method can better incorporate knowledge in response generation . 4 .The paper is technically sound . Cons : 1 ) The proposed method is not compared with other related works . 2 ) It is not clear how the proposed approach can be applied to other dialogue generation tasks such as Persona-chatting and Wizard-of-Wikipedia . 3 ) The paper does not compare with other existing work on dialogue generation , e.g. , [ Zhou et al. , 2018 ] and [ Ghazvininejad et al .2018 ] .It would be better if the authors can compare with them .
1_2005.06628 = This paper proposes a pruning-based approach to reduce the number of parameters in the BERT language model . The main idea is to prune the architecture of the encoder and the attention head of the Transformer . The pruning is done by optimizing the architecture design dimensions of each layer of the model , and the pruned parameters are then used to train the model in a pre-trained fashion . The authors show that the proposed method can achieve 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the model is interesting . 3 .The proposed pruning method is simple and effective . Cons : 1.1 . It is not clear why the authors chose to optimize the design dimensions in the pruning framework rather than launching a pretraining job for each of these choices . 2.2 .The authors should compare the performance of the proposed pruned model with other pruning methods such as RoBERTa and DistilBERT . 3.3 .The experiments are only conducted on one language modeling task ( GLUE ) . It would be better if the authors can conduct more experiments on other language modeling tasks such as NLP tasks .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a reinforcement learning based framework for the task of video grounding , which aims to temporally localize a natural language description in a video . To alleviate this problem , the authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset and Charades-STA dataset while observing only 10 or less clips per video . Pros : 1 . The paper is well written and easy to follow . 2 .The proposed method is novel in that it successfully combines supervised learning with reinforcement learning in a multi-task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The experimental results show that the proposed method outperforms the baselines . Cons : 1.1 . The novelty of this paper is limited . The main contribution of the paper is to formulate the video grounding problem as a sequential decision-making problem . However , it is not clear to me how this problem can be solved by the proposed approach . 2.2 .The novelty of the proposed model is limited to the RL framework . 3.3 .The experiments are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation with knowledge-grounded responses . The authors propose a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . The paper is well-written and easy to follow . However , I have the following concerns . 1 .The authors claim that their model is the first neural model which incorporates the posterior distribution as guidance , enabling accurate knowledge lookups and high quality response generation . But , it is not clear to me how this is achieved . For example , the prior distribution p ( k|x ) is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution is inferred only from utterances only , so that appropriate knowledge can be selected even without responses during the inference process . Compared with the previous work , our model can better incorporate appropriate knowledge in response generation , which is better than previous work . 2 .In Table 1 , the authors compare the performance of their model with Seq2Seq model with a GRU decoder where knowledge is concatenated . It would be better if the authors can show the results of different ways of incorporating knowledge incorporating knowledge . 3 .It is better to show the effect of different loss functions in Table 1 .
1_2005.06628 = This paper proposes a pruning-based approach to reduce the number of parameters in the BERT language model . The main idea is to prune the architecture of the encoder and the attention head of the Transformer . The pruning is done by optimizing the architecture design dimensions of each layer of the model , and the pruned parameters are then used to train the model in a pre-trained fashion . The authors show that the proposed method can achieve 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the model is interesting . 3 .The proposed pruning method is simple and effective . Cons : 1.1 . It is not clear why the authors chose to optimize the design dimensions in the pruning framework rather than launching a pretraining job for each of these choices . 2.2 .The authors should compare the performance of the proposed pruned model with other pruning methods such as RoBERTa and DistilBERT . 3.3 .The experiments are only conducted on one language modeling task ( GLUE ) . It would be better if the authors can conduct more experiments on other language modeling tasks such as NLP tasks .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a reinforcement learning based framework for the task of video grounding , which aims to temporally localize a natural language description in a video . To alleviate this problem , the authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset and Charades-STA dataset while observing only 10 or less clips per video . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The main contribution of this paper is to use multi-task learning to improve the performance by considering additional supervised boundary information during training . 2 .The experimental results are not convincing . The performance of this method is not better than other baselines . For example , the results in Table 1 and Table 2 are not comparable . 3 .The proposed method does not outperform the baselines in terms of the number of ground truth grounding locations .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is simple and easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The authors also show that SWA converges arbitrarily close to the optimal solution for quadratic objectives and to a noise ball asymptotically smaller than low precision SGd in strongly convex settings . The paper is well-written and the idea is interesting . However , I have the following concerns : 1 . In the experiments , it is not clear why SWA and HALP are compared in Table 1 . It would be better if the authors can show the results of HALP and SWA in the same table . 2 .In Table 1 , the authors only compare SWA with SGD with a fixed learning rate . It is better to compare with other methods such as QSGD or HALP . 3 .In the experiment , it would be more convincing if the author can show results with different learning rate schedules .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation that uses both prior and posterior distributions over knowledge to facilitate knowledge selection . The paper is well-written and easy to follow . The idea of using the posterior distribution over knowledge is interesting and novel . However , I have the following concerns : 1 . In the prior knowledge module , the prior distribution p ( k|x ) is inferred from both the utterance and the response , while the posterior p ( y|x , y ) is only inferred from the responses . It is not clear to me how the prior p ( x ) can be used to select appropriate knowledge . 2 .In the inference process , the authors use the Kullback-Leibler divergence loss ( KLDivLoss ) to measure the distance between the prior and the posterior distributions . It seems that the authors do not use the KL divergence between the posterior and the prior . 3 .The authors should compare the performance of the proposed model with the baselines on the Persona dataset and Wizard-of-Wikipedia dataset . 4 .The proposed model is not compared with the baseline methods in the literature .
1_2005.06628 = This paper proposes a pruning approach to reduce the number of parameters in the BERT model . The main idea is to prune the parameters of each layer of BERT by five different dimensions , and then train multiple variants of the model with different values chosen for these dimensions by applying pruning-based architecture search technique . Experiments are conducted on GLUE and SQuAD datasets , and the proposed method achieves 6.6 % higher average accuracy compared to BERT with three encoder layers while having the same number parameters .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a reinforcement learning based framework for the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset and Charades-STA dataset . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The main contribution of this paper is to combine supervised learning with reinforcement learning in a multi-task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 2 .The experimental results are not convincing . The performance of this method is not better than other baselines . For example , the results in Table 1 are not comparable with the state of the art methods . 3 .The proposed method does not show the advantage of using supervised boundary information during training . 4 .It is not clear to me why the proposed model is better than the baselines in Table 2 .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
1_2012.04715 = In this paper , the authors attempt to solve Lam 's problem from projective geometry by translating the problem into Boolean logic and using satisfiability ( SAT ) solvers to produce nonexistence certificates that can be verified by a third party . The paper is well-written and easy to follow . The main contribution of the paper is to use SAT solver to solve the projective plane of order 10 problem . The authors also show that the consistency issues in the original search and the independent verification in 2011 are due to the use of special-purpose search code . However , these searches were each performed using highly specialized custom-written code and did not produce a nonexistence certificate . This paper shows that using well-tested SAT solvers is less error-prone than writing special purpose search code , which is a reality of developing software for computer-assisted proofs is that it is extremely difficult to make custom-writing code both correct and efficient .
1_1810.00826 = This paper studies the expressive power of graph neural networks ( GNNs ) in the context of the Weisfeiler-Lehman graph isomorphism test . In particular , the paper shows that the most powerful GNN , Graph Isomorphism Network ( GIN ) , has high representational power as it almost perfectly fits the training data , whereas GNN variants often severely underfit the data . The paper then proposes a simple architecture , GraphSAGE , that is provably the most expressive among the class of GNN . The experimental results show that the proposed model achieves state-of-the-art performance on a number of graph classification tasks . The main contribution of this paper is to provide a theoretical framework for analyzing GNN ’ s expressive power to capture different graph structures . However , I have the following concerns about this paper . 1 .The paper claims that GNN can represent injective multiset functions , however , it is not clear to me how to interpret this statement . For example , in Eq . ( 5.1 ) , the authors claim that h ( c , X ) = ( 1 + ) + f ( c ) + \sum_ { x } _ { f ( x ) } is unique for each pair ( c,X ) , where c and X are multisets of bounded size . But in the proof of Theorem 3.1 , it seems to me that for infinitely many choices of f , including all irrational numbers , the function f is not unique . 2 .In the experimental results , the performance of the proposed GIN is not better than other GNN models . In Table 1 , the results of GCN ( Hamilton et al. , 2017 ) is better than the proposed one . It is not convincing to me why the proposed method is superior to GCN . 3 .In Table 2 , the result of GIN with 1-layer perceptrons are better than GCN with 2-layer in terms of classification accuracy . It seems that GCN has better classification accuracy than GIN . It would be better if the authors can provide more explanation about this result . 4 .In Section 5.2 , the author claimed that GIN can not learn to distinguish simple graph structures , which is not true . The authors should provide more explanations about this claim .
1_1907.07355 = This paper studies the effect of spurious statistical cues on BERT 's performance on the argument reasoning comprehension task . The authors show that a range of models all exploit these cues , and propose an adversarial dataset on which all models achieve random accuracy . The paper is well-written and easy to follow . The main contribution of this paper is to show that the BERT performance is entirely accounted for by exploitation of spurious statistics in the dataset . This is an interesting result . However , I have some concerns about this paper . 1 .The authors claim that BERT ’ s performance is only three points below the average untrained human baseline , which is surprising . It is not clear to me how this result can be explained by the spurious statistics . 2 .In the experiments , the authors only consider unigrams and bigrams , although more sophisticated cues may be present . It would be interesting to see the results of more complex cues , such as bigrams . 3 .In Table 1 , it would be better if the authors can report the mean and median performance of BERT . 4 .In Figure 1 , the error bars are too small . It will be better to show the average error bars . 5 .In Section 3.2 , it is unclear to me why the authors did not include the results in Table 1 .
1_1904.11943 = This paper proposes SWALP , an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule . The proposed method is easy to implement and can match the performance of full precision SGD even with all numbers quantized down to 8 bits , including the gradient accumulators . The paper is well written and easy to follow . The experimental results are convincing and show that the proposed method converges arbitrarily close to the optimal solution for quadratic objectives , and to a noise ball asymptotically smaller than low Precision SGD in strongly convex settings .
1_1902.04911 = This paper proposes an end-to-end neural model for dialogue generation with knowledge-grounded responses . The authors propose a knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection . Experiments on both automatic and human evaluation verify the superiority of the proposed model over previous baselines . The paper is well-written and easy to follow . However , I have the following concerns . 1 .The authors claim that their model is the first neural model which incorporates the posterior distribution as guidance , enabling accurate knowledge lookups and high quality response generation . But , it is not clear to me how this is achieved . For example , the prior distribution p ( k|x ) is inferred from both utterances and responses , and it ensures the appropriate selection of knowledge during the training process . Meanwhile , a prior distribution is inferred only from utterances only , so that appropriate knowledge can be selected even without responses during the inference process . Compared with the previous work , our model can better incorporate appropriate knowledge in response generation , which is better than previous work . 2 .In Table 1 , the authors compare the performance of their model with Seq2Seq model with a GRU decoder where knowledge is concatenated . It would be better if the authors can show the results of different ways of incorporating knowledge incorporating knowledge . 3 .It is better to show the effect of different loss functions in Table 1 .
1_2005.06628 = This paper proposes a pruning-based approach to reduce the number of parameters in the BERT language model . The main idea is to prune the architecture of the encoder and the attention head of the Transformer . The pruning is done by optimizing the architecture design dimensions of each layer of the model , and the pruned parameters are then used to train the model in a pre-trained fashion . The authors show that the proposed method can achieve 6.6 % higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number parameters . Pros : 1 . The paper is well written and easy to follow . 2 .The idea of pruning the model is interesting . 3 .The proposed pruning method is simple and effective . Cons : 1.1 . It is not clear why the authors chose to optimize the design dimensions in the pruning framework rather than launching a pretraining job for each of these choices . 2.2 .The authors should compare the performance of the proposed pruned model with other pruning methods such as RoBERTa and DistilBERT . 3.3 .The experiments are only conducted on one language modeling task ( GLUE ) . It would be better if the authors can conduct more experiments on other language modeling tasks such as NLP tasks .
1_2102.07983 = This paper introduces a new low-shot WSD dataset , FEWS ( Few-shot Examples of Word Senses ) , a new dataset automatically extracted from example sentences in Wiktionary . The dataset has high sense coverage across different natural language domains and provides : ( 1 ) a large training set that covers many more senses than previous datasets and ( 2 ) a comprehensive evaluation set containing few-and-zero-shot examples of a wide variety of senses . The authors establish baselines on the new dataset with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with the dataset better capture rare senses in existing WSD datasets . Finally , the authors find that humans outperform the best baseline models on the dataset . Overall , the paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the dataset is limited . It is a collection of sentences extracted from Wikipedia . 2 .The authors claim that the dataset provides high coverage of many diverse words in a low shot manner , however , it is not clear to me how this is achieved . For example , in Table 1 , the number of rare senses is not reported . 3 .In Table 2 , the performance of the biencoder is not compared with human annotators . 4 .The evaluation set is not well-defined . For instance , how many senses are used in the evaluation set ? 5 .It is unclear to me why the authors did not compare with the SemCor dataset , which is the current de facto benchmark for modeling English WSD .
1_1812.02425 = This paper proposes a method for compressing large , complex trained ensembles into a single network , where knowledge from a variety of trained deep neural networks ( DNNs ) is distilled and transferred to a single DNN . In order to distill diverse knowledge from different trained ( teacher ) models , the authors propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models , and to promote the discriminator network to distinguish teacher vs. student features simultaneously . Extensive experiments on CIFAR-10/100 , SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method . The proposed ensemble method ( MEAL ) of transferring distilled knowledge with adversarial learning exhibits three important advantages : ( 1 ) the student network that learns the distilled knowledge by discriminators is optimized better than the original model ; ( 2 ) fast inference is realized by a single forward pass , while the performance is even better than traditional ensemble from multi-original models ; ( 3 ) student network can learn the distilled Knowledge from a teacher model that has arbitrary structures . The paper is well-written and easy to follow . However , I have the following concerns : 1 . The novelty of the proposed method is limited . The idea of distilling knowledge from trained neural networks and transferring it to another new network has been well explored in ( Hinton , Vinyals , and Dean 2015 ; Chen , Goodfellow , and Shlens 2016 ; Li et al.2017 ; Yim et al .2017 ; Bagherinezhad , 2018 ; Anil , 2018 ) . 2 .The experiments are not convincing enough . In Table 1 , the performance of ResNet-50 based MEAL achieves top1/5 21.79 %/5.99 % val error , which outperforms the original models by 2.06 %/1.14 % .
1_1901.06829 = This paper proposes a novel approach to the task of video grounding , which aims to temporally localize a natural language description in a video . The authors formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy . Specifically , they propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training . The proposed framework achieves state-of-the-art performance on ActivityNet ’ 18 DenseCaption dataset ( Krishna et al. 2017 ) and Charades-STA dataset ( Sigurdsson et al .2016 ) while observing only 10 or less clips per video . Pros : 1 . The paper is well-written and easy to follow . 2 .The proposed approach is novel in that it successfully combines supervised learning with reinforcement learning in a multi task learning framework , which helps the agent obtain more accurate information about the environment and better explore the state space to encounter more reward , thus it can act more effectively towards task accom- plishment . 3 .The paper is technically sound . Cons : 1.1 . The novelty of the proposed method is limited . 2.2 .The experimental results are not convincing .
1_1909.07557 = This paper studies the Housing Market problem , where each agent can only receive a single object and has preferences over all objects . The authors consider whether an object is reachable for a given agent under a social network , where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade . Assume that the preferences of the agents are strict ( no tie among objects is allowed ) . This problem is polynomial-time solvable in a starnetwork and NP-complete in a tree-network . It is left as an open problem whether the problem can be solved in polynomially time when the network is a path or a star . The paper shows that when the preferences are weak ( no ties among objects are allowed ) , the problem becomes NP-hard . Then , the authors show that the problem is solvable when the networks is path or star . This paper is well-written and easy to follow . However , I have the following concerns : 1 . The main idea of this paper is to study the problem of finding the optimal assignment for an agent in the housing market problem . But , it is not clear to me how to solve the problem in the social network setting . 2 .The paper is not well-organized . For example , in Section 2.3.1 , the definition of Pareto Efficiency is not clearly explained . 3 .In Section 3.2 , the proof of Theorem 2.1 and Theorem 3.1 are not clear . 4 .In Theorem 4 , the algorithm is not explained clearly . 5 .Theorem 4.1 is not proved clearly .
